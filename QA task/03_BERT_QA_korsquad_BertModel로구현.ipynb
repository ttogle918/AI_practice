{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_BERT_QA_squad_BertForQuestionAnswering.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOkRZie01GLvjnXviI6Mait",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2f2ea853c5d24587ade7a5eb620f5517": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a8e613c53b144d118afe8c97f2d54379",
              "IPY_MODEL_2aacecfe541e4222bb95f17a0a1247e4",
              "IPY_MODEL_871e6076927d47d99842a2a57e35fe77"
            ],
            "layout": "IPY_MODEL_1604955b90594bc7b9bf9387d344475e"
          }
        },
        "a8e613c53b144d118afe8c97f2d54379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09e51d9502f94282b42f743efd85d139",
            "placeholder": "​",
            "style": "IPY_MODEL_31706a4fa6ee45d6a49c4097317e55e8",
            "value": "100%"
          }
        },
        "2aacecfe541e4222bb95f17a0a1247e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0111af2cfda14b309f25e0703dc9514e",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_85ab79506080438c8196ca530a2a9809",
            "value": 2
          }
        },
        "871e6076927d47d99842a2a57e35fe77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d40452d5c712449cacb967a5670d0b28",
            "placeholder": "​",
            "style": "IPY_MODEL_042914c4cbab417bb45198ea426c899b",
            "value": " 2/2 [00:00&lt;00:00, 55.69it/s]"
          }
        },
        "1604955b90594bc7b9bf9387d344475e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09e51d9502f94282b42f743efd85d139": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31706a4fa6ee45d6a49c4097317e55e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0111af2cfda14b309f25e0703dc9514e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85ab79506080438c8196ca530a2a9809": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d40452d5c712449cacb967a5670d0b28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "042914c4cbab417bb45198ea426c899b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttogle918/AI_practice/blob/main/QA%20task/03_BERT_QA_korsquad_BertModel%EB%A1%9C%EA%B5%AC%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# korsquad를 사용하여 QA task 풀기(BertModel fine-tuning)\n",
        "\n",
        "참고 사이트\n",
        "\n",
        "[huggingface QA 설명](https://huggingface.co/course/chapter7/7?fw=tf)\n",
        "\n",
        "[huggingface git : ModelOutput](https://github.com/huggingface/transformers/blob/v4.21.0/src/transformers/utils/generic.py#L147)\n",
        "\n",
        "[huggingface git : QuestionAnsweringModelOutput](https://github.com/huggingface/transformers/blob/a9eee2ffecc874df7dd635b2c6abb246fdb318cc/src/transformers/modeling_outputs.py#L764)\n",
        "\n",
        "[huggingface docs](https://huggingface.co/docs/transformers/model_doc/bert)"
      ],
      "metadata": {
        "id": "QocCi_jhZ-ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "62Oa6eDdlfHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Qu0sRPezlVoA"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric#, list_metrics\n",
        "\n",
        "from transformers import BertModel, AutoTokenizer, BertConfig, BertPreTrainedModel\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers.modeling_outputs import QuestionAnsweringModelOutput\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "1N7PgGshrtGh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gpu 연산이 가능하면 'cuda:0', 아니면 'cpu' 출력\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device, torch.cuda.device_count()"
      ],
      "metadata": {
        "id": "NMC8y1NUSDHP",
        "outputId": "24a9df3f-9f9f-485d-e5d8-413f43f3e4d3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(device(type='cuda', index=0), 1)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Load"
      ],
      "metadata": {
        "id": "50EDgWKELGI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/datasets/squad_kor_v1/blob/main/squad_kor_v1.py\n",
        "# squad_kor_v2\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('squad_kor_v1')\n",
        "dataset, dataset['train'][0]"
      ],
      "metadata": {
        "id": "STFveXBu634F",
        "outputId": "1b316c41-4f34-443b-a3db-d052f2397fbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418,
          "referenced_widgets": [
            "2f2ea853c5d24587ade7a5eb620f5517",
            "a8e613c53b144d118afe8c97f2d54379",
            "2aacecfe541e4222bb95f17a0a1247e4",
            "871e6076927d47d99842a2a57e35fe77",
            "1604955b90594bc7b9bf9387d344475e",
            "09e51d9502f94282b42f743efd85d139",
            "31706a4fa6ee45d6a49c4097317e55e8",
            "0111af2cfda14b309f25e0703dc9514e",
            "85ab79506080438c8196ca530a2a9809",
            "d40452d5c712449cacb967a5670d0b28",
            "042914c4cbab417bb45198ea426c899b"
          ]
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset squad_kor_v1 (/root/.cache/huggingface/datasets/squad_kor_v1/squad_kor_v1/1.0.0/18d4f44736b8ee85671f63cb84965bfb583fa0a4ff2df3c2e10eee9693796725)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2f2ea853c5d24587ade7a5eb620f5517"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 60407\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 5774\n",
              "    })\n",
              "}),\n",
              " {'answers': {'answer_start': [54], 'text': ['교향곡']},\n",
              "  'context': '1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.',\n",
              "  'id': '6566495-0-0',\n",
              "  'question': '바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?',\n",
              "  'title': '파우스트_서곡'})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_length, question_length, answer_location = [], [], []\n",
        "for data in dataset['train'] :\n",
        "  context_length.append(len(data['context']))\n",
        "  question_length.append(len(data['question']))\n",
        "  answer_location.append(context_length[-1] - data['answers']['answer_start'][0])    # answer_start가 1개라는 가정하에(잘못된 값이 없다는 가정) 문맥의 끝에서부터 위치 구하기\n",
        "                                                                                    # (적게 차이나면 tokenizer에서 truncation때문에 잘릴 위험이 있다.) => truncation 제거!\n",
        "print(f'max context_length : {max(context_length)}  min context_length : {min(context_length)}  mean context_length : {np.mean(np.array(context_length))}')\n",
        "print(f'max question_length : {max(question_length)}  min question_length : {min(question_length)}  mean question_length : {np.mean(np.array(question_length))}')\n",
        "print(f'max answer_location : {max(answer_location)}  min answer_location : {min(answer_location)}')"
      ],
      "metadata": {
        "id": "l48LDFruCBrJ",
        "outputId": "40b6410e-52bf-4db9-a2fc-ecdb69abc68d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max context_length : 10012  min context_length : 348  mean context_length : 519.2681808399689\n",
            "max question_length : 146  min question_length : 5  mean question_length : 33.79881470690483\n",
            "max answer_location : 10005  min answer_location : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"min answer_location : {np.argmin(answer_location)}\")\n",
        "print(f\"min answer_idx's context : {dataset['train']['context'][np.argmin(np.array(answer_location))]}\")\n",
        "print(f\"min answer_idx's context length : {len(dataset['train']['context'][np.argmin(np.array(answer_location))])}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tJq1YxlnNcjm",
        "outputId": "68197403-2ca6-4513-efa4-33da2a0553b8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "min answer_location : 1670\n",
            "min answer_idx's context : 고조 사후, 혜제가 즉위하고 고황후가 실권을 잡아 나라를 다스렸다. 관영은 열후로서 고황후를 보좌했다. 태후가 죽자, 여록 등의 여씨 일족이 장안의 군사를 장악하고 있었고, 고조의 손자, 유비의 아들로 제나라의 왕을 지내는 유양은 불만을 품고 반란을 일으켜 여씨 세력을 쓸어버리고자 했다. 여씨 일족은 관영에게 군사를 맡겨 제나라의 반란을 진압하도록 했으나, 관영은 태위 주발, 승상 진평 등과 모의해 여씨를 치도록 작정했다. 그러기에 관영은 형양에 주둔하면서 소문을 퍼뜨려 제나라 군사가 진격을 멈추게 하고, 주발 등이 이 틈에 여씨들을 말갛게 쓸었다. 유양과 관영 모두 군사를 해산해 돌아갔고, 관영은 주발, 진평과 함께 고조의 아들 대왕을 황제로 세우니, 이가 곧 문제다.\n",
            "min answer_idx's context length : 379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가장 긴 질문\n",
        "text = tokenizer(dataset['train']['question'][np.argmax(np.array(question_length))], add_special_tokens=False).input_ids\n",
        "print(text)\n",
        "dataset['train']['question'][np.argmax(np.array(question_length))], len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cucf_49bQq3W",
        "outputId": "adcca552-23b7-4591-e461-62f1ab22abea"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3708, 20340, 5461, 2138, 4934, 19521, 3708, 1091, 22034, 2124, 2226, 27135, 4886, 2470, 4043, 2052, 1513, 2259, 3611, 2179, 21155, 2564, 2170, 2318, 4584, 2328, 2118, 5650, 2084, 2069, 1048, 2318, 19521, 16, 3723, 2068, 2339, 2069, 3864, 19521, 2155, 7900, 2052, 4913, 2470, 4467, 2069, 3704, 2205, 2318, 6159, 9035, 8918, 2200, 4184, 2145, 6942, 2170, 2318, 6585, 2118, 2067, 10574, 2069, 4209, 2259, 886, 2079, 1751, 4008, 2069, 1891, 3611, 2073, 4061, 2179, 2116, 35]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('일본 메이지 대학교를 졸업하고 일본 문교사회국에서 근무한 경험이 있는 사람인 박철웅에게 설립동지회장직을 맡게하고, 대학설립을 준비하고자 도청이 소유한 차량을 사용하게 하고 도지사 명의로 사장과 군수에게 협조지시 공문을 보내는 등의 큰 역할을 한 사람은 누구인가?',\n",
              " 78)"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer('하나의 교향곡을 쓰려는 뜻을 갖는다.', add_special_tokens=False).input_ids\n",
        "answer = tokenizer('하나의 교향곡', add_special_tokens=False).input_ids\n",
        "tokens = tokenizer(['1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다.', '이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다'], \n",
        "                   ['이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다', '이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다'], \n",
        "                   return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "text, answer, tokens"
      ],
      "metadata": {
        "id": "03UjdR3LmyiF",
        "outputId": "6c829fbc-fbaf-4455-bce0-a075709d0353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([3657, 2079, 19282, 2069, 1363, 2370, 2259, 936, 2069, 554, 2259, 2062, 18],\n",
              " [3657, 2079, 19282],\n",
              " {'input_ids': tensor([[    2, 13934,  2236,  2440, 27982,  2259, 21310,  2079, 11994,  3791,\n",
              "           2069,  3790,  1508,  2088,   636,  3800,  2170,  3717,  2052,  9001,\n",
              "           8345,  4642,  2200,  3689,  3657,  2079, 19282,  2069,  1363,  2370,\n",
              "           2259,   936,  2069,   554,  2259,  2062,    18,     3,  8345,  4642,\n",
              "           2200,  3689,  3657,  2079, 19282,  2069,  1363,  2370,  2259,   936,\n",
              "           2069,   554,  2259,  2062,     3],\n",
              "         [    2,  8345,  4642,  2200,  3689,  3657,  2079, 19282,  2069,  1363,\n",
              "           2370,  2259,   936,  2069,   554,  2259,  2062,     3,  8345,  4642,\n",
              "           2200,  3689,  3657,  2079, 19282,  2069,  1363,  2370,  2259,   936,\n",
              "           2069,   554,  2259,  2062,     3,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1],\n",
              "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0]])})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensorized_label = np.zeros(tokens.input_ids.shape)\n",
        "print(answer)\n",
        "for i in [0,1] :\n",
        "  padding_start = (tokens['attention_mask'][i] == 1).nonzero()[-1].item()+1\n",
        "  print(padding_start)\n",
        "  tensorized_label[i, padding_start-len(text): padding_start-len(text)+len(answer)] = 1\n",
        "  print(tensorized_label)\n",
        "  print(tokens['input_ids'][i])\n",
        "  print(tokens['input_ids'][i, padding_start-len(text): padding_start-len(text)+len(answer)])\n",
        "  print('\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO9NIqjvq7i_",
        "outputId": "6db96d7f-8b54-47ba-ac9c-3dbad9d98fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3657, 2079, 19282]\n",
            "55\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]]\n",
            "tensor([    2, 13934,  2236,  2440, 27982,  2259, 21310,  2079, 11994,  3791,\n",
            "         2069,  3790,  1508,  2088,   636,  3800,  2170,  3717,  2052,  9001,\n",
            "         8345,  4642,  2200,  3689,  3657,  2079, 19282,  2069,  1363,  2370,\n",
            "         2259,   936,  2069,   554,  2259,  2062,    18,     3,  8345,  4642,\n",
            "         2200,  3689,  3657,  2079, 19282,  2069,  1363,  2370,  2259,   936,\n",
            "         2069,   554,  2259,  2062,     3])\n",
            "tensor([ 3657,  2079, 19282])\n",
            "\n",
            "\n",
            "\n",
            "35\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]]\n",
            "tensor([    2,  8345,  4642,  2200,  3689,  3657,  2079, 19282,  2069,  1363,\n",
            "         2370,  2259,   936,  2069,   554,  2259,  2062,     3,  8345,  4642,\n",
            "         2200,  3689,  3657,  2079, 19282,  2069,  1363,  2370,  2259,   936,\n",
            "         2069,   554,  2259,  2062,     3,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0])\n",
            "tensor([ 3657,  2079, 19282])\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = dataset['train']['context'][np.argmax(context_length)]\n",
        "li = tokenizer(context).input_ids\n",
        "# 정의되지않은 단어(한자 등)은 어떻게 할 것인가?//정답에 한문이 포함되어있기도 해서... 가지고 가야할 것 같다\n",
        "tokenizer.decode([1]), li.count(1), dataset['train']['answers'][np.argmax(context_length)]"
      ],
      "metadata": {
        "id": "OeWJ7Ue0G00N",
        "outputId": "5cc551dc-5d71-4369-e1af-57e4a218f206",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[UNK]', 127, {'answer_start': [19], 'text': ['인길(仁吉)']})"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.question, self.context, self.answer_start, self.answer_text = self.make_dataset(dataset)\n",
        "\n",
        "    def make_dataset(self, dataset):\n",
        "        context, question, answer_start, answer_text = [], [], [], []\n",
        "        for i, data in enumerate(dataset) :\n",
        "          start = data['answers']['answer_start']\n",
        "          if len(start) != 1 : # 답이 없을 때\n",
        "            print(i, data)\n",
        "            continue\n",
        "          text, start = self.get_text(data['context'], start[0])\n",
        "          answer_start.append(start)\n",
        "          answer_text.append(data['answers']['text'][0])\n",
        "          context.append(data['context'])\n",
        "          question.append(data['question'])\n",
        "        return question, context, answer_start, answer_text\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.question)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.question[idx], self.context[idx], self.answer_start[idx], self.answer_text[idx]\n",
        "\n",
        "    def get_text(self, text, start_loc) :\n",
        "        text_splited = text.split('. ')\n",
        "        length_text_answer_idx = -1\n",
        "        length_text = [0]\n",
        "\n",
        "        for i, t in enumerate(text_splited) :\n",
        "          length_text.append(length_text[-1]+len(t)+2)\n",
        "          \n",
        "          if length_text_answer_idx == -1 and start_loc < length_text[-1] :\n",
        "            length_text_answer_idx = i-1\n",
        "        length_text[-1] -= 2\n",
        "\n",
        "        start, end = 0, len(length_text)-1\n",
        "        # 이후에 question + context가 512 token 이하여야 한다.\n",
        "        # context가 512일 때, Token indices sequence length is longer than the specified maximum sequence length for this model (946 > 512), 256일 때 (521 > 512), \n",
        "        # 정답이 context의 마지막에 위치할 수도 있기에(뒤에서 4번째에 위치하기도 함) context 길이 200으로 지정\n",
        "        while length_text[end] - length_text[start] > 160 :  \n",
        "            if start_loc - length_text[start] > length_text[end] - start_loc :\n",
        "              start += 1\n",
        "            else :\n",
        "              end -= 1\n",
        "        \n",
        "        return text[length_text[start]:length_text[end]], start_loc - length_text[start]"
      ],
      "metadata": {
        "id": "Amz1l80dSL1m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch):\n",
        "    global tokenizer\n",
        "    question_list, context_list, answer_start, answer_text, after_list = [], [], [], [], []\n",
        "\n",
        "    for _question, _context, _start, _text in batch:\n",
        "        question_list.append(_question)\n",
        "        context_list.append(_context)\n",
        "        after_list.append(_context[_start:])\n",
        "        answer_start.append(_start)\n",
        "        answer_text.append(_text)\n",
        "\n",
        "    tensorized_input = tokenizer(    # 정답 잘림 방지를 위해 max_len과 truncation 제외?\n",
        "        question_list, context_list,\n",
        "        add_special_tokens=True,\n",
        "        padding=\"longest\",  # 배치내 가장 긴 문장을 기준으로 부족한 문장은 [PAD] 토큰을 추가\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # answer_start token의 위치를 찾기 위해 list로 반환\n",
        "    after_text = tokenizer(\n",
        "        after_list,\n",
        "        add_special_tokens=False,\n",
        "        return_tensors=None\n",
        "    ).input_ids\n",
        "\n",
        "    # answer text token만 변환\n",
        "    answer_tokens = tokenizer(\n",
        "        answer_text,\n",
        "        add_special_tokens=False,\n",
        "        return_tensors=None\n",
        "    ).input_ids\n",
        "\n",
        "    tensorized_label_ones = np.ones(tensorized_input.input_ids.shape)    # input_ids만큼의 길이이고 1으로 된 array\n",
        "    tensorized_label_zero = np.zeros(tensorized_input.input_ids.shape)    # input_ids만큼의 길이이고 0으로 된 array\n",
        "\n",
        "    for i, zipped in enumerate(zip(after_text, answer_tokens)) :\n",
        "        text, answer = zipped\n",
        "        padding_start = (tensorized_input['attention_mask'][i] == 1).nonzero()[-1].item()+1\n",
        "        tensorized_label_ones[i, padding_start-len(text): padding_start-len(text)+len(answer)] = 0\n",
        "        tensorized_label_zero[i, padding_start-len(text): padding_start-len(text)+len(answer)] = 1\n",
        "\n",
        "    tensorized_label_ones = torch.from_numpy(tensorized_label_ones)\n",
        "    tensorized_label_zero = torch.from_numpy(tensorized_label_zero)\n",
        "    tensorized_label = torch.stack([tensorized_label_ones, tensorized_label_zero], dim=2)\n",
        "    return tensorized_input, tensorized_label"
      ],
      "metadata": {
        "id": "H8bLGE3dVVR8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataloader(dataset, tokenizer, batch_size, s='train') :\n",
        "  dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size =batch_size,\n",
        "      sampler = RandomSampler(dataset) if s == 'train' else SequentialSampler(dataset),\n",
        "      collate_fn = custom_collate_fn\n",
        "  )\n",
        "  print(f'batch_size : {batch_size}')\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "1eTDFIVxYu73"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 설명\n"
      ],
      "metadata": {
        "id": "IJoPh3xpqUKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomBertForQuestionAnswering(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.labels_type = [0, 1]   # 1은 정답 token의 위치(여러개 가능)\n",
        "        self.num_labels = len(self.labels_type)\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\n",
        "        self.qa_output = nn.Sequential(nn.Linear(config.hidden_size, config.hidden_size),\n",
        "                                      nn.GELU(),\n",
        "                                      nn.Linear(config.hidden_size, 128),\n",
        "                                      nn.GELU(),\n",
        "                                      nn.Linear(128, self.num_labels))\n",
        "        # self.dropout = nn.Dropout(config.dripout_rate)\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)['last_hidden_state']\n",
        "\n",
        "        logits = self.qa_output(outputs)    # linear 통과해서 num_label로 분류\n",
        "        # logits.requires_grad(True)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "AyiUyiJ6Mtse"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_params(model) :\n",
        "  params = list(model.named_parameters())\n",
        "  print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
        "\n",
        "  print('==== Embedding Layer ====\\n')\n",
        "\n",
        "  for p in params[0:5]:\n",
        "      print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "  print('\\n==== First Transformer ====\\n')\n",
        "\n",
        "  for p in params[5:21]:\n",
        "      print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
        "\n",
        "  print('\\n==== Output Layer ====\\n')\n",
        "\n",
        "  for p in params[-6:]:\n",
        "      print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"
      ],
      "metadata": {
        "id": "D7k2FDWzQ6s3"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train"
      ],
      "metadata": {
        "id": "tZXfyh4_Mta4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initializer(train_dataloader, epochs=2, model_name='klue/bert-base', lr=4e-5, wd=4e-5):\n",
        "    \"\"\"\n",
        "    모델, 옵티마이저, 스케쥴러 초기화\n",
        "    \"\"\"\n",
        "    config = BertConfig.from_pretrained(model_name)\n",
        "    config.max_length = 512\n",
        "    model = CustomBertForQuestionAnswering(config)\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(), # update 대상 파라미터를 입력\n",
        "        lr=lr,    # 2e-5\n",
        "        eps=1e-8,\n",
        "        weight_decay=wd\n",
        "    )\n",
        "    \n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    print(f\"Total train steps with {epochs} epochs: {total_steps}\")\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, \n",
        "        num_warmup_steps = 0, # 여기서는 warmup을 사용하지 않는다.\n",
        "        num_training_steps = total_steps\n",
        "    )\n",
        "    print(f'model_name : {model_name}, lr : {lr}, weight_decay : {wd}, epochs : {epochs}')\n",
        "    return model, optimizer, scheduler"
      ],
      "metadata": {
        "id": "GDcl_0m3Zj0f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(path, model, optimizer, scheduler, epoch, loss, f1, model_name=''):\n",
        "    file_name = f'{path}/epoch:{epoch}_loss:{loss:.4f}_f1:{f1:.4f}.ckpt'\n",
        "    \n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'loss' : loss,\n",
        "            'f1' : f1\n",
        "        }, \n",
        "        file_name\n",
        "    )\n",
        "    \n",
        "    print(f\"Saving epoch {epoch} checkpoint at {file_name}\")"
      ],
      "metadata": {
        "id": "TnpT2s-qZnG-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train code"
      ],
      "metadata": {
        "id": "ut6WMU93Z0in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, scheduler, train_dataloader, valid_dataloader=None, epochs=1):\n",
        "  loss_fct = nn.MSELoss()\n",
        "  em_fct = load_metric('exact_match')\n",
        "  f1_fct = load_metric('f1')\n",
        "  acc_fct = load_metric('accuracy')\n",
        "\n",
        "  train_dict = {'loss' : [], 'f1' : []}\n",
        "  valid_dict = {'loss' : [], 'f1' : [], 'em' : []}\n",
        "\n",
        "  for epoch in range(epochs) :\n",
        "\n",
        "    print(f\"*****Epoch {epoch} Train Start*****\")\n",
        "    total_loss, total_acc, batch_acc, total_f1, batch_f1, batch_em, total_em, batch_loss, batch_count = 0,0,0,0,0,0,0,0,0\n",
        "    \n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    \n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      batch_count+=1\n",
        "      \n",
        "      batch = tuple(item.to(device) for item in batch)\n",
        "      batch_input, batch_label = batch\n",
        "      #  batch마다 모델이 갖고 있는 기존 gradient를 초기화\n",
        "      model.zero_grad()\n",
        "\n",
        "      outputs = model(**batch_input)  # forward\n",
        "      # outputs = torch.argmax(outputs, dim=2)\n",
        "\n",
        "      loss = loss_fct(outputs.float(), batch_label.float())\n",
        "      # loss = loss_fct(labels.to(torch.float32), probs.to(torch.float32))\n",
        "      # backward -> 파라미터의 미분(gradient)를 자동으로 계산\n",
        "      loss.backward()\n",
        "\n",
        "      # gradient clipping 적용 \n",
        "      clip_grad_norm_(model.parameters(), 1.0)\n",
        "      \n",
        "      # optimizer & scheduler 업데이트\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "\n",
        "      # 그래디언트 초기화\n",
        "      model.zero_grad()\n",
        "\n",
        "      batch_loss += loss.item()\n",
        "      total_loss += loss.item()\n",
        "      em_value = 0\n",
        "      for output, label in zip(outputs, batch_label) :\n",
        "        em = em_fct.compute(predictions=output, references=label)['exact_match']\n",
        "        em_value += em\n",
        "      em_value /= len(outputs)\n",
        "      em = em_value\n",
        "\n",
        "      f1 = f1_fct.compute(predictions=outputs.view([-1, 1]).squeeze(), references=batch_label.view([-1, 1]).squeeze(), average='macro')['f1']\n",
        "      accuracy = acc_fct.compute(predictions=outputs.view([-1, 1]).squeeze(), references=batch_label.view([-1, 1]).squeeze())['accuracy']\n",
        "\n",
        "      batch_em += em\n",
        "      total_em += em\n",
        "\n",
        "      batch_f1 += f1\n",
        "      total_f1 += f1\n",
        "      \n",
        "      batch_acc += accuracy\n",
        "      total_acc += accuracy\n",
        "\n",
        "      if (step % 128 == 0 and step != 0):\n",
        "          learning_rate = optimizer.param_groups[0]['lr']\n",
        "          print(f\"Epoch: {epoch}, Step : {step}, LR : {learning_rate:.10f}, Avg Loss : {batch_loss / batch_count:.4f}, f1 score : {batch_f1 / batch_count:.4f}\")\n",
        "          \n",
        "          if (round(batch_f1 / batch_count, 5) == 0) and (round(learning_rate, 10) == 0) :\n",
        "              print(\"Train Finished, learning_rate is 0 and train_f1 is 0\")\n",
        "              return train_dict, valid_dict\n",
        "\n",
        "          batch_loss, batch_f1, batch_count = 0,0,0\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch} Total Mean Loss : {total_loss/(step+1):.4f}\")\n",
        "    print(f\"Epoch {epoch} Total Mean f1 : {total_f1/(step+1):.4f}\")\n",
        "    print(f\"*****Epoch {epoch} Train Finish*****\\n\")\n",
        "\n",
        "    train_dict['f1'].append(total_f1/(step+1))\n",
        "    train_dict['loss'].append(total_loss/(step+1))\n",
        "    # train_dict['em'].append(total_em/(step+1))\n",
        "    \n",
        "    if valid_dataloader is not None:\n",
        "        print(f\"*****Epoch {epoch} Valid Start*****\")\n",
        "        valid_loss, valid_em, valid_f1, valid_acc = validate(model, valid_dataloader, f1_fct, em_fct, acc_fct)\n",
        "        print(f\"Epoch {epoch} Valid Loss : {valid_loss:.4f} Valid f1 : {valid_f1:.4f} Valid em : {valid_em:.4f} Valid acc : {valid_acc:.4f}\")\n",
        "        print(f\"*****Epoch {epoch} Valid Finish*****\\n\")\n",
        "\n",
        "    valid_dict['f1'].append(valid_f1)\n",
        "    valid_dict['loss'].append(valid_loss)\n",
        "    valid_dict['em'].append(valid_em)\n",
        "    if round(valid_f1, 4) == 0 :\n",
        "        break\n",
        "    # if before_loss > valid_loss :\n",
        "    #     before_loss = valid_loss\n",
        "    #     save_checkpoint(\"/content/drive/MyDrive/Colab Notebooks/nlp/qa\", model, optimizer, scheduler, epoch, valid_loss, valid_f1, model_name)\n",
        "\n",
        "    # elif before_f1 < valid_f1  :\n",
        "    #     before_f1 = valid_f1\n",
        "    #     save_checkpoint(\"/content/drive/MyDrive/Colab Notebooks/nlp/qa\", model, optimizer, scheduler, epoch, valid_loss, valid_f1, model_name)\n",
        "\n",
        "  print(\"Train Finished\")\n",
        "  return train_dict, valid_dict"
      ],
      "metadata": {
        "id": "0tx78MsnZzoL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### validation code"
      ],
      "metadata": {
        "id": "5iByxzyfaJ_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, valid_dataloader, f1_fct, em_fct, acc_fct):\n",
        "    loss_fct = nn.MSELoss()\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    \n",
        "    total_loss, total_em, total_f1, total_acc= 0,0, 0, 0\n",
        "        \n",
        "    for step, batch in enumerate(valid_dataloader):\n",
        "        \n",
        "        batch = tuple(item.to(device) for item in batch)\n",
        "            \n",
        "        batch_input, batch_label = batch\n",
        "            \n",
        "        # gradient 계산하지 않음\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch_input)\n",
        "            # outputs = torch.argmax(outputs, dim=2)\n",
        "\n",
        "        loss = loss_fct(outputs.float(), batch_label.float())\n",
        "        em_value = 0\n",
        "        for output, label in zip(outputs, batch_label) :\n",
        "          em = em_fct.compute(predictions=output, references=label)['exact_match']\n",
        "          em_value += em\n",
        "        em_value /= len(outputs)\n",
        "        em = em_value\n",
        "\n",
        "        f1 = f1_fct.compute(predictions=outputs.view([-1, 1]).squeeze(), references=batch_label.view([-1, 1]).squeeze())['f1']\n",
        "        acc = acc_fct.compute(predictions=outputs.view([-1, 1]).squeeze(), references=batch_label.view([-1, 1]).squeeze())['accuracy']\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        total_f1 += f1\n",
        "        total_em += em\n",
        "        total_acc += acc\n",
        "\n",
        "    total_loss = total_loss/(step+1)\n",
        "    total_em = total_em/(step+1)\n",
        "    total_f1 = total_f1/(step+1)\n",
        "    total_acc = total_acc/(step+1)\n",
        "    return total_loss, total_em, total_f1, total_acc"
      ],
      "metadata": {
        "id": "C0mOhdJxaJJN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### draw_plot"
      ],
      "metadata": {
        "id": "z7z2RcB8aMbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss와 f1-score의 변화를 epoch마다 보기 위한 plot\n",
        "def draw_plot(train_dict, valid_dict, i) :\n",
        "  print('green is loss, gray is f1')\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.title('Loss and F1 of Train data')\n",
        "  x_values= [n for n in range(len(train_dict['loss']))]\n",
        "  plt.plot(x_values, train_dict['loss'], color='green', marker='o')  # loss\n",
        "  plt.plot(x_values, train_dict['f1'], color='#AAAAAA', marker='*')  # f1\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.title('Loss and F1 of Validation data')\n",
        "  x_values= [n for n in range(len(valid_dict['loss']))]\n",
        "  plt.plot(x_values, valid_dict['loss'], color='green', marker='o')  # loss\n",
        "  plt.plot(x_values, valid_dict['f1'], color='#AAAAAA', marker='*')  # f1\n",
        "\n",
        "  plt.show()\n",
        "  plt.savefig(f'figure_{i}.png')"
      ],
      "metadata": {
        "id": "GB1JmBRkaNrs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'klue/bert-base'   # 다시 설정 필요\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "4YIfLFs9aTTU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(dataset['train'])\n",
        "valid_dataset = CustomDataset(dataset['validation'])"
      ],
      "metadata": {
        "id": "u7zOF7oMvtSE"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "XRje4KyFaj9t",
        "outputId": "1ccf1cdd-7b61-41a9-809e-180444f5d314",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1600"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = make_dataloader(train_dataset, model_name, 16, 'train')\n",
        "valid_dataloader = make_dataloader(valid_dataset, model_name, 8, 'valid')"
      ],
      "metadata": {
        "id": "0ye9Fwqve2LD",
        "outputId": "83d1d6c0-acd9-491f-e0f7-0fd03de3f64b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size : 16\n",
            "batch_size : 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 5e-3\n",
        "weight_decay = 4e-5\n",
        "model, optimizer, scheduler = initializer(train_dataloader, 10, model_name, learning_rate, weight_decay)\n",
        "start = time.time()"
      ],
      "metadata": {
        "id": "KzpgKfJfJDEh",
        "outputId": "4a189dca-6fee-40af-859e-7e87ff74fe84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total train steps with 10 epochs: 37760\n",
            "model_name : klue/bert-base, lr : 0.005, weight_decay : 4e-05, epochs : 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print_params(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nCV8-oeERVsP",
        "outputId": "ae13a45e-3a6c-4cda-fd34-585734398d5a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The BERT model has 203 different named parameters.\n",
            "\n",
            "==== Embedding Layer ====\n",
            "\n",
            "bert.embeddings.word_embeddings.weight                  (32000, 768)\n",
            "bert.embeddings.position_embeddings.weight                (512, 768)\n",
            "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
            "bert.embeddings.LayerNorm.weight                              (768,)\n",
            "bert.embeddings.LayerNorm.bias                                (768,)\n",
            "\n",
            "==== First Transformer ====\n",
            "\n",
            "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
            "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
            "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
            "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
            "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
            "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
            "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
            "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
            "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
            "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
            "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
            "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
            "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
            "\n",
            "==== Output Layer ====\n",
            "\n",
            "qa_output.0.weight                                        (768, 768)\n",
            "qa_output.0.bias                                              (768,)\n",
            "qa_output.2.weight                                        (128, 768)\n",
            "qa_output.2.bias                                              (128,)\n",
            "qa_output.4.weight                                          (2, 128)\n",
            "qa_output.4.bias                                                (2,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del train_dataset\n",
        "del valid_dataset\n",
        "del dataset"
      ],
      "metadata": {
        "id": "M0iKksS2ShWq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gc.collect()"
      ],
      "metadata": {
        "id": "30hb6iqNrYPC",
        "outputId": "52074f28-38b8-4876-a582-f8cf19dae71c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "100"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "metadata": {
        "id": "LA9hAnBnIOsg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dict, valid_dict = train(model, optimizer, scheduler, train_dataloader, valid_dataloader, 10)\n",
        "end = time.time()\n",
        "print(f\"time : {(end - start)//60}분 {(end - start)%60}초\")\n",
        "\n",
        "# draw_plot(train_dict, valid_dict, 0)"
      ],
      "metadata": {
        "id": "1-L2JHPFhZKU",
        "outputId": "4a033f96-f8d1-451d-acff-b0cdc552bdfe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****Epoch 0 Train Start*****\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (967 > 512). Running this sequence through the model will result in indexing errors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0, Step : 128, LR : 0.0049829184, Avg Loss : 1.9370, f1 score : 0.5944\n",
            "Epoch: 0, Step : 256, LR : 0.0049659693, Avg Loss : 0.0072, f1 score : 0.5471\n",
            "Epoch: 0, Step : 384, LR : 0.0049490201, Avg Loss : 0.0069, f1 score : 0.5657\n",
            "Epoch: 0, Step : 512, LR : 0.0049320710, Avg Loss : 0.0066, f1 score : 0.5662\n",
            "Epoch: 0, Step : 640, LR : 0.0049151218, Avg Loss : 0.0070, f1 score : 0.5760\n",
            "Epoch: 0, Step : 768, LR : 0.0048981727, Avg Loss : 0.0070, f1 score : 0.5543\n",
            "Epoch: 0, Step : 896, LR : 0.0048812235, Avg Loss : 0.0070, f1 score : 0.5712\n",
            "Epoch: 0, Step : 1024, LR : 0.0048642744, Avg Loss : 0.0069, f1 score : 0.5782\n",
            "Epoch: 0, Step : 1152, LR : 0.0048473252, Avg Loss : 0.0069, f1 score : 0.5509\n",
            "Epoch: 0, Step : 1280, LR : 0.0048303761, Avg Loss : 0.0068, f1 score : 0.5793\n",
            "Epoch: 0, Step : 1408, LR : 0.0048134269, Avg Loss : 0.0069, f1 score : 0.5716\n",
            "Epoch: 0, Step : 1536, LR : 0.0047964778, Avg Loss : 0.0068, f1 score : 0.5449\n",
            "Epoch: 0, Step : 1664, LR : 0.0047795286, Avg Loss : 0.0069, f1 score : 0.5903\n",
            "Epoch: 0, Step : 1792, LR : 0.0047625794, Avg Loss : 0.0069, f1 score : 0.5743\n",
            "Epoch: 0, Step : 1920, LR : 0.0047456303, Avg Loss : 0.0069, f1 score : 0.5612\n",
            "Epoch: 0, Step : 2048, LR : 0.0047286811, Avg Loss : 0.0068, f1 score : 0.5492\n",
            "Epoch: 0, Step : 2176, LR : 0.0047117320, Avg Loss : 0.0070, f1 score : 0.5335\n",
            "Epoch: 0, Step : 2304, LR : 0.0046947828, Avg Loss : 0.0070, f1 score : 0.5201\n",
            "Epoch: 0, Step : 2432, LR : 0.0046778337, Avg Loss : 0.0069, f1 score : 0.5469\n",
            "Epoch: 0, Step : 2560, LR : 0.0046608845, Avg Loss : 0.0068, f1 score : 0.5365\n",
            "Epoch: 0, Step : 2688, LR : 0.0046439354, Avg Loss : 0.0068, f1 score : 0.5118\n",
            "Epoch: 0, Step : 2816, LR : 0.0046269862, Avg Loss : 0.0070, f1 score : 0.5327\n",
            "Epoch: 0, Step : 2944, LR : 0.0046100371, Avg Loss : 0.0069, f1 score : 0.5302\n",
            "Epoch: 0, Step : 3072, LR : 0.0045930879, Avg Loss : 0.0065, f1 score : 0.5146\n",
            "Epoch: 0, Step : 3200, LR : 0.0045761388, Avg Loss : 0.0066, f1 score : 0.5250\n",
            "Epoch: 0, Step : 3328, LR : 0.0045591896, Avg Loss : 0.0068, f1 score : 0.4933\n",
            "Epoch: 0, Step : 3456, LR : 0.0045422405, Avg Loss : 0.0069, f1 score : 0.5335\n",
            "Epoch: 0, Step : 3584, LR : 0.0045252913, Avg Loss : 0.0070, f1 score : 0.5453\n",
            "Epoch: 0, Step : 3712, LR : 0.0045083422, Avg Loss : 0.0067, f1 score : 0.4962\n",
            "Epoch 0 Total Mean Loss : 0.0728\n",
            "Epoch 0 Total Mean f1 : 0.5474\n",
            "*****Epoch 0 Train Finish*****\n",
            "\n",
            "*****Epoch 0 Valid Start*****\n",
            "Epoch 0 Valid Loss : 0.0103 Valid f1 : 0.9902 Valid em : 0.0000 Valid acc : 0.9902\n",
            "*****Epoch 0 Valid Finish*****\n",
            "\n",
            "*****Epoch 1 Train Start*****\n"
          ]
        }
      ]
    }
  ]
}