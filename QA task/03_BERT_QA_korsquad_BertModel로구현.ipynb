{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_BERT_QA_squad_BertForQuestionAnswering.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNEL7ylz+lNOPF37/cf8hag",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06f2d7dbe00f4a48925c1a7fe5922f46": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_474e93972ef14721957fdaf86363b3ce",
              "IPY_MODEL_c3d4c3f3fe5a4f4799524883bdac06da",
              "IPY_MODEL_71a07cbfdc5847aca325cfd051c851a2"
            ],
            "layout": "IPY_MODEL_99ca39aff80d4bceaa5ab1b2eb6e81bf"
          }
        },
        "474e93972ef14721957fdaf86363b3ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dffffadf712c4851bffde978b87baada",
            "placeholder": "​",
            "style": "IPY_MODEL_2d0038dc85af43d9a438e8d1cc71cbd7",
            "value": "100%"
          }
        },
        "c3d4c3f3fe5a4f4799524883bdac06da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66c0e743faa3413a8807f3a1cef962f3",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_237477f3cfa346dab0466ddf5e9f0eaf",
            "value": 2
          }
        },
        "71a07cbfdc5847aca325cfd051c851a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da4c5aa8ae31454180b2b611e86257b2",
            "placeholder": "​",
            "style": "IPY_MODEL_54a1f7f5ae4d4631bcfbfceecc14020c",
            "value": " 2/2 [00:00&lt;00:00, 44.23it/s]"
          }
        },
        "99ca39aff80d4bceaa5ab1b2eb6e81bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dffffadf712c4851bffde978b87baada": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d0038dc85af43d9a438e8d1cc71cbd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66c0e743faa3413a8807f3a1cef962f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "237477f3cfa346dab0466ddf5e9f0eaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da4c5aa8ae31454180b2b611e86257b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54a1f7f5ae4d4631bcfbfceecc14020c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttogle918/AI_practice/blob/main/QA%20task/03_BERT_QA_korsquad_BertModel%EB%A1%9C%EA%B5%AC%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# korsquad를 사용하여 QA task 풀기(BertModel fine-tuning)\n",
        "\n",
        "참고 사이트\n",
        "\n",
        "[huggingface QA 설명](https://huggingface.co/course/chapter7/7?fw=tf)\n",
        "\n",
        "[huggingface git : ModelOutput](https://github.com/huggingface/transformers/blob/v4.21.0/src/transformers/utils/generic.py#L147)\n",
        "\n",
        "[huggingface git : QuestionAnsweringModelOutput](https://github.com/huggingface/transformers/blob/a9eee2ffecc874df7dd635b2c6abb246fdb318cc/src/transformers/modeling_outputs.py#L764)\n",
        "\n",
        "[huggingface docs](https://huggingface.co/docs/transformers/model_doc/bert)"
      ],
      "metadata": {
        "id": "QocCi_jhZ-ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers\n",
        "# !pip install datasets"
      ],
      "metadata": {
        "id": "62Oa6eDdlfHv"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Qu0sRPezlVoA"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric#, list_metrics\n",
        "\n",
        "from transformers import BertModel, AutoTokenizer, BertConfig, BertPreTrainedModel\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers.modeling_outputs import QuestionAnsweringModelOutput\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "1N7PgGshrtGh"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gpu 연산이 가능하면 'cuda:0', 아니면 'cpu' 출력\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device, torch.cuda.device_count()"
      ],
      "metadata": {
        "id": "NMC8y1NUSDHP",
        "outputId": "14be80bd-269a-412e-9d2e-4418c48b93d5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(device(type='cuda', index=0), 1)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Load"
      ],
      "metadata": {
        "id": "50EDgWKELGI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/datasets/squad_kor_v1/blob/main/squad_kor_v1.py\n",
        "# squad_kor_v2\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('squad_kor_v1')\n",
        "dataset, dataset['train'][0]"
      ],
      "metadata": {
        "id": "STFveXBu634F",
        "outputId": "4cbca02d-9fad-457f-e986-ba35da8550f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530,
          "referenced_widgets": [
            "06f2d7dbe00f4a48925c1a7fe5922f46",
            "474e93972ef14721957fdaf86363b3ce",
            "c3d4c3f3fe5a4f4799524883bdac06da",
            "71a07cbfdc5847aca325cfd051c851a2",
            "99ca39aff80d4bceaa5ab1b2eb6e81bf",
            "dffffadf712c4851bffde978b87baada",
            "2d0038dc85af43d9a438e8d1cc71cbd7",
            "66c0e743faa3413a8807f3a1cef962f3",
            "237477f3cfa346dab0466ddf5e9f0eaf",
            "da4c5aa8ae31454180b2b611e86257b2",
            "54a1f7f5ae4d4631bcfbfceecc14020c"
          ]
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset squad_kor_v1 (/root/.cache/huggingface/datasets/squad_kor_v1/squad_kor_v1/1.0.0/18d4f44736b8ee85671f63cb84965bfb583fa0a4ff2df3c2e10eee9693796725)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "06f2d7dbe00f4a48925c1a7fe5922f46"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 60407\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 5774\n",
              "    })\n",
              "}),\n",
              " {'answers': {'answer_start': [54], 'text': ['교향곡']},\n",
              "  'context': '1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.',\n",
              "  'id': '6566495-0-0',\n",
              "  'question': '바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?',\n",
              "  'title': '파우스트_서곡'})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.context, self.question, self.answer_start, self.answer_text = self.make_dataset(dataset)\n",
        "\n",
        "    def make_dataset(self, dataset):\n",
        "        context, question, answer_start, answer_text = [], [], [], []\n",
        "        global max_length\n",
        "        for i, data in enumerate(dataset) :\n",
        "          start = data['answers']['answer_start']\n",
        "          if len(answer_start) != 1 : # 답이 없을 때\n",
        "            print(i, data)\n",
        "            continue\n",
        "          answer_start.append(start)\n",
        "          answer_text.append(data['answers']['text'])\n",
        "          # answers.append([answer_start[0], answer_start[0] + len(text)])  # 정답의 시작과 끝 index\n",
        "          context.append(data['context'])\n",
        "          question.append(data['question'])\n",
        "        return question, context, answer_start, answer_text\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.question)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.context[idx], self.question[idx], self.answer_start[idx], self.answer_text[idx]"
      ],
      "metadata": {
        "id": "Amz1l80dSL1m"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a, b = torch.tensor([0,1,2]), torch.tensor([4,5,6])\n",
        "torch.stack([a,b], dim=-1)"
      ],
      "metadata": {
        "id": "AlzGD_YRsjVN",
        "outputId": "f964495d-e562-45c8-a288-31a435f49258",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 4],\n",
              "        [1, 5],\n",
              "        [2, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer('1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다.', return_tensors='pt').input_ids[0,9]"
      ],
      "metadata": {
        "id": "03UjdR3LmyiF",
        "outputId": "c27f443c-ddec-4331-ea44-5d9c7d5b769a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(3791)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch):\n",
        "    global tokenizer\n",
        "    input1_list, input2_list, answer_start, answer_text = [], [], [], []\n",
        "\n",
        "    for _input1, _input2, _start, _text in batch:\n",
        "        input1_list.append(_input1)\n",
        "        input2_list.append(_input2)\n",
        "        answer_text.append(_text)\n",
        "        answer_start.append(_start)\n",
        "    \n",
        "    tensorized_input = tokenizer( # max length 구하기! dim=1\n",
        "        input1_list, input2_list,\n",
        "        add_special_tokens=True,\n",
        "        padding=\"longest\",  # 배치내 가장 긴 문장을 기준으로 부족한 문장은 [PAD] 토큰을 추가\n",
        "        return_tensors='pt',\n",
        "        max_length=128,   # context가 있는 질문이 너무 길 때,\n",
        "        truncation=True\n",
        "    )\n",
        "    tensorized_label = torch.zeros(tensorized_input.input_ids.shape)    # input_ids만큼의 길이이고 0으로 된 tensor\n",
        "\n",
        "    before_token_text = tokenizer.encode(input2_list[:answer_start], return_tensors='pt') # 본문 ~ start_idx \n",
        "    answer_token = tokenizer.encode(answer_text, return_tensors='pt')\n",
        "    answer_token = torch.stack([before_token_text, answer_token], dim=-1)   # [[start_idx, end_idx], ...]\n",
        "    # 시작 index와 끝 index에 1\n",
        "    for i, ans in enumerate(answer_token) :\n",
        "        tensorized_label[i, len(ans[0])], tensorized_label[i, len(ans[1])-1] = 1, 1\n",
        "\n",
        "    return tensorized_input, tensorized_label"
      ],
      "metadata": {
        "id": "H8bLGE3dVVR8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataloader(dataset, tokenizer, batch_size, s='train') :\n",
        "  dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size =batch_size,\n",
        "      sampler = RandomSampler(dataset) if s == 'train' else SequentialSampler(dataset),\n",
        "      collate_fn = custom_collate_fn\n",
        "  )\n",
        "  print(f'batch_size : {batch_size}')\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "1eTDFIVxYu73"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 설명\n"
      ],
      "metadata": {
        "id": "IJoPh3xpqUKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomBertForQuestionAnswering(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\n",
        "        self.qa_output = nn.Linear(config.hidden_size, config.num_labels)\n",
        "        self.loss_fct = CrossEntropyLoss()\n",
        "\n",
        "        self.post_init()\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None,\n",
        "                positions=None, output_attentions=None, output_hidden_states=None):\n",
        "        \n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids,\n",
        "                            head_mask=head_mask, inputs_embeds=inputs_embeds,\n",
        "                  output_attentions=output_attentions, output_hidden_states=output_hidden_states)\n",
        "        \n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.qa_output(sequence_output)    # linear 통과해서 num_label로 분류\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1).contiguous()\n",
        "        end_logits = end_logits.squeeze(-1).contiguous()\n",
        "\n",
        "        total_loss = None\n",
        "        start_positions, end_positions = positions.split(1, dim=-1)\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "          if len(start_positions.size()) > 1:\n",
        "              start_positions = start_positions.squeeze(-1)\n",
        "          if len(end_positions.size()) > 1:\n",
        "              end_positions = end_positions.squeeze(-1)\n",
        "\n",
        "          ignored_index = start_logits.size(1)\n",
        "          start_positions = start_positions.clamp(0, ignored_index)\n",
        "          end_positions = end_positions.clamp(0, ignored_index)\n",
        "\n",
        "          start_logits_idx, end_logits_idx = torch.argmax(start_logits, dim=-1).float(), torch.tensor(end_logits.argmax(dim=-1), dtype=torch.float16)\n",
        "          start_loss = self.loss_fct(start_logits_idx, start_positions.float())\n",
        "          end_loss = self.loss_fct(end_logits_idx, end_positions.float())\n",
        "          total_loss = (start_loss + end_loss) / 2\n",
        "          total_loss.requires_grad_(True)\n",
        "\n",
        "        return QuestionAnsweringModelOutput(loss=total_loss, \n",
        "                                            start_logits=start_logits_idx, \n",
        "                                            end_logits=start_logits_idx,\n",
        "                                            hidden_states=outputs.hidden_states, \n",
        "                                            attentions=outputs.attentions)\n"
      ],
      "metadata": {
        "id": "AyiUyiJ6Mtse"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train"
      ],
      "metadata": {
        "id": "tZXfyh4_Mta4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initializer(train_dataloader, epochs=2, model_name='klue/bert-base', lr=4e-5, wd=4e-5):\n",
        "    \"\"\"\n",
        "    모델, 옵티마이저, 스케쥴러 초기화\n",
        "    \"\"\"\n",
        "    config = BertConfig.from_pretrained(model_name)\n",
        "    config.max_length = 512\n",
        "    model = CustomBertForQuestionAnswering(config)\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(), # update 대상 파라미터를 입력\n",
        "        lr=lr,    # 2e-5\n",
        "        eps=1e-8,\n",
        "        weight_decay=wd\n",
        "    )\n",
        "    \n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    print(f\"Total train steps with {epochs} epochs: {total_steps}\")\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, \n",
        "        num_warmup_steps = 0, # 여기서는 warmup을 사용하지 않는다.\n",
        "        num_training_steps = total_steps\n",
        "    )\n",
        "    print(f'model_name : {model_name}, lr : {lr}, weight_decay : {wd}, epochs : {epochs}')\n",
        "    return model, optimizer, scheduler"
      ],
      "metadata": {
        "id": "GDcl_0m3Zj0f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(path, model, optimizer, scheduler, epoch, loss, f1, model_name=''):\n",
        "    file_name = f'{path}/epoch:{epoch}_loss:{loss:.4f}_f1:{f1:.4f}.ckpt'\n",
        "    \n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'loss' : loss,\n",
        "            'f1' : f1\n",
        "        }, \n",
        "        file_name\n",
        "    )\n",
        "    \n",
        "    print(f\"Saving epoch {epoch} checkpoint at {file_name}\")"
      ],
      "metadata": {
        "id": "TnpT2s-qZnG-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train code"
      ],
      "metadata": {
        "id": "ut6WMU93Z0in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, scheduler, train_dataloader, valid_dataloader=None, epochs=1):\n",
        "  loss_fct = nn.MSELoss()\n",
        "  em_fct = load_metric('exact_match')\n",
        "  f1_fct = load_metric('f1')\n",
        "\n",
        "\n",
        "  train_dict = {'loss' : [], 'f1' : []}\n",
        "  valid_dict = {'loss' : [], 'f1' : [], 'em' : []}\n",
        "\n",
        "  for epoch in range(epochs) :\n",
        "\n",
        "    print(f\"*****Epoch {epoch} Train Start*****\")\n",
        "    total_loss, total_f1, batch_f1, batch_em, total_em, batch_loss, batch_count = 0,0,0,0,0,0,0\n",
        "    \n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    \n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      batch_count+=1\n",
        "      \n",
        "      batch = tuple(item.to(device) for item in batch)\n",
        "      batch_input, batch_label = batch\n",
        "      \n",
        "      model.zero_grad()\n",
        "      \n",
        "      outputs = model(**batch_input, positions=batch_label)  # forward\n",
        "      # start_logits_idx, end_logits_idx = torch.argmax(outputs.start_logits, dim=-1).float16(), torch.argmax(outputs.end_logits, dim=-1).float16()\n",
        "\n",
        "      start_pos, end_pos = batch_label.split(1, dim=-1)\n",
        "      if start_pos is not None and end_pos is not None:\n",
        "        if len(start_pos.size()) > 1:\n",
        "            start_pos = start_pos.squeeze(-1)\n",
        "        if len(end_pos.size()) > 1:\n",
        "            end_pos = end_pos.squeeze(-1)\n",
        "\n",
        "        ignored_index = outputs.start_logits.size(0)\n",
        "        start_pos = start_pos.clamp(0, ignored_index)\n",
        "        end_pos = end_pos.clamp(0, ignored_index)\n",
        "\n",
        "      # start_loss = loss_fct(start_logits_idx, start_pos.float())\n",
        "      # end_loss = loss_fct(end_logits_idx, end_pos.float())\n",
        "      # loss = (start_loss + end_loss) / 2\n",
        "      \n",
        "      loss = outputs.loss\n",
        "      batch_loss += loss.item()\n",
        "      total_loss += loss.item()\n",
        "\n",
        "\n",
        "      # start_pos, end_pos = batch_label.split(1, dim=-1)\n",
        "      # if start_pos is not None and end_pos is not None:\n",
        "      #   if len(start_pos.size()) > 1:\n",
        "      #       start_pos = start_pos.squeeze(-1)\n",
        "      #   if len(end_pos.size()) > 1:\n",
        "      #       end_pos = end_pos.squeeze(-1)\n",
        "      # ignored_index = probs['start_logits_idx'].size(0)\n",
        "      # start_pos = start_pos.clamp(0, ignored_index)\n",
        "      # end_pos = end_pos.clamp(0, ignored_index)\n",
        "\n",
        "      # probs_idx = torch.stack([probs['start_logits_idx'], probs['end_logits_idx']], dim=-1)\n",
        "      # em = em_fct.compute(predictions=torch.stack([start_logits_idx, end_logits_idx], dim=-1),\n",
        "      #                       references=batch_label)['exact_match']\n",
        "\n",
        "      # batch_em += em\n",
        "      # total_em += em\n",
        "      \n",
        "      start_results = f1_fct.compute(predictions=outputs.start_logits, references=start_pos, average='micro')['f1']\n",
        "      end_results = f1_fct.compute(predictions=outputs.end_logits, references=end_pos, average='micro')['f1']\n",
        "\n",
        "      f1 = (start_results + end_results)/2\n",
        "      batch_f1 += f1\n",
        "      total_f1 += f1\n",
        "\n",
        "      # backward -> 파라미터의 미분(gradient)를 자동으로 계산\n",
        "      loss.backward()\n",
        "\n",
        "      # gradient clipping 적용 \n",
        "      clip_grad_norm_(model.parameters(), 1.0)\n",
        "      \n",
        "      # optimizer & scheduler 업데이트\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "\n",
        "      # 그래디언트 초기화\n",
        "      model.zero_grad()\n",
        "\n",
        "      if (step % 128 == 0 and step != 0):\n",
        "          learning_rate = optimizer.param_groups[0]['lr']\n",
        "          print(f\"Epoch: {epoch}, Step : {step}, LR : {learning_rate:.10f}, Avg Loss : {batch_loss / batch_count:.4f}, f1 score : {batch_f1 / batch_count:.4f}\")\n",
        "          \n",
        "          if (round(batch_f1 / batch_count, 5) == 0) and (round(learning_rate, 10) == 0) :\n",
        "              print(\"Train Finished, learning_rate is 0 and train_f1 is 0\")\n",
        "              return train_dict, valid_dict\n",
        "\n",
        "          batch_loss, batch_f1, batch_count = 0,0,0\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch} Total Mean Loss : {total_loss/(step+1):.4f}\")\n",
        "    print(f\"Epoch {epoch} Total Mean f1 : {total_f1/(step+1):.4f}\")\n",
        "    print(f\"*****Epoch {epoch} Train Finish*****\\n\")\n",
        "\n",
        "    train_dict['f1'].append(total_f1/(step+1))\n",
        "    train_dict['loss'].append(total_loss/(step+1))\n",
        "    # train_dict['em'].append(total_em/(step+1))\n",
        "    \n",
        "    if valid_dataloader is not None:\n",
        "        print(f\"*****Epoch {epoch} Valid Start*****\")\n",
        "        valid_loss, valid_em, valid_f1 = validate(model, valid_dataloader, f1_fct, em_fct)\n",
        "        print(f\"Epoch {epoch} Valid Loss : {valid_loss:.4f} Valid f1 : {valid_f1:.4f} Valid em : {valid_em:.4f}\")\n",
        "        print(f\"*****Epoch {epoch} Valid Finish*****\\n\")\n",
        "\n",
        "    valid_dict['f1'].append(valid_f1)\n",
        "    valid_dict['loss'].append(valid_loss)\n",
        "    valid_dict['em'].append(valid_em)\n",
        "    if round(valid_f1, 4) == 0 :\n",
        "        break\n",
        "    # if before_loss > valid_loss :\n",
        "    #     before_loss = valid_loss\n",
        "    #     save_checkpoint(\"/content/drive/MyDrive/Colab Notebooks/nlp/qa\", model, optimizer, scheduler, epoch, valid_loss, valid_f1, model_name)\n",
        "\n",
        "    # elif before_f1 < valid_f1  :\n",
        "    #     before_f1 = valid_f1\n",
        "    #     save_checkpoint(\"/content/drive/MyDrive/Colab Notebooks/nlp/qa\", model, optimizer, scheduler, epoch, valid_loss, valid_f1, model_name)\n",
        "\n",
        "  print(\"Train Finished\")\n",
        "  return train_dict, valid_dict"
      ],
      "metadata": {
        "id": "0tx78MsnZzoL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### validation code"
      ],
      "metadata": {
        "id": "5iByxzyfaJ_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, valid_dataloader, f1_fct, em_fct):\n",
        "    loss_fct = nn.MSELoss()\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    \n",
        "    total_loss, total_em, total_f1= 0,0, 0\n",
        "        \n",
        "    for step, batch in enumerate(valid_dataloader):\n",
        "        \n",
        "        batch = tuple(item.to(device) for item in batch)\n",
        "            \n",
        "        batch_input, batch_label = batch\n",
        "            \n",
        "        # gradient 계산하지 않음\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch_input, positions=batch_label)\n",
        "            # start_logit_idx, end_logit_idx = torch.argmax(outputs.start_logits, dim=-1).float16(), torch.argmax(outputs.end_logits, dim=-1).float16()\n",
        "            \n",
        "        start_pos, end_pos = batch_label.split(1, dim=-1)\n",
        "        if start_pos is not None and end_pos is not None:\n",
        "          if len(start_pos.size()) > 1:\n",
        "              start_pos = start_pos.squeeze(-1)\n",
        "          if len(end_pos.size()) > 1:\n",
        "              end_pos = end_pos.squeeze(-1)\n",
        "        ignored_index = outputs.end_logits.size(0)\n",
        "        start_pos = start_pos.clamp(0, ignored_index)\n",
        "        end_pos = end_pos.clamp(0, ignored_index)\n",
        "        \n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        em = em_fct.compute(predictions=torch.stack([outputs.start_logits, outputs.end_logits], dim=-1), \n",
        "                            references=batch_label)\n",
        "        total_em += em['exact_match']\n",
        "        \n",
        "        start_results = f1_fct.compute(predictions=outputs.start_logits, references=start_pos, average='micro')['f1']\n",
        "        end_results = f1_fct.compute(predictions=outputs.end_logits, references=end_pos, average='micro')['f1']\n",
        "        f1 = (start_results + end_results)/2\n",
        "        total_f1 += f1\n",
        "\n",
        "    total_loss = total_loss/(step+1)\n",
        "    total_em = total_em/(step+1)\n",
        "    total_f1 = total_f1/(step+1)\n",
        "    return total_loss, total_em, total_f1"
      ],
      "metadata": {
        "id": "C0mOhdJxaJJN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### draw_plot"
      ],
      "metadata": {
        "id": "z7z2RcB8aMbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss와 f1-score의 변화를 epoch마다 보기 위한 plot\n",
        "def draw_plot(train_dict, valid_dict, i) :\n",
        "  print('green is loss, gray is f1')\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.title('Loss and F1 of Train data')\n",
        "  x_values= [n for n in range(len(train_dict['loss']))]\n",
        "  plt.plot(x_values, train_dict['loss'], color='green', marker='o')  # loss\n",
        "  plt.plot(x_values, train_dict['f1'], color='#AAAAAA', marker='*')  # f1\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.title('Loss and F1 of Validation data')\n",
        "  x_values= [n for n in range(len(valid_dict['loss']))]\n",
        "  plt.plot(x_values, valid_dict['loss'], color='green', marker='o')  # loss\n",
        "  plt.plot(x_values, valid_dict['f1'], color='#AAAAAA', marker='*')  # f1\n",
        "\n",
        "  plt.show()\n",
        "  plt.savefig(f'figure_{i}.png')"
      ],
      "metadata": {
        "id": "GB1JmBRkaNrs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'klue/bert-base'   # 다시 설정 필요\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "train_dataset = CustomDataset(dataset['train'])\n",
        "valid_dataset = CustomDataset(dataset['validation'])\n",
        "\n",
        "del dataset"
      ],
      "metadata": {
        "id": "4YIfLFs9aTTU"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "XRje4KyFaj9t",
        "outputId": "0c1c31c8-0702-4a0b-a842-ef00cf52fbf2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1483"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = make_dataloader(train_dataset, model_name, 16, 'train')\n",
        "valid_dataloader = make_dataloader(valid_dataset, model_name, 8, 'valid')\n",
        "\n",
        "learning_rate = 5e-5\n",
        "weight_decay = 4e-5\n",
        "model, optimizer, scheduler = initializer(train_dataloader, 4, model_name, learning_rate, weight_decay)\n",
        "start = time.time()\n"
      ],
      "metadata": {
        "id": "0ye9Fwqve2LD",
        "outputId": "c1ce5c91-95b0-4d49-e986-db37cb52b437",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size : 16\n",
            "batch_size : 8\n",
            "Total train steps with 4 epochs: 15104\n",
            "model_name : klue/bert-base, lr : 5e-05, weight_decay : 4e-05, epochs : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "del train_dataset\n",
        "del valid_dataset"
      ],
      "metadata": {
        "id": "M0iKksS2ShWq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "30hb6iqNrYPC",
        "outputId": "0c8038d4-3654-479e-c80e-50ca848f58e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "metadata": {
        "id": "LA9hAnBnIOsg"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dict, valid_dict = train(model, optimizer, scheduler, train_dataloader, valid_dataloader, 4)\n",
        "end = time.time()\n",
        "print(f\"time : {(end - start)//60}분 {(end - start)%60}초\")\n",
        "\n",
        "# draw_plot(train_dict, valid_dict, 0)"
      ],
      "metadata": {
        "id": "1-L2JHPFhZKU",
        "outputId": "41818c92-4311-4d08-bbd8-e174a61c6491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****Epoch 0 Train Start*****\n",
            "Epoch: 0, Step : 128, LR : 0.0000370730, Avg Loss : 559556.4972, f1 score : 0.0000\n",
            "Epoch: 0, Step : 256, LR : 0.0000366492, Avg Loss : 531714.2206, f1 score : 0.0005\n",
            "Epoch: 0, Step : 384, LR : 0.0000362255, Avg Loss : 568919.5496, f1 score : 0.0022\n",
            "Epoch: 0, Step : 512, LR : 0.0000358018, Avg Loss : 544838.4856, f1 score : 0.0007\n",
            "Epoch: 0, Step : 640, LR : 0.0000353780, Avg Loss : 584851.4816, f1 score : 0.0005\n",
            "Epoch: 0, Step : 768, LR : 0.0000349543, Avg Loss : 551967.8732, f1 score : 0.0002\n",
            "Epoch: 0, Step : 896, LR : 0.0000345306, Avg Loss : 544566.5043, f1 score : 0.0005\n",
            "Epoch: 0, Step : 1024, LR : 0.0000341069, Avg Loss : 559160.8174, f1 score : 0.0007\n",
            "Epoch: 0, Step : 1152, LR : 0.0000336831, Avg Loss : 561093.9429, f1 score : 0.0010\n",
            "Epoch: 0, Step : 1280, LR : 0.0000332594, Avg Loss : 523954.7056, f1 score : 0.0024\n",
            "Epoch: 0, Step : 1408, LR : 0.0000328357, Avg Loss : 566050.6188, f1 score : 0.0000\n",
            "Epoch: 0, Step : 1536, LR : 0.0000324119, Avg Loss : 616570.4418, f1 score : 0.0000\n",
            "Epoch: 0, Step : 1664, LR : 0.0000319882, Avg Loss : 571505.1260, f1 score : 0.0007\n",
            "Epoch: 0, Step : 1792, LR : 0.0000315645, Avg Loss : 558446.9888, f1 score : 0.0010\n",
            "Epoch: 0, Step : 1920, LR : 0.0000311408, Avg Loss : 543406.5389, f1 score : 0.0005\n",
            "Epoch: 0, Step : 2048, LR : 0.0000307170, Avg Loss : 552863.3674, f1 score : 0.0007\n",
            "Epoch: 0, Step : 2176, LR : 0.0000302933, Avg Loss : 568954.1434, f1 score : 0.0015\n",
            "Epoch: 0, Step : 2304, LR : 0.0000298696, Avg Loss : 585102.3337, f1 score : 0.0007\n",
            "Epoch: 0, Step : 2432, LR : 0.0000294458, Avg Loss : 556427.5887, f1 score : 0.0017\n",
            "Epoch: 0, Step : 2560, LR : 0.0000290221, Avg Loss : 586303.2214, f1 score : 0.0000\n",
            "Epoch: 0, Step : 2688, LR : 0.0000285984, Avg Loss : 562093.1567, f1 score : 0.0017\n",
            "Epoch: 0, Step : 2816, LR : 0.0000281747, Avg Loss : 565016.6960, f1 score : 0.0000\n",
            "Epoch: 0, Step : 2944, LR : 0.0000277509, Avg Loss : 577308.6141, f1 score : 0.0010\n",
            "Epoch: 0, Step : 3072, LR : 0.0000273272, Avg Loss : 568966.2322, f1 score : 0.0012\n",
            "Epoch: 0, Step : 3200, LR : 0.0000269035, Avg Loss : 594047.5979, f1 score : 0.0007\n",
            "Epoch: 0, Step : 3328, LR : 0.0000264797, Avg Loss : 531348.0811, f1 score : 0.0007\n",
            "Epoch: 0, Step : 3456, LR : 0.0000260560, Avg Loss : 547591.8610, f1 score : 0.0015\n",
            "Epoch: 0, Step : 3584, LR : 0.0000256323, Avg Loss : 543152.8743, f1 score : 0.0020\n",
            "Epoch: 0, Step : 3712, LR : 0.0000252086, Avg Loss : 582938.6180, f1 score : 0.0002\n",
            "Epoch 0 Total Mean Loss : 562115.5202\n",
            "Epoch 0 Total Mean f1 : 0.0009\n",
            "*****Epoch 0 Train Finish*****\n",
            "\n",
            "*****Epoch 0 Valid Start*****\n",
            "Epoch 0 Valid Loss : 154458.0493 Valid f1 : 0.0001 Valid em : 0.0000\n",
            "*****Epoch 0 Valid Finish*****\n",
            "\n",
            "*****Epoch 1 Train Start*****\n",
            "Epoch: 1, Step : 128, LR : 0.0000245730, Avg Loss : 546296.9008, f1 score : 0.0000\n",
            "Epoch: 1, Step : 256, LR : 0.0000241492, Avg Loss : 567516.9204, f1 score : 0.0010\n",
            "Epoch: 1, Step : 384, LR : 0.0000237255, Avg Loss : 574306.1903, f1 score : 0.0007\n",
            "Epoch: 1, Step : 512, LR : 0.0000233018, Avg Loss : 543443.1831, f1 score : 0.0012\n",
            "Epoch: 1, Step : 640, LR : 0.0000228780, Avg Loss : 569175.1370, f1 score : 0.0020\n",
            "Epoch: 1, Step : 768, LR : 0.0000224543, Avg Loss : 591472.4955, f1 score : 0.0005\n",
            "Epoch: 1, Step : 896, LR : 0.0000220306, Avg Loss : 583334.8469, f1 score : 0.0010\n",
            "Epoch: 1, Step : 1024, LR : 0.0000216069, Avg Loss : 616493.9297, f1 score : 0.0005\n",
            "Epoch: 1, Step : 1152, LR : 0.0000211831, Avg Loss : 535906.4286, f1 score : 0.0024\n",
            "Epoch: 1, Step : 1280, LR : 0.0000207594, Avg Loss : 539977.5632, f1 score : 0.0027\n",
            "Epoch: 1, Step : 1408, LR : 0.0000203357, Avg Loss : 564939.9321, f1 score : 0.0015\n",
            "Epoch: 1, Step : 1536, LR : 0.0000199119, Avg Loss : 544599.8450, f1 score : 0.0005\n",
            "Epoch: 1, Step : 1664, LR : 0.0000194882, Avg Loss : 516507.5815, f1 score : 0.0010\n",
            "Epoch: 1, Step : 1792, LR : 0.0000190645, Avg Loss : 547023.4609, f1 score : 0.0015\n",
            "Epoch: 1, Step : 1920, LR : 0.0000186408, Avg Loss : 591405.5308, f1 score : 0.0000\n",
            "Epoch: 1, Step : 2048, LR : 0.0000182170, Avg Loss : 572721.4536, f1 score : 0.0017\n",
            "Epoch: 1, Step : 2176, LR : 0.0000177933, Avg Loss : 534988.6035, f1 score : 0.0005\n",
            "Epoch: 1, Step : 2304, LR : 0.0000173696, Avg Loss : 579402.8401, f1 score : 0.0005\n",
            "Epoch: 1, Step : 2432, LR : 0.0000169458, Avg Loss : 546354.7593, f1 score : 0.0002\n",
            "Epoch: 1, Step : 2560, LR : 0.0000165221, Avg Loss : 585708.6035, f1 score : 0.0007\n",
            "Epoch: 1, Step : 2688, LR : 0.0000160984, Avg Loss : 556460.1726, f1 score : 0.0015\n",
            "Epoch: 1, Step : 2816, LR : 0.0000156747, Avg Loss : 557822.5363, f1 score : 0.0012\n",
            "Epoch: 1, Step : 2944, LR : 0.0000152509, Avg Loss : 560462.5646, f1 score : 0.0002\n",
            "Epoch: 1, Step : 3072, LR : 0.0000148272, Avg Loss : 582397.4939, f1 score : 0.0017\n",
            "Epoch: 1, Step : 3200, LR : 0.0000144035, Avg Loss : 587137.0718, f1 score : 0.0015\n",
            "Epoch: 1, Step : 3328, LR : 0.0000139797, Avg Loss : 572326.7604, f1 score : 0.0015\n",
            "Epoch: 1, Step : 3456, LR : 0.0000135560, Avg Loss : 560981.0422, f1 score : 0.0007\n",
            "Epoch: 1, Step : 3584, LR : 0.0000131323, Avg Loss : 559459.1140, f1 score : 0.0002\n",
            "Epoch: 1, Step : 3712, LR : 0.0000127086, Avg Loss : 567589.6980, f1 score : 0.0015\n",
            "Epoch 1 Total Mean Loss : 563534.2482\n",
            "Epoch 1 Total Mean f1 : 0.0010\n",
            "*****Epoch 1 Train Finish*****\n",
            "\n",
            "*****Epoch 1 Valid Start*****\n",
            "Epoch 1 Valid Loss : 154458.0493 Valid f1 : 0.0001 Valid em : 0.0000\n",
            "*****Epoch 1 Valid Finish*****\n",
            "\n",
            "*****Epoch 2 Train Start*****\n",
            "Epoch: 2, Step : 128, LR : 0.0000120730, Avg Loss : 566979.6999, f1 score : 0.0002\n",
            "Epoch: 2, Step : 256, LR : 0.0000116492, Avg Loss : 550597.0730, f1 score : 0.0005\n",
            "Epoch: 2, Step : 384, LR : 0.0000112255, Avg Loss : 562318.6910, f1 score : 0.0010\n",
            "Epoch: 2, Step : 512, LR : 0.0000108018, Avg Loss : 558555.4163, f1 score : 0.0000\n",
            "Epoch: 2, Step : 640, LR : 0.0000103780, Avg Loss : 547399.7885, f1 score : 0.0010\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-913e55b277a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"time : {(end - start)//60}분 {(end - start)%60}초\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# draw_plot(train_dict, valid_dict, 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-15dd7be247b3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, train_dataloader, valid_dataloader, epochs)\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_label\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m       \u001b[0;31m# start_logits_idx, end_logits_idx = torch.argmax(outputs.start_logits, dim=-1).float16(), torch.argmax(outputs.end_logits, dim=-1).float16()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-06fb2ef95fdb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, positions, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m     13\u001b[0m         outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids,\n\u001b[1;32m     14\u001b[0m                             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                   output_attentions=output_attentions, output_hidden_states=output_hidden_states)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m         )\n\u001b[1;32m   1030\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m                 )\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m         )\n\u001b[1;32m    500\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         )\n\u001b[1;32m    432\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "전혀 수렴하지 못하고 있다.\n",
        "\n",
        "start_idx와 end_idx가 영역에 존재하는지 여부를 나타내는 평가지표를 추가해야겠다.\n",
        "\n",
        "(end_idx - start_idx)/(예측된 end_idx - 예측된 start_idx)\n",
        "\n",
        "그리고, target인 start_idx가 tokenizer된 이후를 나타내지 않는다. 이 부분도 수정해야한다.\n",
        "\n",
        "start_idx 이전까지 tokenizing, 정답 text tokenizing하면 동일할 것이다."
      ],
      "metadata": {
        "id": "OgJ6Wnb7jeDK"
      }
    }
  ]
}