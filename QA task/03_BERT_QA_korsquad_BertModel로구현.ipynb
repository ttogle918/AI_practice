{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_BERT_QA_squad_BertForQuestionAnswering.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyObz6EJTLCT5QMiFTwH/yid",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4fc1c8ee42944534a20140088169d678": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_346eaccc032a4af896bfeab1d248fd2e",
              "IPY_MODEL_957b06f641ac423998b6b43e7a520604",
              "IPY_MODEL_ae8cf2b0a9004bc09c7aa2bedb8c68e3"
            ],
            "layout": "IPY_MODEL_9effacb5234c4ebcb7f52c9fbb570cdd"
          }
        },
        "346eaccc032a4af896bfeab1d248fd2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fda39c149e554f8ea82aaa52b01858fc",
            "placeholder": "​",
            "style": "IPY_MODEL_1ea9844781884ce995b1cbc5d0339fea",
            "value": "100%"
          }
        },
        "957b06f641ac423998b6b43e7a520604": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83803216c8fc4386893e02518568fc95",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_22ea3c82bb694ea9b9dc0b51c5de043d",
            "value": 2
          }
        },
        "ae8cf2b0a9004bc09c7aa2bedb8c68e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6019d0f658314bbf9eff561b3c29b0d0",
            "placeholder": "​",
            "style": "IPY_MODEL_20aab5cadcb0435bad1c5fcb33f7645d",
            "value": " 2/2 [00:00&lt;00:00, 51.03it/s]"
          }
        },
        "9effacb5234c4ebcb7f52c9fbb570cdd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fda39c149e554f8ea82aaa52b01858fc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1ea9844781884ce995b1cbc5d0339fea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "83803216c8fc4386893e02518568fc95": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22ea3c82bb694ea9b9dc0b51c5de043d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6019d0f658314bbf9eff561b3c29b0d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20aab5cadcb0435bad1c5fcb33f7645d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttogle918/AI_practice/blob/main/QA%20task/03_BERT_QA_korsquad_BertModel%EB%A1%9C%EA%B5%AC%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "해야할 것!\n",
        "\n",
        "1. 평가지표 바꾸기\n",
        "2. train에서 answer과 비교하기! 수식 작성"
      ],
      "metadata": {
        "id": "-o9eBPTacxdK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "62Oa6eDdlfHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Qu0sRPezlVoA"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric#, list_metrics\n",
        "\n",
        "from transformers import BertModel, AutoTokenizer, BertConfig, BertPreTrainedModel\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers.modeling_outputs import QuestionAnsweringModelOutput\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "1N7PgGshrtGh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gpu 연산이 가능하면 'cuda:0', 아니면 'cpu' 출력\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device, torch.cuda.device_count()"
      ],
      "metadata": {
        "id": "NMC8y1NUSDHP",
        "outputId": "dbaa25cd-9cd8-4708-d663-3d374882845a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(device(type='cuda', index=0), 1)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Load"
      ],
      "metadata": {
        "id": "50EDgWKELGI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/datasets/squad_kor_v1/blob/main/squad_kor_v1.py\n",
        "# squad_kor_v2\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('squad_kor_v1')\n",
        "dataset, dataset['train'][0]"
      ],
      "metadata": {
        "id": "STFveXBu634F",
        "outputId": "fcfafde3-72af-4e98-c7ae-2e9ca1a86016",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530,
          "referenced_widgets": [
            "4fc1c8ee42944534a20140088169d678",
            "346eaccc032a4af896bfeab1d248fd2e",
            "957b06f641ac423998b6b43e7a520604",
            "ae8cf2b0a9004bc09c7aa2bedb8c68e3",
            "9effacb5234c4ebcb7f52c9fbb570cdd",
            "fda39c149e554f8ea82aaa52b01858fc",
            "1ea9844781884ce995b1cbc5d0339fea",
            "83803216c8fc4386893e02518568fc95",
            "22ea3c82bb694ea9b9dc0b51c5de043d",
            "6019d0f658314bbf9eff561b3c29b0d0",
            "20aab5cadcb0435bad1c5fcb33f7645d"
          ]
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset squad_kor_v1 (/root/.cache/huggingface/datasets/squad_kor_v1/squad_kor_v1/1.0.0/18d4f44736b8ee85671f63cb84965bfb583fa0a4ff2df3c2e10eee9693796725)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4fc1c8ee42944534a20140088169d678"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 60407\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 5774\n",
              "    })\n",
              "}),\n",
              " {'answers': {'answer_start': [54], 'text': ['교향곡']},\n",
              "  'context': '1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.',\n",
              "  'id': '6566495-0-0',\n",
              "  'question': '바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?',\n",
              "  'title': '파우스트_서곡'})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# list_metrics()    # 'exact_match', 'squad', 'f1'"
      ],
      "metadata": {
        "id": "lgAhVEjEYqgG"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.context, self.question, self.answers = self.make_dataset(dataset)\n",
        "\n",
        "    def make_dataset(self, dataset):\n",
        "        context, question, answers = [], [], []\n",
        "        global max_length\n",
        "        for i, data in enumerate(dataset) :\n",
        "          answer_start, text = data['answers']['answer_start'], data['answers']['text']\n",
        "          if len(answer_start) != 1 : # 답이 없을 때\n",
        "            print(i, data)\n",
        "            continue\n",
        "          answers.append([answer_start[0], answer_start[0] + len(text)])  # 정답의 시작과 끝 index\n",
        "          context.append(data['context'])\n",
        "          question.append(data['question'])\n",
        "        return question, context, answers   # question + context, target으로 인코딩될 예정\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.question)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.context[idx], self.question[idx], self.answers[idx]"
      ],
      "metadata": {
        "id": "Amz1l80dSL1m"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch):\n",
        "    global tokenizer\n",
        "    input1_list, input2_list, target_list = [], [], []\n",
        "\n",
        "    for _input1, _input2, _target in batch:\n",
        "        input1_list.append(_input1)\n",
        "        input2_list.append(_input2)\n",
        "        target_list.append(_target)\n",
        "    \n",
        "    tensorized_input = tokenizer( # max length 구하기! dim=1\n",
        "        input1_list, input2_list,\n",
        "        add_special_tokens=True,\n",
        "        padding=\"longest\",  # 배치내 가장 긴 문장을 기준으로 부족한 문장은 [PAD] 토큰을 추가\n",
        "        return_tensors='pt',\n",
        "        max_length=512,\n",
        "        truncation=True\n",
        "    )\n",
        "    tensorized_label = torch.tensor(target_list)\n",
        "    print(len(tensorized_input['input_ids']))\n",
        "    return tensorized_input, tensorized_label"
      ],
      "metadata": {
        "id": "H8bLGE3dVVR8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataloader(dataset, tokenizer, batch_size, s='train') :\n",
        "  dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size =batch_size,\n",
        "      sampler = RandomSampler(dataset) if s == 'train' else SequentialSampler(dataset),\n",
        "      collate_fn = custom_collate_fn\n",
        "  )\n",
        "  print(f'batch_size : {batch_size}')\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "1eTDFIVxYu73"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 설명\n"
      ],
      "metadata": {
        "id": "IJoPh3xpqUKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomBertForQuestionAnswering(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\n",
        "        self.qa_output = nn.Linear(config.hidden_size, config.num_labels)\n",
        "        self.loss_fct = CrossEntropyLoss()\n",
        "\n",
        "        self.post_init()\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None,\n",
        "                positions=None, output_attentions=None, output_hidden_states=None):\n",
        "        \n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids,\n",
        "                            head_mask=head_mask, inputs_embeds=inputs_embeds,\n",
        "                  output_attentions=output_attentions, output_hidden_states=output_hidden_states)\n",
        "        \n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.qa_output(sequence_output)    # linear 통과해서 num_label로 분류\n",
        "        # print(logits[0].shape)    (387, 2)\n",
        "        \n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1).contiguous()\n",
        "        end_logits = end_logits.squeeze(-1).contiguous()\n",
        "\n",
        "        total_loss = None\n",
        "        start_positions, end_positions = positions.split(1, dim=-1)\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "          if len(start_positions.size()) > 1:\n",
        "              start_positions = start_positions.squeeze(-1)\n",
        "          if len(end_positions.size()) > 1:\n",
        "              end_positions = end_positions.squeeze(-1)\n",
        "\n",
        "          ignored_index = start_logits.size(1)\n",
        "          start_positions = start_positions.clamp(0, ignored_index)\n",
        "          end_positions = end_positions.clamp(0, ignored_index)\n",
        "\n",
        "          start_logits_idx, end_logits_idx = torch.argmax(start_logits, dim=-1).float(), torch.tensor(end_logits.argmax(dim=-1), dtype=torch.float16)\n",
        "          start_loss = self.loss_fct(start_logits_idx, start_positions.float())\n",
        "          end_loss = self.loss_fct(end_logits_idx, end_positions.float())\n",
        "          total_loss = (start_loss + end_loss) / 2\n",
        "\n",
        "        return {'loss':total_loss,\n",
        "            'start_logits_idx':start_logits_idx,\n",
        "            'end_logits_idx':end_logits_idx,\n",
        "            'hidden_states':outputs.hidden_states,\n",
        "            'attentions':outputs.attentions}"
      ],
      "metadata": {
        "id": "AyiUyiJ6Mtse"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train"
      ],
      "metadata": {
        "id": "tZXfyh4_Mta4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initializer(train_dataloader, epochs=2, model_name='klue/bert-base', lr=4e-5, wd=4e-5):\n",
        "    \"\"\"\n",
        "    모델, 옵티마이저, 스케쥴러 초기화\n",
        "    \"\"\"\n",
        "    config = BertConfig.from_pretrained(model_name)\n",
        "    config.max_length = 512\n",
        "    model = CustomBertForQuestionAnswering(config)\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(), # update 대상 파라미터를 입력\n",
        "        lr=lr,    # 2e-5\n",
        "        eps=1e-8,\n",
        "        weight_decay=wd\n",
        "    )\n",
        "    \n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    print(f\"Total train steps with {epochs} epochs: {total_steps}\")\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, \n",
        "        num_warmup_steps = 0, # 여기서는 warmup을 사용하지 않는다.\n",
        "        num_training_steps = total_steps\n",
        "    )\n",
        "    print(f'model_name : {model_name}, lr : {lr}, weight_decay : {wd}, epochs : {epochs}')\n",
        "    return model, optimizer, scheduler"
      ],
      "metadata": {
        "id": "GDcl_0m3Zj0f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(path, model, optimizer, scheduler, epoch, loss, f1, model_name=''):\n",
        "    file_name = f'{path}/epoch:{epoch}_loss:{loss:.4f}_f1:{f1:.4f}.ckpt'\n",
        "    \n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'loss' : loss,\n",
        "            'f1' : f1\n",
        "        }, \n",
        "        file_name\n",
        "    )\n",
        "    \n",
        "    print(f\"Saving epoch {epoch} checkpoint at {file_name}\")"
      ],
      "metadata": {
        "id": "TnpT2s-qZnG-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train code"
      ],
      "metadata": {
        "id": "ut6WMU93Z0in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# torch.stack([torch.tensor([1,2,3]), torch.tensor([4,5,6])], dim=1), torch.stack([torch.tensor([1,2,3]), torch.tensor([4,5,6])], dim=1).argmax(dim=-1)"
      ],
      "metadata": {
        "id": "hTIT2g3SkkLl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, scheduler, train_dataloader, valid_dataloader=None, epochs=1):\n",
        "  loss_fct = nn.MSELoss()\n",
        "  metric_squad = load_metric('squad')\n",
        "\n",
        "  train_dict = {'loss' : [], 'f1' : [], 'em' : []}\n",
        "  valid_dict = {'loss' : [], 'f1' : [], 'em' : []}\n",
        "\n",
        "  for epoch in range(epochs) :\n",
        "\n",
        "    print(f\"*****Epoch {epoch} Train Start*****\")\n",
        "    total_loss, total_f1, batch_f1, batch_em, total_em, batch_loss, batch_count = 0,0,0,0,0,0,0\n",
        "    \n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    \n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      batch_count+=1\n",
        "      \n",
        "      batch = tuple(item.to(device) for item in batch)\n",
        "      batch_input, batch_label = batch\n",
        "      \n",
        "      model.zero_grad()\n",
        "      \n",
        "      probs = model(**batch_input, positions=batch_label)  # forward\n",
        "      loss = probs['loss']\n",
        "\n",
        "      batch_loss += loss.item()\n",
        "      total_loss += loss.item()\n",
        "      \n",
        "      start_pos, end_pos = batch_label.split(1, dim=-1)\n",
        "      if start_pos is not None and end_pos is not None:\n",
        "        if len(start_pos.size()) > 1:\n",
        "            start_pos = start_pos.squeeze(-1)\n",
        "        if len(end_pos.size()) > 1:\n",
        "            end_pos = end_pos.squeeze(-1)\n",
        "      ignored_index = probs['start_logits_idx'].size(0)\n",
        "      start_pos = start_pos.clamp(0, ignored_index)\n",
        "      end_pos = end_pos.clamp(0, ignored_index)\n",
        "\n",
        "      # probs_idx = torch.stack([probs['start_logits_idx'], probs['end_logits_idx']], dim=-1)\n",
        "      print(start_pos.shape, end_pos.shape, probs['start_logits_idx'].shape)\n",
        "      start_results = metric_squad.compute(predictions=probs['start_logits_idx'], references=start_pos)\n",
        "      end_results = metric_squad.compute(predictions=probs['end_logits_idx'], references=end_pos)\n",
        "\n",
        "      f1 = results['f1']\n",
        "      batch_f1 += f1\n",
        "      total_f1 += f1\n",
        "\n",
        "      em = results['exact_match']\n",
        "      batch_em += em\n",
        "      total_em += em\n",
        "\n",
        "      # backward -> 파라미터의 미분(gradient)를 자동으로 계산\n",
        "      loss.backward()\n",
        "\n",
        "      # gradient clipping 적용 \n",
        "      clip_grad_norm_(model.parameters(), 1.0)\n",
        "      \n",
        "      # optimizer & scheduler 업데이트\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "\n",
        "      # 그래디언트 초기화\n",
        "      model.zero_grad()\n",
        "\n",
        "      if (step % 128 == 0 and step != 0):\n",
        "          learning_rate = optimizer.param_groups[0]['lr']\n",
        "          print(f\"Epoch: {epoch}, Step : {step}, LR : {learning_rate:.10f}, Avg Loss : {batch_loss / batch_count:.4f}, f1 score : {batch_f1 / batch_count:.4f}, em : {batch_em / batch_count:.4f}\")\n",
        "          \n",
        "          if (round(batch_f1 / batch_count, 5) == 0) and (round(learning_rate, 10) == 0) :\n",
        "              print(\"Train Finished, learning_rate is 0 and train_f1 is 0\")\n",
        "              return train_dict, valid_dict\n",
        "\n",
        "          batch_loss, batch_f1, batch_count = 0,0,0\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch} Total Mean Loss : {total_loss/(step+1):.4f}\")\n",
        "    print(f\"Epoch {epoch} Total Mean f1 : {total_f1/(step+1):.4f}\")\n",
        "    print(f\"*****Epoch {epoch} Train Finish*****\\n\")\n",
        "\n",
        "    train_dict['f1'].append(total_f1/(step+1))\n",
        "    train_dict['loss'].append(total_loss/(step+1))\n",
        "    train_dict['em'].append(total_em/(step+1))\n",
        "    \n",
        "    if valid_dataloader is not None:\n",
        "        print(f\"*****Epoch {epoch} Valid Start*****\")\n",
        "        valid_loss, valid_em, valid_f1 = validate(model, valid_dataloader)\n",
        "        print(f\"Epoch {epoch} Valid Loss : {valid_loss:.4f} Valid f1 : {valid_f1:.4f} Valid em : {valid_em:.4f}\")\n",
        "        print(f\"*****Epoch {epoch} Valid Finish*****\\n\")\n",
        "\n",
        "    valid_dict['f1'].append(valid_f1)\n",
        "    valid_dict['loss'].append(valid_loss)\n",
        "    valid_dict['em'].append(valid_em)\n",
        "    if round(valid_f1, 4) == 0 :\n",
        "        break\n",
        "    # if before_loss > valid_loss :\n",
        "    #     before_loss = valid_loss\n",
        "    #     save_checkpoint(\"/content/drive/MyDrive/Colab Notebooks/nlp/qa\", model, optimizer, scheduler, epoch, valid_loss, valid_f1, model_name)\n",
        "\n",
        "    # elif before_f1 < valid_f1  :\n",
        "    #     before_f1 = valid_f1\n",
        "    #     save_checkpoint(\"/content/drive/MyDrive/Colab Notebooks/nlp/qa\", model, optimizer, scheduler, epoch, valid_loss, valid_f1, model_name)\n",
        "\n",
        "  print(\"Train Finished\")\n",
        "  return train_dict, valid_dict"
      ],
      "metadata": {
        "id": "0tx78MsnZzoL"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### validation code"
      ],
      "metadata": {
        "id": "5iByxzyfaJ_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, valid_dataloader):\n",
        "    loss_fct = nn.MSELoss()\n",
        "    metric_squad = load_metric('squad')\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    \n",
        "    total_loss, total_em, total_f1= 0,0, 0\n",
        "        \n",
        "    for step, batch in enumerate(valid_dataloader):\n",
        "        \n",
        "        batch = tuple(item.to(device) for item in batch)\n",
        "            \n",
        "        batch_input, batch_label = batch\n",
        "            \n",
        "        # gradient 계산하지 않음\n",
        "        with torch.no_grad():\n",
        "            probs = model(**batch_input)\n",
        "            \n",
        "        loss = loss_fct(probs['loss'], batch_label)\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        results = metric_squad.compute(references=batch_label, predictions=probs)\n",
        "\n",
        "        f1 = results['f1']\n",
        "        total_f1 += f1\n",
        "\n",
        "        em = results['exact_match']\n",
        "        total_em += em\n",
        "\n",
        "    total_loss = total_loss/(step+1)\n",
        "    total_em = total_em/(step+1)\n",
        "    total_f1 = total_f1/(step+1)\n",
        "    return total_loss, total_em, total_f1"
      ],
      "metadata": {
        "id": "C0mOhdJxaJJN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### draw_plot"
      ],
      "metadata": {
        "id": "z7z2RcB8aMbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss와 f1-score의 변화를 epoch마다 보기 위한 plot\n",
        "def draw_plot(train_dict, valid_dict, i) :\n",
        "  print('green is loss, gray is f1')\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.title('Loss and F1 of Train data')\n",
        "  x_values= [n for n in range(len(train_dict['loss']))]\n",
        "  plt.plot(x_values, train_dict['loss'], color='green', marker='o')  # loss\n",
        "  plt.plot(x_values, train_dict['f1'], color='#AAAAAA', marker='*')  # f1\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.title('Loss and F1 of Validation data')\n",
        "  x_values= [n for n in range(len(valid_dict['loss']))]\n",
        "  plt.plot(x_values, valid_dict['loss'], color='green', marker='o')  # loss\n",
        "  plt.plot(x_values, valid_dict['f1'], color='#AAAAAA', marker='*')  # f1\n",
        "\n",
        "  plt.show()\n",
        "  plt.savefig(f'figure_{i}.png')"
      ],
      "metadata": {
        "id": "GB1JmBRkaNrs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'klue/bert-base'   # 다시 설정 필요\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "train_dataset = CustomDataset(dataset['train'])\n",
        "valid_dataset = CustomDataset(dataset['validation'])\n",
        "\n"
      ],
      "metadata": {
        "id": "4YIfLFs9aTTU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "XRje4KyFaj9t",
        "outputId": "4f9f3cfb-11ef-4e1f-c6fa-71aa749d5cec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1500"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = make_dataloader(train_dataset, model_name, 16, 'train')\n",
        "valid_dataloader = make_dataloader(valid_dataset, model_name, 8, 'valid')\n",
        "\n",
        "learning_rate = 5e-5\n",
        "weight_decay = 4e-5\n",
        "model, optimizer, scheduler = initializer(train_dataloader, 4, model_name, learning_rate, weight_decay)\n",
        "start = time.time()\n"
      ],
      "metadata": {
        "id": "0ye9Fwqve2LD",
        "outputId": "96f9f23e-a0be-465d-d8ac-8088e70062c4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size : 16\n",
            "batch_size : 8\n",
            "Total train steps with 4 epochs: 15104\n",
            "model_name : klue/bert-base, lr : 5e-05, weight_decay : 4e-05, epochs : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "30hb6iqNrYPC",
        "outputId": "3f96d5f1-fea8-4f81-bc5b-e1e825e6b348",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "257"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dict, valid_dict = train(model, optimizer, scheduler, train_dataloader, valid_dataloader, 4)\n",
        "end = time.time()\n",
        "print(f\"time : {(end - start)//60}분 {(end - start)%60}초\")\n",
        "\n",
        "# draw_plot(train_dict, valid_dict, 0)"
      ],
      "metadata": {
        "id": "1-L2JHPFhZKU",
        "outputId": "ccb1e0d5-afed-4d9b-b440-d1e92ee41028",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****Epoch 0 Train Start*****\n",
            "16\n",
            "torch.Size([16]) torch.Size([16]) torch.Size([16])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:38: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-44-913e55b277a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"time : {(end - start)//60}분 {(end - start)%60}초\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# draw_plot(train_dict, valid_dict, 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-43-b471a3f07d1a>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, train_dataloader, valid_dataloader, epochs)\u001b[0m\n\u001b[1;32m     40\u001b[0m       \u001b[0;31m# probs_idx = torch.stack([probs['start_logits_idx'], probs['end_logits_idx']], dim=-1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m       \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_pos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start_logits_idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m       \u001b[0mstart_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_squad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'start_logits_idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m       \u001b[0mend_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric_squad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'end_logits_idx'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend_pos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/metric.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/metric.py\u001b[0m in \u001b[0;36madd_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"predictions\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"references\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mintput_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mintput_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mintput_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36mencode_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m   1629\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m             \u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast_to_python_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0mencoded_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mencode_nested_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoded_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1629\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m             \u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast_to_python_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0mencoded_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mencode_nested_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoded_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36mencode_nested_example\u001b[0;34m(schema, obj, level)\u001b[0m\n\u001b[1;32m   1167\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msub_schema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_obj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m             }\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1163\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Got None but expected a dictionary instead\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1164\u001b[0m         return (\n\u001b[0;32m-> 1165\u001b[0;31m             {\n\u001b[0m\u001b[1;32m   1166\u001b[0m                 \u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mencode_nested_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_schema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msub_schema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_obj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36mzip_dict\u001b[0;34m(*dicts)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# set merge all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;31m# Will raise KeyError if the dict don't have the same keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/utils/py_utils.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munique_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitertools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mdicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# set merge all keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;31m# Will raise KeyError if the dict don't have the same keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m         \u001b[0;32myield\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdicts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: invalid index to scalar variable."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VrjDvfFem-nQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}