{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_BERT_QA_squad_BertForQuestionAnswering.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyOYGpzWpBEYtQqOeCbhniEt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c4aeb8daf0ea4bd08ca5920efd1512af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4836c52c5e994b02ba88c53ec8f4134f",
              "IPY_MODEL_f58ed92a2ad242ba84de5bb2c1582dc5",
              "IPY_MODEL_1c16712076ae4f17be9fdaf5dc500e2e"
            ],
            "layout": "IPY_MODEL_e93f538b2cba4eb1be5008d702f2c27d"
          }
        },
        "4836c52c5e994b02ba88c53ec8f4134f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d4a2dc5476b49d89310b72b9ab21c2f",
            "placeholder": "​",
            "style": "IPY_MODEL_386b5654544f4685bac54a5b7c97da27",
            "value": "100%"
          }
        },
        "f58ed92a2ad242ba84de5bb2c1582dc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df9707a87be84b789d47110746e90e17",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9e810914d75c44bea0a9fdea8e72389d",
            "value": 2
          }
        },
        "1c16712076ae4f17be9fdaf5dc500e2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49a0fb755e864fe99ff0f5c990151737",
            "placeholder": "​",
            "style": "IPY_MODEL_cef5b4365e5c4fc2ac9fa9e698b1cf15",
            "value": " 2/2 [00:00&lt;00:00, 57.03it/s]"
          }
        },
        "e93f538b2cba4eb1be5008d702f2c27d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d4a2dc5476b49d89310b72b9ab21c2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "386b5654544f4685bac54a5b7c97da27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df9707a87be84b789d47110746e90e17": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9e810914d75c44bea0a9fdea8e72389d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49a0fb755e864fe99ff0f5c990151737": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cef5b4365e5c4fc2ac9fa9e698b1cf15": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttogle918/AI_practice/blob/main/QA%20task/03_BERT_QA_korsquad_BertModel%EB%A1%9C%EA%B5%AC%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# korsquad를 사용하여 QA task 풀기(BertModel fine-tuning)\n",
        "\n",
        "참고 사이트\n",
        "\n",
        "[huggingface QA 설명](https://huggingface.co/course/chapter7/7?fw=tf)\n",
        "\n",
        "[huggingface git : ModelOutput](https://github.com/huggingface/transformers/blob/v4.21.0/src/transformers/utils/generic.py#L147)\n",
        "\n",
        "[huggingface git : QuestionAnsweringModelOutput](https://github.com/huggingface/transformers/blob/a9eee2ffecc874df7dd635b2c6abb246fdb318cc/src/transformers/modeling_outputs.py#L764)\n",
        "\n",
        "[huggingface docs](https://huggingface.co/docs/transformers/model_doc/bert)"
      ],
      "metadata": {
        "id": "QocCi_jhZ-ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install transformers\n",
        "# !pip install datasets"
      ],
      "metadata": {
        "id": "62Oa6eDdlfHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Qu0sRPezlVoA"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric#, list_metrics\n",
        "\n",
        "from transformers import BertModel, AutoTokenizer, BertConfig, BertPreTrainedModel\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers.modeling_outputs import QuestionAnsweringModelOutput\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "1N7PgGshrtGh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gpu 연산이 가능하면 'cuda:0', 아니면 'cpu' 출력\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device, torch.cuda.device_count()"
      ],
      "metadata": {
        "id": "NMC8y1NUSDHP",
        "outputId": "1444e02b-3d4a-4b63-b89f-dc546964730d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(device(type='cuda', index=0), 1)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Load"
      ],
      "metadata": {
        "id": "50EDgWKELGI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/datasets/squad_kor_v1/blob/main/squad_kor_v1.py\n",
        "# squad_kor_v2\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('squad_kor_v1')\n",
        "dataset, dataset['train'][0]"
      ],
      "metadata": {
        "id": "STFveXBu634F",
        "outputId": "dee651df-08fa-4bfb-fc24-34173c0abe18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 418,
          "referenced_widgets": [
            "c4aeb8daf0ea4bd08ca5920efd1512af",
            "4836c52c5e994b02ba88c53ec8f4134f",
            "f58ed92a2ad242ba84de5bb2c1582dc5",
            "1c16712076ae4f17be9fdaf5dc500e2e",
            "e93f538b2cba4eb1be5008d702f2c27d",
            "8d4a2dc5476b49d89310b72b9ab21c2f",
            "386b5654544f4685bac54a5b7c97da27",
            "df9707a87be84b789d47110746e90e17",
            "9e810914d75c44bea0a9fdea8e72389d",
            "49a0fb755e864fe99ff0f5c990151737",
            "cef5b4365e5c4fc2ac9fa9e698b1cf15"
          ]
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Reusing dataset squad_kor_v1 (/root/.cache/huggingface/datasets/squad_kor_v1/squad_kor_v1/1.0.0/18d4f44736b8ee85671f63cb84965bfb583fa0a4ff2df3c2e10eee9693796725)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c4aeb8daf0ea4bd08ca5920efd1512af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 60407\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 5774\n",
              "    })\n",
              "}),\n",
              " {'answers': {'answer_start': [54], 'text': ['교향곡']},\n",
              "  'context': '1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.',\n",
              "  'id': '6566495-0-0',\n",
              "  'question': '바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?',\n",
              "  'title': '파우스트_서곡'})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.question, self.context, self.answer_start, self.answer_text = self.make_dataset(dataset)\n",
        "\n",
        "    def make_dataset(self, dataset):\n",
        "        context, question, answer_start, answer_text = [], [], [], []\n",
        "        for i, data in enumerate(dataset) :\n",
        "          start = data['answers']['answer_start']\n",
        "          if len(start) != 1 : # 답이 없을 때\n",
        "            print(i, data)\n",
        "            continue\n",
        "          text, start = self.get_text(data['context'], start[0])\n",
        "          answer_start.append(start)\n",
        "          answer_text.append(data['answers']['text'])\n",
        "          # answers.append([answer_start[0], answer_start[0] + len(text)])  # 정답의 시작과 끝 index\n",
        "          context.append(data['context'])\n",
        "          question.append(data['question'])\n",
        "        return question, context, answer_start, answer_text\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.question)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.question[idx], self.context[idx], self.answer_start[idx], self.answer_text[idx]\n",
        "\n",
        "    def get_text(self, text, start_loc) :\n",
        "        text_splited = text.split('. ')\n",
        "        length_text_answer_idx = -1\n",
        "        length_text = [0]\n",
        "\n",
        "        for i, t in enumerate(text_splited) :\n",
        "          length_text.append(length_text[-1]+len(t)+2)\n",
        "          \n",
        "          if length_text_answer_idx == -1 and start_loc > length_text[-1] :\n",
        "            length_text_answer_idx = i\n",
        "        length_text[-1] -= 2\n",
        "\n",
        "        start, end = 0, len(length_text)-1\n",
        "        while length_text[end] - length_text[start] > 512 :\n",
        "            if start_loc - length_text[start] > length_text[end] - start_loc :\n",
        "              start += 1\n",
        "            else :\n",
        "              end -= 1\n",
        "        \n",
        "        return text[text_splited[start]:text_splited[end]], start_loc - text_splited[start]"
      ],
      "metadata": {
        "id": "Amz1l80dSL1m"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer('하나의 교향곡을 쓰려는 뜻을 갖는다.', add_special_tokens=False).input_ids\n",
        "answer = tokenizer('하나의 교향곡', add_special_tokens=False).input_ids\n",
        "tokens = tokenizer(['1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다.', '이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다'], \n",
        "                   ['이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다', '이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다'], \n",
        "                   return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "text, answer, tokens"
      ],
      "metadata": {
        "id": "03UjdR3LmyiF",
        "outputId": "38594536-2b71-4791-9786-d69791e126e2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([3657, 2079, 19282, 2069, 1363, 2370, 2259, 936, 2069, 554, 2259, 2062, 18],\n",
              " [3657, 2079, 19282],\n",
              " {'input_ids': tensor([[    2, 13934,  2236,  2440, 27982,  2259, 21310,  2079, 11994,  3791,\n",
              "           2069,  3790,  1508,  2088,   636,  3800,  2170,  3717,  2052,  9001,\n",
              "           8345,  4642,  2200,  3689,  3657,  2079, 19282,  2069,  1363,  2370,\n",
              "           2259,   936,  2069,   554,  2259,  2062,    18,     3,  8345,  4642,\n",
              "           2200,  3689,  3657,  2079, 19282,  2069,  1363,  2370,  2259,   936,\n",
              "           2069,   554,  2259,  2062,     3],\n",
              "         [    2,  8345,  4642,  2200,  3689,  3657,  2079, 19282,  2069,  1363,\n",
              "           2370,  2259,   936,  2069,   554,  2259,  2062,     3,  8345,  4642,\n",
              "           2200,  3689,  3657,  2079, 19282,  2069,  1363,  2370,  2259,   936,\n",
              "           2069,   554,  2259,  2062,     3,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1],\n",
              "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0]])})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensorized_label = np.zeros(tokens.input_ids.shape)\n",
        "print(answer)\n",
        "for i in [0,1] :\n",
        "  padding_start = (tokens['attention_mask'][i] == 1).nonzero()[-1].item()+1\n",
        "  print(padding_start)\n",
        "  tensorized_label[i, padding_start-len(text): padding_start-len(text)+len(answer)] = 1\n",
        "  print(tensorized_label)\n",
        "  print(tokens['input_ids'][i])\n",
        "  print(tokens['input_ids'][i, padding_start-len(text): padding_start-len(text)+len(answer)])\n",
        "  print('\\n\\n')"
      ],
      "metadata": {
        "id": "cO9NIqjvq7i_",
        "outputId": "4b2d24e2-77d7-49db-9ed7-f144a12e38e9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3657, 2079, 19282]\n",
            "55\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]]\n",
            "tensor([    2, 13934,  2236,  2440, 27982,  2259, 21310,  2079, 11994,  3791,\n",
            "         2069,  3790,  1508,  2088,   636,  3800,  2170,  3717,  2052,  9001,\n",
            "         8345,  4642,  2200,  3689,  3657,  2079, 19282,  2069,  1363,  2370,\n",
            "         2259,   936,  2069,   554,  2259,  2062,    18,     3,  8345,  4642,\n",
            "         2200,  3689,  3657,  2079, 19282,  2069,  1363,  2370,  2259,   936,\n",
            "         2069,   554,  2259,  2062,     3])\n",
            "tensor([ 3657,  2079, 19282])\n",
            "\n",
            "\n",
            "\n",
            "35\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]]\n",
            "tensor([    2,  8345,  4642,  2200,  3689,  3657,  2079, 19282,  2069,  1363,\n",
            "         2370,  2259,   936,  2069,   554,  2259,  2062,     3,  8345,  4642,\n",
            "         2200,  3689,  3657,  2079, 19282,  2069,  1363,  2370,  2259,   936,\n",
            "         2069,   554,  2259,  2062,     3,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0])\n",
            "tensor([ 3657,  2079, 19282])\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch):\n",
        "    global tokenizer\n",
        "    question_list, context_list, answer_start, answer_text, after_list = [], [], [], [], []\n",
        "\n",
        "    for _question, _context, _start, _text in batch:\n",
        "        question_list.append(_question)\n",
        "        context_list.append(_context)\n",
        "        after_list.append(_context[_start:])\n",
        "        answer_start.append(_start)\n",
        "        answer_text.append(_text)\n",
        "    \n",
        "    tensorized_input = tokenizer(\n",
        "        question_list, context_list,\n",
        "        add_special_tokens=True,\n",
        "        padding=\"longest\",  # 배치내 가장 긴 문장을 기준으로 부족한 문장은 [PAD] 토큰을 추가\n",
        "        return_tensors='pt',\n",
        "        max_length=512,\n",
        "        truncation=True\n",
        "    )\n",
        "    # answer_start token의 위치를 찾기 위해 list로 반환\n",
        "    after_text = tokenizer(\n",
        "        after_list,\n",
        "        add_special_tokens=False,\n",
        "        return_tensors=None\n",
        "    ).input_ids\n",
        "\n",
        "    # answer text token만 변환\n",
        "    answer_tokens = tokenizer(\n",
        "        after_list,\n",
        "        add_special_tokens=False,\n",
        "        return_tensors=None\n",
        "    ).input_ids\n",
        "\n",
        "    tensorized_label = np.zeros(tensorized_input.input_ids.shape)    # input_ids만큼의 길이이고 0으로 된 array\n",
        "\n",
        "    for i, text, answer in enumerate(zip(after_text, answer_tokens)) :\n",
        "        padding_start = (tensorized_input['attention_mask'][i] == 1).nonzero()[-1].item()+1\n",
        "        tensorized_label[i, padding_start-len(text): padding_start-len(text)+len(answer)] = 1\n",
        "\n",
        "    return tensorized_input, torch.from_numpy(tensorized_label)"
      ],
      "metadata": {
        "id": "H8bLGE3dVVR8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataloader(dataset, tokenizer, batch_size, s='train') :\n",
        "  dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size =batch_size,\n",
        "      sampler = RandomSampler(dataset) if s == 'train' else SequentialSampler(dataset),\n",
        "      collate_fn = custom_collate_fn\n",
        "  )\n",
        "  print(f'batch_size : {batch_size}')\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "1eTDFIVxYu73"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 설명\n"
      ],
      "metadata": {
        "id": "IJoPh3xpqUKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomBertForQuestionAnswering(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\n",
        "        self.qa_output = nn.Linear(config.hidden_size, config.num_labels)\n",
        "        self.loss_fct = CrossEntropyLoss()\n",
        "\n",
        "        self.post_init()\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, position_ids=None, head_mask=None, inputs_embeds=None,\n",
        "                positions=None, output_attentions=None, output_hidden_states=None):\n",
        "        \n",
        "        outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids,\n",
        "                            head_mask=head_mask, inputs_embeds=inputs_embeds,\n",
        "                  output_attentions=output_attentions, output_hidden_states=output_hidden_states)\n",
        "        \n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.qa_output(sequence_output)    # linear 통과해서 num_label로 분류\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1).contiguous()\n",
        "        end_logits = end_logits.squeeze(-1).contiguous()\n",
        "\n",
        "        total_loss = None\n",
        "        start_positions, end_positions = positions.split(1, dim=-1)\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "          if len(start_positions.size()) > 1:\n",
        "              start_positions = start_positions.squeeze(-1)\n",
        "          if len(end_positions.size()) > 1:\n",
        "              end_positions = end_positions.squeeze(-1)\n",
        "\n",
        "          ignored_index = start_logits.size(1)\n",
        "          start_positions = start_positions.clamp(0, ignored_index)\n",
        "          end_positions = end_positions.clamp(0, ignored_index)\n",
        "\n",
        "          start_logits_idx, end_logits_idx = torch.argmax(start_logits, dim=-1).float(), torch.tensor(end_logits.argmax(dim=-1), dtype=torch.float16)\n",
        "          start_loss = self.loss_fct(start_logits_idx, start_positions.float())\n",
        "          end_loss = self.loss_fct(end_logits_idx, end_positions.float())\n",
        "          total_loss = (start_loss + end_loss) / 2\n",
        "          total_loss.requires_grad_(True)\n",
        "\n",
        "        return QuestionAnsweringModelOutput(loss=total_loss, \n",
        "                                            start_logits=start_logits_idx, \n",
        "                                            end_logits=start_logits_idx,\n",
        "                                            hidden_states=outputs.hidden_states, \n",
        "                                            attentions=outputs.attentions)\n"
      ],
      "metadata": {
        "id": "AyiUyiJ6Mtse"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train"
      ],
      "metadata": {
        "id": "tZXfyh4_Mta4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initializer(train_dataloader, epochs=2, model_name='klue/bert-base', lr=4e-5, wd=4e-5):\n",
        "    \"\"\"\n",
        "    모델, 옵티마이저, 스케쥴러 초기화\n",
        "    \"\"\"\n",
        "    config = BertConfig.from_pretrained(model_name)\n",
        "    config.max_length = 512\n",
        "    model = CustomBertForQuestionAnswering(config)\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(), # update 대상 파라미터를 입력\n",
        "        lr=lr,    # 2e-5\n",
        "        eps=1e-8,\n",
        "        weight_decay=wd\n",
        "    )\n",
        "    \n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    print(f\"Total train steps with {epochs} epochs: {total_steps}\")\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, \n",
        "        num_warmup_steps = 0, # 여기서는 warmup을 사용하지 않는다.\n",
        "        num_training_steps = total_steps\n",
        "    )\n",
        "    print(f'model_name : {model_name}, lr : {lr}, weight_decay : {wd}, epochs : {epochs}')\n",
        "    return model, optimizer, scheduler"
      ],
      "metadata": {
        "id": "GDcl_0m3Zj0f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(path, model, optimizer, scheduler, epoch, loss, f1, model_name=''):\n",
        "    file_name = f'{path}/epoch:{epoch}_loss:{loss:.4f}_f1:{f1:.4f}.ckpt'\n",
        "    \n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'loss' : loss,\n",
        "            'f1' : f1\n",
        "        }, \n",
        "        file_name\n",
        "    )\n",
        "    \n",
        "    print(f\"Saving epoch {epoch} checkpoint at {file_name}\")"
      ],
      "metadata": {
        "id": "TnpT2s-qZnG-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train code"
      ],
      "metadata": {
        "id": "ut6WMU93Z0in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, scheduler, train_dataloader, valid_dataloader=None, epochs=1):\n",
        "  loss_fct = nn.MSELoss()\n",
        "  em_fct = load_metric('exact_match')\n",
        "  f1_fct = load_metric('f1')\n",
        "\n",
        "\n",
        "  train_dict = {'loss' : [], 'f1' : []}\n",
        "  valid_dict = {'loss' : [], 'f1' : [], 'em' : []}\n",
        "\n",
        "  for epoch in range(epochs) :\n",
        "\n",
        "    print(f\"*****Epoch {epoch} Train Start*****\")\n",
        "    total_loss, total_f1, batch_f1, batch_em, total_em, batch_loss, batch_count = 0,0,0,0,0,0,0\n",
        "    \n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    \n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      batch_count+=1\n",
        "      \n",
        "      batch = tuple(item.to(device) for item in batch)\n",
        "      batch_input, batch_label = batch\n",
        "      \n",
        "      model.zero_grad()\n",
        "      \n",
        "      outputs = model(**batch_input, positions=batch_label)  # forward\n",
        "      # start_logits_idx, end_logits_idx = torch.argmax(outputs.start_logits, dim=-1).float16(), torch.argmax(outputs.end_logits, dim=-1).float16()\n",
        "\n",
        "      start_pos, end_pos = batch_label.split(1, dim=-1)\n",
        "      if start_pos is not None and end_pos is not None:\n",
        "        if len(start_pos.size()) > 1:\n",
        "            start_pos = start_pos.squeeze(-1)\n",
        "        if len(end_pos.size()) > 1:\n",
        "            end_pos = end_pos.squeeze(-1)\n",
        "\n",
        "        ignored_index = outputs.start_logits.size(0)\n",
        "        start_pos = start_pos.clamp(0, ignored_index)\n",
        "        end_pos = end_pos.clamp(0, ignored_index)\n",
        "\n",
        "      # start_loss = loss_fct(start_logits_idx, start_pos.float())\n",
        "      # end_loss = loss_fct(end_logits_idx, end_pos.float())\n",
        "      # loss = (start_loss + end_loss) / 2\n",
        "      \n",
        "      loss = outputs.loss\n",
        "      batch_loss += loss.item()\n",
        "      total_loss += loss.item()\n",
        "\n",
        "\n",
        "      # start_pos, end_pos = batch_label.split(1, dim=-1)\n",
        "      # if start_pos is not None and end_pos is not None:\n",
        "      #   if len(start_pos.size()) > 1:\n",
        "      #       start_pos = start_pos.squeeze(-1)\n",
        "      #   if len(end_pos.size()) > 1:\n",
        "      #       end_pos = end_pos.squeeze(-1)\n",
        "      # ignored_index = probs['start_logits_idx'].size(0)\n",
        "      # start_pos = start_pos.clamp(0, ignored_index)\n",
        "      # end_pos = end_pos.clamp(0, ignored_index)\n",
        "\n",
        "      # probs_idx = torch.stack([probs['start_logits_idx'], probs['end_logits_idx']], dim=-1)\n",
        "      # em = em_fct.compute(predictions=torch.stack([start_logits_idx, end_logits_idx], dim=-1),\n",
        "      #                       references=batch_label)['exact_match']\n",
        "\n",
        "      # batch_em += em\n",
        "      # total_em += em\n",
        "      \n",
        "      start_results = f1_fct.compute(predictions=outputs.start_logits, references=start_pos, average='micro')['f1']\n",
        "      end_results = f1_fct.compute(predictions=outputs.end_logits, references=end_pos, average='micro')['f1']\n",
        "\n",
        "      f1 = (start_results + end_results)/2\n",
        "      batch_f1 += f1\n",
        "      total_f1 += f1\n",
        "\n",
        "      # backward -> 파라미터의 미분(gradient)를 자동으로 계산\n",
        "      loss.backward()\n",
        "\n",
        "      # gradient clipping 적용 \n",
        "      clip_grad_norm_(model.parameters(), 1.0)\n",
        "      \n",
        "      # optimizer & scheduler 업데이트\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "\n",
        "      # 그래디언트 초기화\n",
        "      model.zero_grad()\n",
        "\n",
        "      if (step % 128 == 0 and step != 0):\n",
        "          learning_rate = optimizer.param_groups[0]['lr']\n",
        "          print(f\"Epoch: {epoch}, Step : {step}, LR : {learning_rate:.10f}, Avg Loss : {batch_loss / batch_count:.4f}, f1 score : {batch_f1 / batch_count:.4f}\")\n",
        "          \n",
        "          if (round(batch_f1 / batch_count, 5) == 0) and (round(learning_rate, 10) == 0) :\n",
        "              print(\"Train Finished, learning_rate is 0 and train_f1 is 0\")\n",
        "              return train_dict, valid_dict\n",
        "\n",
        "          batch_loss, batch_f1, batch_count = 0,0,0\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch} Total Mean Loss : {total_loss/(step+1):.4f}\")\n",
        "    print(f\"Epoch {epoch} Total Mean f1 : {total_f1/(step+1):.4f}\")\n",
        "    print(f\"*****Epoch {epoch} Train Finish*****\\n\")\n",
        "\n",
        "    train_dict['f1'].append(total_f1/(step+1))\n",
        "    train_dict['loss'].append(total_loss/(step+1))\n",
        "    # train_dict['em'].append(total_em/(step+1))\n",
        "    \n",
        "    if valid_dataloader is not None:\n",
        "        print(f\"*****Epoch {epoch} Valid Start*****\")\n",
        "        valid_loss, valid_em, valid_f1 = validate(model, valid_dataloader, f1_fct, em_fct)\n",
        "        print(f\"Epoch {epoch} Valid Loss : {valid_loss:.4f} Valid f1 : {valid_f1:.4f} Valid em : {valid_em:.4f}\")\n",
        "        print(f\"*****Epoch {epoch} Valid Finish*****\\n\")\n",
        "\n",
        "    valid_dict['f1'].append(valid_f1)\n",
        "    valid_dict['loss'].append(valid_loss)\n",
        "    valid_dict['em'].append(valid_em)\n",
        "    if round(valid_f1, 4) == 0 :\n",
        "        break\n",
        "    # if before_loss > valid_loss :\n",
        "    #     before_loss = valid_loss\n",
        "    #     save_checkpoint(\"/content/drive/MyDrive/Colab Notebooks/nlp/qa\", model, optimizer, scheduler, epoch, valid_loss, valid_f1, model_name)\n",
        "\n",
        "    # elif before_f1 < valid_f1  :\n",
        "    #     before_f1 = valid_f1\n",
        "    #     save_checkpoint(\"/content/drive/MyDrive/Colab Notebooks/nlp/qa\", model, optimizer, scheduler, epoch, valid_loss, valid_f1, model_name)\n",
        "\n",
        "  print(\"Train Finished\")\n",
        "  return train_dict, valid_dict"
      ],
      "metadata": {
        "id": "0tx78MsnZzoL"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### validation code"
      ],
      "metadata": {
        "id": "5iByxzyfaJ_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, valid_dataloader, f1_fct, em_fct):\n",
        "    loss_fct = nn.MSELoss()\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    \n",
        "    total_loss, total_em, total_f1= 0,0, 0\n",
        "        \n",
        "    for step, batch in enumerate(valid_dataloader):\n",
        "        \n",
        "        batch = tuple(item.to(device) for item in batch)\n",
        "            \n",
        "        batch_input, batch_label = batch\n",
        "            \n",
        "        # gradient 계산하지 않음\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch_input, positions=batch_label)\n",
        "            # start_logit_idx, end_logit_idx = torch.argmax(outputs.start_logits, dim=-1).float16(), torch.argmax(outputs.end_logits, dim=-1).float16()\n",
        "            \n",
        "        start_pos, end_pos = batch_label.split(1, dim=-1)\n",
        "        if start_pos is not None and end_pos is not None:\n",
        "          if len(start_pos.size()) > 1:\n",
        "              start_pos = start_pos.squeeze(-1)\n",
        "          if len(end_pos.size()) > 1:\n",
        "              end_pos = end_pos.squeeze(-1)\n",
        "        ignored_index = outputs.end_logits.size(0)\n",
        "        start_pos = start_pos.clamp(0, ignored_index)\n",
        "        end_pos = end_pos.clamp(0, ignored_index)\n",
        "        \n",
        "        loss = outputs.loss\n",
        "        total_loss += loss.item()\n",
        "        \n",
        "        em = em_fct.compute(predictions=torch.stack([outputs.start_logits, outputs.end_logits], dim=-1), \n",
        "                            references=batch_label)\n",
        "        total_em += em['exact_match']\n",
        "        \n",
        "        start_results = f1_fct.compute(predictions=outputs.start_logits, references=start_pos, average='micro')['f1']\n",
        "        end_results = f1_fct.compute(predictions=outputs.end_logits, references=end_pos, average='micro')['f1']\n",
        "        f1 = (start_results + end_results)/2\n",
        "        total_f1 += f1\n",
        "\n",
        "    total_loss = total_loss/(step+1)\n",
        "    total_em = total_em/(step+1)\n",
        "    total_f1 = total_f1/(step+1)\n",
        "    return total_loss, total_em, total_f1"
      ],
      "metadata": {
        "id": "C0mOhdJxaJJN"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### draw_plot"
      ],
      "metadata": {
        "id": "z7z2RcB8aMbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss와 f1-score의 변화를 epoch마다 보기 위한 plot\n",
        "def draw_plot(train_dict, valid_dict, i) :\n",
        "  print('green is loss, gray is f1')\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.title('Loss and F1 of Train data')\n",
        "  x_values= [n for n in range(len(train_dict['loss']))]\n",
        "  plt.plot(x_values, train_dict['loss'], color='green', marker='o')  # loss\n",
        "  plt.plot(x_values, train_dict['f1'], color='#AAAAAA', marker='*')  # f1\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.title('Loss and F1 of Validation data')\n",
        "  x_values= [n for n in range(len(valid_dict['loss']))]\n",
        "  plt.plot(x_values, valid_dict['loss'], color='green', marker='o')  # loss\n",
        "  plt.plot(x_values, valid_dict['f1'], color='#AAAAAA', marker='*')  # f1\n",
        "\n",
        "  plt.show()\n",
        "  plt.savefig(f'figure_{i}.png')"
      ],
      "metadata": {
        "id": "GB1JmBRkaNrs"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'klue/bert-base'   # 다시 설정 필요\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "4YIfLFs9aTTU"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(dataset['train'])\n",
        "valid_dataset = CustomDataset(dataset['validation'])\n",
        "\n",
        "del dataset"
      ],
      "metadata": {
        "id": "u7zOF7oMvtSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "XRje4KyFaj9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = make_dataloader(train_dataset, model_name, 16, 'train')\n",
        "valid_dataloader = make_dataloader(valid_dataset, model_name, 8, 'valid')\n",
        "\n",
        "learning_rate = 5e-5\n",
        "weight_decay = 4e-5\n",
        "model, optimizer, scheduler = initializer(train_dataloader, 4, model_name, learning_rate, weight_decay)\n",
        "start = time.time()\n"
      ],
      "metadata": {
        "id": "0ye9Fwqve2LD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "del train_dataset\n",
        "del valid_dataset"
      ],
      "metadata": {
        "id": "M0iKksS2ShWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "30hb6iqNrYPC",
        "outputId": "0c8038d4-3654-479e-c80e-50ca848f58e3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "88"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "metadata": {
        "id": "LA9hAnBnIOsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dict, valid_dict = train(model, optimizer, scheduler, train_dataloader, valid_dataloader, 4)\n",
        "end = time.time()\n",
        "print(f\"time : {(end - start)//60}분 {(end - start)%60}초\")\n",
        "\n",
        "# draw_plot(train_dict, valid_dict, 0)"
      ],
      "metadata": {
        "id": "1-L2JHPFhZKU",
        "outputId": "41818c92-4311-4d08-bbd8-e174a61c6491",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****Epoch 0 Train Start*****\n",
            "Epoch: 0, Step : 128, LR : 0.0000370730, Avg Loss : 559556.4972, f1 score : 0.0000\n",
            "Epoch: 0, Step : 256, LR : 0.0000366492, Avg Loss : 531714.2206, f1 score : 0.0005\n",
            "Epoch: 0, Step : 384, LR : 0.0000362255, Avg Loss : 568919.5496, f1 score : 0.0022\n",
            "Epoch: 0, Step : 512, LR : 0.0000358018, Avg Loss : 544838.4856, f1 score : 0.0007\n",
            "Epoch: 0, Step : 640, LR : 0.0000353780, Avg Loss : 584851.4816, f1 score : 0.0005\n",
            "Epoch: 0, Step : 768, LR : 0.0000349543, Avg Loss : 551967.8732, f1 score : 0.0002\n",
            "Epoch: 0, Step : 896, LR : 0.0000345306, Avg Loss : 544566.5043, f1 score : 0.0005\n",
            "Epoch: 0, Step : 1024, LR : 0.0000341069, Avg Loss : 559160.8174, f1 score : 0.0007\n",
            "Epoch: 0, Step : 1152, LR : 0.0000336831, Avg Loss : 561093.9429, f1 score : 0.0010\n",
            "Epoch: 0, Step : 1280, LR : 0.0000332594, Avg Loss : 523954.7056, f1 score : 0.0024\n",
            "Epoch: 0, Step : 1408, LR : 0.0000328357, Avg Loss : 566050.6188, f1 score : 0.0000\n",
            "Epoch: 0, Step : 1536, LR : 0.0000324119, Avg Loss : 616570.4418, f1 score : 0.0000\n",
            "Epoch: 0, Step : 1664, LR : 0.0000319882, Avg Loss : 571505.1260, f1 score : 0.0007\n",
            "Epoch: 0, Step : 1792, LR : 0.0000315645, Avg Loss : 558446.9888, f1 score : 0.0010\n",
            "Epoch: 0, Step : 1920, LR : 0.0000311408, Avg Loss : 543406.5389, f1 score : 0.0005\n",
            "Epoch: 0, Step : 2048, LR : 0.0000307170, Avg Loss : 552863.3674, f1 score : 0.0007\n",
            "Epoch: 0, Step : 2176, LR : 0.0000302933, Avg Loss : 568954.1434, f1 score : 0.0015\n",
            "Epoch: 0, Step : 2304, LR : 0.0000298696, Avg Loss : 585102.3337, f1 score : 0.0007\n",
            "Epoch: 0, Step : 2432, LR : 0.0000294458, Avg Loss : 556427.5887, f1 score : 0.0017\n",
            "Epoch: 0, Step : 2560, LR : 0.0000290221, Avg Loss : 586303.2214, f1 score : 0.0000\n",
            "Epoch: 0, Step : 2688, LR : 0.0000285984, Avg Loss : 562093.1567, f1 score : 0.0017\n",
            "Epoch: 0, Step : 2816, LR : 0.0000281747, Avg Loss : 565016.6960, f1 score : 0.0000\n",
            "Epoch: 0, Step : 2944, LR : 0.0000277509, Avg Loss : 577308.6141, f1 score : 0.0010\n",
            "Epoch: 0, Step : 3072, LR : 0.0000273272, Avg Loss : 568966.2322, f1 score : 0.0012\n",
            "Epoch: 0, Step : 3200, LR : 0.0000269035, Avg Loss : 594047.5979, f1 score : 0.0007\n",
            "Epoch: 0, Step : 3328, LR : 0.0000264797, Avg Loss : 531348.0811, f1 score : 0.0007\n",
            "Epoch: 0, Step : 3456, LR : 0.0000260560, Avg Loss : 547591.8610, f1 score : 0.0015\n",
            "Epoch: 0, Step : 3584, LR : 0.0000256323, Avg Loss : 543152.8743, f1 score : 0.0020\n",
            "Epoch: 0, Step : 3712, LR : 0.0000252086, Avg Loss : 582938.6180, f1 score : 0.0002\n",
            "Epoch 0 Total Mean Loss : 562115.5202\n",
            "Epoch 0 Total Mean f1 : 0.0009\n",
            "*****Epoch 0 Train Finish*****\n",
            "\n",
            "*****Epoch 0 Valid Start*****\n",
            "Epoch 0 Valid Loss : 154458.0493 Valid f1 : 0.0001 Valid em : 0.0000\n",
            "*****Epoch 0 Valid Finish*****\n",
            "\n",
            "*****Epoch 1 Train Start*****\n",
            "Epoch: 1, Step : 128, LR : 0.0000245730, Avg Loss : 546296.9008, f1 score : 0.0000\n",
            "Epoch: 1, Step : 256, LR : 0.0000241492, Avg Loss : 567516.9204, f1 score : 0.0010\n",
            "Epoch: 1, Step : 384, LR : 0.0000237255, Avg Loss : 574306.1903, f1 score : 0.0007\n",
            "Epoch: 1, Step : 512, LR : 0.0000233018, Avg Loss : 543443.1831, f1 score : 0.0012\n",
            "Epoch: 1, Step : 640, LR : 0.0000228780, Avg Loss : 569175.1370, f1 score : 0.0020\n",
            "Epoch: 1, Step : 768, LR : 0.0000224543, Avg Loss : 591472.4955, f1 score : 0.0005\n",
            "Epoch: 1, Step : 896, LR : 0.0000220306, Avg Loss : 583334.8469, f1 score : 0.0010\n",
            "Epoch: 1, Step : 1024, LR : 0.0000216069, Avg Loss : 616493.9297, f1 score : 0.0005\n",
            "Epoch: 1, Step : 1152, LR : 0.0000211831, Avg Loss : 535906.4286, f1 score : 0.0024\n",
            "Epoch: 1, Step : 1280, LR : 0.0000207594, Avg Loss : 539977.5632, f1 score : 0.0027\n",
            "Epoch: 1, Step : 1408, LR : 0.0000203357, Avg Loss : 564939.9321, f1 score : 0.0015\n",
            "Epoch: 1, Step : 1536, LR : 0.0000199119, Avg Loss : 544599.8450, f1 score : 0.0005\n",
            "Epoch: 1, Step : 1664, LR : 0.0000194882, Avg Loss : 516507.5815, f1 score : 0.0010\n",
            "Epoch: 1, Step : 1792, LR : 0.0000190645, Avg Loss : 547023.4609, f1 score : 0.0015\n",
            "Epoch: 1, Step : 1920, LR : 0.0000186408, Avg Loss : 591405.5308, f1 score : 0.0000\n",
            "Epoch: 1, Step : 2048, LR : 0.0000182170, Avg Loss : 572721.4536, f1 score : 0.0017\n",
            "Epoch: 1, Step : 2176, LR : 0.0000177933, Avg Loss : 534988.6035, f1 score : 0.0005\n",
            "Epoch: 1, Step : 2304, LR : 0.0000173696, Avg Loss : 579402.8401, f1 score : 0.0005\n",
            "Epoch: 1, Step : 2432, LR : 0.0000169458, Avg Loss : 546354.7593, f1 score : 0.0002\n",
            "Epoch: 1, Step : 2560, LR : 0.0000165221, Avg Loss : 585708.6035, f1 score : 0.0007\n",
            "Epoch: 1, Step : 2688, LR : 0.0000160984, Avg Loss : 556460.1726, f1 score : 0.0015\n",
            "Epoch: 1, Step : 2816, LR : 0.0000156747, Avg Loss : 557822.5363, f1 score : 0.0012\n",
            "Epoch: 1, Step : 2944, LR : 0.0000152509, Avg Loss : 560462.5646, f1 score : 0.0002\n",
            "Epoch: 1, Step : 3072, LR : 0.0000148272, Avg Loss : 582397.4939, f1 score : 0.0017\n",
            "Epoch: 1, Step : 3200, LR : 0.0000144035, Avg Loss : 587137.0718, f1 score : 0.0015\n",
            "Epoch: 1, Step : 3328, LR : 0.0000139797, Avg Loss : 572326.7604, f1 score : 0.0015\n",
            "Epoch: 1, Step : 3456, LR : 0.0000135560, Avg Loss : 560981.0422, f1 score : 0.0007\n",
            "Epoch: 1, Step : 3584, LR : 0.0000131323, Avg Loss : 559459.1140, f1 score : 0.0002\n",
            "Epoch: 1, Step : 3712, LR : 0.0000127086, Avg Loss : 567589.6980, f1 score : 0.0015\n",
            "Epoch 1 Total Mean Loss : 563534.2482\n",
            "Epoch 1 Total Mean f1 : 0.0010\n",
            "*****Epoch 1 Train Finish*****\n",
            "\n",
            "*****Epoch 1 Valid Start*****\n",
            "Epoch 1 Valid Loss : 154458.0493 Valid f1 : 0.0001 Valid em : 0.0000\n",
            "*****Epoch 1 Valid Finish*****\n",
            "\n",
            "*****Epoch 2 Train Start*****\n",
            "Epoch: 2, Step : 128, LR : 0.0000120730, Avg Loss : 566979.6999, f1 score : 0.0002\n",
            "Epoch: 2, Step : 256, LR : 0.0000116492, Avg Loss : 550597.0730, f1 score : 0.0005\n",
            "Epoch: 2, Step : 384, LR : 0.0000112255, Avg Loss : 562318.6910, f1 score : 0.0010\n",
            "Epoch: 2, Step : 512, LR : 0.0000108018, Avg Loss : 558555.4163, f1 score : 0.0000\n",
            "Epoch: 2, Step : 640, LR : 0.0000103780, Avg Loss : 547399.7885, f1 score : 0.0010\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-913e55b277a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"time : {(end - start)//60}분 {(end - start)%60}초\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# draw_plot(train_dict, valid_dict, 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-15dd7be247b3>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, train_dataloader, valid_dataloader, epochs)\u001b[0m\n\u001b[1;32m     24\u001b[0m       \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mbatch_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_label\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m       \u001b[0;31m# start_logits_idx, end_logits_idx = torch.argmax(outputs.start_logits, dim=-1).float16(), torch.argmax(outputs.end_logits, dim=-1).float16()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-06fb2ef95fdb>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, positions, output_attentions, output_hidden_states)\u001b[0m\n\u001b[1;32m     13\u001b[0m         outputs = self.bert(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, position_ids=position_ids,\n\u001b[1;32m     14\u001b[0m                             \u001b[0mhead_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                   output_attentions=output_attentions, output_hidden_states=output_hidden_states)\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m         )\n\u001b[1;32m   1030\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    612\u001b[0m                     \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m                     \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    615\u001b[0m                 )\n\u001b[1;32m    616\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    496\u001b[0m             \u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself_attn_past_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m         )\n\u001b[1;32m    500\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attention_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    428\u001b[0m             \u001b[0mencoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    431\u001b[0m         )\n\u001b[1;32m    432\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mkey_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m             \u001b[0mvalue_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m         \u001b[0mquery_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose_for_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmixed_query_layer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "전혀 수렴하지 못하고 있다.\n",
        "\n",
        "start_idx와 end_idx가 영역에 존재하는지 여부를 나타내는 평가지표를 추가해야겠다.\n",
        "\n",
        "(end_idx - start_idx)/(예측된 end_idx - 예측된 start_idx)\n",
        "\n",
        "그리고, target인 start_idx가 tokenizer된 이후를 나타내지 않는다. 이 부분도 수정해야한다.\n",
        "\n",
        "start_idx 이전까지 tokenizing, 정답 text tokenizing하면 동일할 것이다."
      ],
      "metadata": {
        "id": "OgJ6Wnb7jeDK"
      }
    }
  ]
}