{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02_BERT_QA_squad_BertForQuestionAnswering.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyNgAmTYUQ3x9M9ccVHqa0lM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2c3120898dee411885b08da9786b357d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_97ddb6e4f5024bdda888a007fac8914f",
              "IPY_MODEL_3c2c845a35a948a4b8e0f1fabd9628e4",
              "IPY_MODEL_51e9a38e4d2b4b42819efdc11bf94d6d"
            ],
            "layout": "IPY_MODEL_f8cf179ca13d4350aee85ee6130b8e99"
          }
        },
        "97ddb6e4f5024bdda888a007fac8914f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5569d34efca24a24b6f15b51b875f2a9",
            "placeholder": "​",
            "style": "IPY_MODEL_d04996e8719f4bfebb1065d003f4f04a",
            "value": "100%"
          }
        },
        "3c2c845a35a948a4b8e0f1fabd9628e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a6ba5e258d3d4662a964cb56911413c3",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2bd9dbbf0d844eee81d0a288e9de89e0",
            "value": 2
          }
        },
        "51e9a38e4d2b4b42819efdc11bf94d6d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0599035bd2a843a6952e6945b4d8f38a",
            "placeholder": "​",
            "style": "IPY_MODEL_b86141628e10402a8a5840edf6170de3",
            "value": " 2/2 [00:00&lt;00:00, 55.81it/s]"
          }
        },
        "f8cf179ca13d4350aee85ee6130b8e99": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5569d34efca24a24b6f15b51b875f2a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d04996e8719f4bfebb1065d003f4f04a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a6ba5e258d3d4662a964cb56911413c3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bd9dbbf0d844eee81d0a288e9de89e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0599035bd2a843a6952e6945b4d8f38a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b86141628e10402a8a5840edf6170de3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttogle918/AI_practice/blob/main/QA%20task/03_BERT_QA_korsquad_BertModel%EB%A1%9C%EA%B5%AC%ED%98%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# korsquad를 사용하여 QA task 풀기(BertModel fine-tuning)\n",
        "\n",
        "참고 사이트\n",
        "\n",
        "[huggingface QA 설명](https://huggingface.co/course/chapter7/7?fw=tf)\n",
        "\n",
        "[huggingface git : ModelOutput](https://github.com/huggingface/transformers/blob/v4.21.0/src/transformers/utils/generic.py#L147)\n",
        "\n",
        "[huggingface git : QuestionAnsweringModelOutput](https://github.com/huggingface/transformers/blob/a9eee2ffecc874df7dd635b2c6abb246fdb318cc/src/transformers/modeling_outputs.py#L764)\n",
        "\n",
        "[huggingface docs](https://huggingface.co/docs/transformers/model_doc/bert)"
      ],
      "metadata": {
        "id": "QocCi_jhZ-ly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "62Oa6eDdlfHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Qu0sRPezlVoA"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, load_metric#, list_metrics\n",
        "\n",
        "from transformers import BertModel, AutoTokenizer, BertConfig, BertPreTrainedModel\n",
        "from transformers.optimization import get_cosine_schedule_with_warmup\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from transformers.modeling_outputs import QuestionAnsweringModelOutput\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "import time\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
      ],
      "metadata": {
        "id": "1N7PgGshrtGh"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# gpu 연산이 가능하면 'cuda:0', 아니면 'cpu' 출력\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device, torch.cuda.device_count()"
      ],
      "metadata": {
        "id": "NMC8y1NUSDHP",
        "outputId": "53fef8c4-e8d0-4156-d68c-ca4f019bf9a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(device(type='cuda', index=0), 1)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Load"
      ],
      "metadata": {
        "id": "50EDgWKELGI-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://huggingface.co/datasets/squad_kor_v1/blob/main/squad_kor_v1.py\n",
        "# squad_kor_v2\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('squad_kor_v1')\n",
        "dataset, dataset['train'][0]"
      ],
      "metadata": {
        "id": "STFveXBu634F",
        "outputId": "6bb93815-c05b-45e5-8e90-d12f5aa884da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530,
          "referenced_widgets": [
            "2c3120898dee411885b08da9786b357d",
            "97ddb6e4f5024bdda888a007fac8914f",
            "3c2c845a35a948a4b8e0f1fabd9628e4",
            "51e9a38e4d2b4b42819efdc11bf94d6d",
            "f8cf179ca13d4350aee85ee6130b8e99",
            "5569d34efca24a24b6f15b51b875f2a9",
            "d04996e8719f4bfebb1065d003f4f04a",
            "a6ba5e258d3d4662a964cb56911413c3",
            "2bd9dbbf0d844eee81d0a288e9de89e0",
            "0599035bd2a843a6952e6945b4d8f38a",
            "b86141628e10402a8a5840edf6170de3"
          ]
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Reusing dataset squad_kor_v1 (/root/.cache/huggingface/datasets/squad_kor_v1/squad_kor_v1/1.0.0/18d4f44736b8ee85671f63cb84965bfb583fa0a4ff2df3c2e10eee9693796725)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2c3120898dee411885b08da9786b357d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 60407\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'title', 'context', 'question', 'answers'],\n",
              "        num_rows: 5774\n",
              "    })\n",
              "}),\n",
              " {'answers': {'answer_start': [54], 'text': ['교향곡']},\n",
              "  'context': '1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다. 이 시기 바그너는 1838년에 빛 독촉으로 산전수전을 다 걲은 상황이라 좌절과 실망에 가득했으며 메피스토펠레스를 만나는 파우스트의 심경에 공감했다고 한다. 또한 파리에서 아브네크의 지휘로 파리 음악원 관현악단이 연주하는 베토벤의 교향곡 9번을 듣고 깊은 감명을 받았는데, 이것이 이듬해 1월에 파우스트의 서곡으로 쓰여진 이 작품에 조금이라도 영향을 끼쳤으리라는 것은 의심할 여지가 없다. 여기의 라단조 조성의 경우에도 그의 전기에 적혀 있는 것처럼 단순한 정신적 피로나 실의가 반영된 것이 아니라 베토벤의 합창교향곡 조성의 영향을 받은 것을 볼 수 있다. 그렇게 교향곡 작곡을 1839년부터 40년에 걸쳐 파리에서 착수했으나 1악장을 쓴 뒤에 중단했다. 또한 작품의 완성과 동시에 그는 이 서곡(1악장)을 파리 음악원의 연주회에서 연주할 파트보까지 준비하였으나, 실제로는 이루어지지는 않았다. 결국 초연은 4년 반이 지난 후에 드레스덴에서 연주되었고 재연도 이루어졌지만, 이후에 그대로 방치되고 말았다. 그 사이에 그는 리엔치와 방황하는 네덜란드인을 완성하고 탄호이저에도 착수하는 등 분주한 시간을 보냈는데, 그런 바쁜 생활이 이 곡을 잊게 한 것이 아닌가 하는 의견도 있다.',\n",
              "  'id': '6566495-0-0',\n",
              "  'question': '바그너는 괴테의 파우스트를 읽고 무엇을 쓰고자 했는가?',\n",
              "  'title': '파우스트_서곡'})"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context_length, question_length, answer_location = [], [], []\n",
        "for data in dataset['train'] :\n",
        "  context_length.append(len(data['context']))\n",
        "  question_length.append(len(data['question']))\n",
        "  answer_location.append(context_length[-1] - data['answers']['answer_start'][0])    # answer_start가 1개라는 가정하에(잘못된 값이 없다는 가정) 문맥의 끝에서부터 위치 구하기\n",
        "                                                                                    # (적게 차이나면 tokenizer에서 truncation때문에 잘릴 위험이 있다.) => truncation 제거!\n",
        "print(f'max context_length : {max(context_length)}  min context_length : {min(context_length)}  mean context_length : {np.mean(np.array(context_length))}')\n",
        "print(f'max question_length : {max(question_length)}  min question_length : {min(question_length)}  mean question_length : {np.mean(np.array(question_length))}')\n",
        "print(f'max answer_location : {max(answer_location)}  min answer_location : {min(answer_location)}')"
      ],
      "metadata": {
        "id": "l48LDFruCBrJ",
        "outputId": "c2700981-c81d-4e6d-920b-829dbd1c4340",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max context_length : 10012  min context_length : 348  mean context_length : 519.2681808399689\n",
            "max question_length : 146  min question_length : 5  mean question_length : 33.79881470690483\n",
            "max answer_location : 10005  min answer_location : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"min answer_location : {np.argmin(answer_location)}\")\n",
        "print(f\"min answer_idx's context : {dataset['train']['context'][np.argmin(np.array(answer_location))]}\")\n",
        "print(f\"min answer_idx's context length : {len(dataset['train']['context'][np.argmin(np.array(answer_location))])}\")"
      ],
      "metadata": {
        "id": "tJq1YxlnNcjm",
        "outputId": "9d080e6e-b51e-408f-982e-5b8825b4d430",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "min answer_location : 1670\n",
            "min answer_idx's context : 고조 사후, 혜제가 즉위하고 고황후가 실권을 잡아 나라를 다스렸다. 관영은 열후로서 고황후를 보좌했다. 태후가 죽자, 여록 등의 여씨 일족이 장안의 군사를 장악하고 있었고, 고조의 손자, 유비의 아들로 제나라의 왕을 지내는 유양은 불만을 품고 반란을 일으켜 여씨 세력을 쓸어버리고자 했다. 여씨 일족은 관영에게 군사를 맡겨 제나라의 반란을 진압하도록 했으나, 관영은 태위 주발, 승상 진평 등과 모의해 여씨를 치도록 작정했다. 그러기에 관영은 형양에 주둔하면서 소문을 퍼뜨려 제나라 군사가 진격을 멈추게 하고, 주발 등이 이 틈에 여씨들을 말갛게 쓸었다. 유양과 관영 모두 군사를 해산해 돌아갔고, 관영은 주발, 진평과 함께 고조의 아들 대왕을 황제로 세우니, 이가 곧 문제다.\n",
            "min answer_idx's context length : 379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 가장 긴 질문\n",
        "dataset['train']['question'][np.argmax(np.array(question_length))]"
      ],
      "metadata": {
        "id": "Cucf_49bQq3W",
        "outputId": "a15e524a-fba0-45e3-981c-a32ce84aceac",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'일본 메이지 대학교를 졸업하고 일본 문교사회국에서 근무한 경험이 있는 사람인 박철웅에게 설립동지회장직을 맡게하고, 대학설립을 준비하고자 도청이 소유한 차량을 사용하게 하고 도지사 명의로 사장과 군수에게 협조지시 공문을 보내는 등의 큰 역할을 한 사람은 누구인가?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer('하나의 교향곡을 쓰려는 뜻을 갖는다.', add_special_tokens=False).input_ids\n",
        "answer = tokenizer('하나의 교향곡', add_special_tokens=False).input_ids\n",
        "tokens = tokenizer(['1839년 바그너는 괴테의 파우스트을 처음 읽고 그 내용에 마음이 끌려 이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다.', '이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다'], \n",
        "                   ['이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다', '이를 소재로 해서 하나의 교향곡을 쓰려는 뜻을 갖는다'], \n",
        "                   return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "text, answer, tokens"
      ],
      "metadata": {
        "id": "03UjdR3LmyiF",
        "outputId": "6c829fbc-fbaf-4455-bce0-a075709d0353",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([3657, 2079, 19282, 2069, 1363, 2370, 2259, 936, 2069, 554, 2259, 2062, 18],\n",
              " [3657, 2079, 19282],\n",
              " {'input_ids': tensor([[    2, 13934,  2236,  2440, 27982,  2259, 21310,  2079, 11994,  3791,\n",
              "           2069,  3790,  1508,  2088,   636,  3800,  2170,  3717,  2052,  9001,\n",
              "           8345,  4642,  2200,  3689,  3657,  2079, 19282,  2069,  1363,  2370,\n",
              "           2259,   936,  2069,   554,  2259,  2062,    18,     3,  8345,  4642,\n",
              "           2200,  3689,  3657,  2079, 19282,  2069,  1363,  2370,  2259,   936,\n",
              "           2069,   554,  2259,  2062,     3],\n",
              "         [    2,  8345,  4642,  2200,  3689,  3657,  2079, 19282,  2069,  1363,\n",
              "           2370,  2259,   936,  2069,   554,  2259,  2062,     3,  8345,  4642,\n",
              "           2200,  3689,  3657,  2079, 19282,  2069,  1363,  2370,  2259,   936,\n",
              "           2069,   554,  2259,  2062,     3,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
              "              0,     0,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1],\n",
              "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1],\n",
              "         [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "          0, 0, 0, 0, 0, 0, 0]])})"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tensorized_label = np.zeros(tokens.input_ids.shape)\n",
        "print(answer)\n",
        "for i in [0,1] :\n",
        "  padding_start = (tokens['attention_mask'][i] == 1).nonzero()[-1].item()+1\n",
        "  print(padding_start)\n",
        "  tensorized_label[i, padding_start-len(text): padding_start-len(text)+len(answer)] = 1\n",
        "  print(tensorized_label)\n",
        "  print(tokens['input_ids'][i])\n",
        "  print(tokens['input_ids'][i, padding_start-len(text): padding_start-len(text)+len(answer)])\n",
        "  print('\\n\\n')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cO9NIqjvq7i_",
        "outputId": "6db96d7f-8b54-47ba-ac9c-3dbad9d98fb5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3657, 2079, 19282]\n",
            "55\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]]\n",
            "tensor([    2, 13934,  2236,  2440, 27982,  2259, 21310,  2079, 11994,  3791,\n",
            "         2069,  3790,  1508,  2088,   636,  3800,  2170,  3717,  2052,  9001,\n",
            "         8345,  4642,  2200,  3689,  3657,  2079, 19282,  2069,  1363,  2370,\n",
            "         2259,   936,  2069,   554,  2259,  2062,    18,     3,  8345,  4642,\n",
            "         2200,  3689,  3657,  2079, 19282,  2069,  1363,  2370,  2259,   936,\n",
            "         2069,   554,  2259,  2062,     3])\n",
            "tensor([ 3657,  2079, 19282])\n",
            "\n",
            "\n",
            "\n",
            "35\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 1.\n",
            "  1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
            "  0. 0. 0. 0. 0. 0. 0.]]\n",
            "tensor([    2,  8345,  4642,  2200,  3689,  3657,  2079, 19282,  2069,  1363,\n",
            "         2370,  2259,   936,  2069,   554,  2259,  2062,     3,  8345,  4642,\n",
            "         2200,  3689,  3657,  2079, 19282,  2069,  1363,  2370,  2259,   936,\n",
            "         2069,   554,  2259,  2062,     3,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0])\n",
            "tensor([ 3657,  2079, 19282])\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "context = dataset['train']['context'][np.argmax(context_length)]\n",
        "li = tokenizer(context).input_ids\n",
        "# 정의되지않은 단어(한자 등)은 어떻게 할 것인가?//정답에 한문이 포함되어있기도 해서... 가지고 가야할 것 같다\n",
        "tokenizer.decode([1]), li.count(1), dataset['train']['answers'][np.argmax(context_length)]"
      ],
      "metadata": {
        "id": "OeWJ7Ue0G00N",
        "outputId": "5cc551dc-5d71-4369-e1af-57e4a218f206",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('[UNK]', 127, {'answer_start': [19], 'text': ['인길(仁吉)']})"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, dataset):\n",
        "        self.question, self.context, self.answer_start, self.answer_text = self.make_dataset(dataset)\n",
        "\n",
        "    def make_dataset(self, dataset):\n",
        "        context, question, answer_start, answer_text = [], [], [], []\n",
        "        for i, data in enumerate(dataset) :\n",
        "          start = data['answers']['answer_start']\n",
        "          if len(start) != 1 : # 답이 없을 때\n",
        "            print(i, data)\n",
        "            continue\n",
        "          text, start = self.get_text(data['context'], start[0])\n",
        "          answer_start.append(start)\n",
        "          answer_text.append(data['answers']['text'])\n",
        "          context.append(data['context'])\n",
        "          question.append(data['question'])\n",
        "        return question, context, answer_start, answer_text\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.question)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.question[idx], self.context[idx], self.answer_start[idx], self.answer_text[idx]\n",
        "\n",
        "    def get_text(self, text, start_loc) :\n",
        "        text_splited = text.split('. ')\n",
        "        length_text_answer_idx = -1\n",
        "        length_text = [0]\n",
        "\n",
        "        for i, t in enumerate(text_splited) :\n",
        "          length_text.append(length_text[-1]+len(t)+2)\n",
        "          \n",
        "          if length_text_answer_idx == -1 and start_loc < length_text[-1] :\n",
        "            length_text_answer_idx = i-1\n",
        "        length_text[-1] -= 2\n",
        "\n",
        "        start, end = 0, len(length_text)-1\n",
        "        # 이후에 question + context가 512 token 이하여야 한다.\n",
        "        # context가 512일 때, Token indices sequence length is longer than the specified maximum sequence length for this model (946 > 512), 256일 때 (521 > 512), \n",
        "        # 정답이 context의 마지막에 위치할 수도 있기에(뒤에서 4번째에 위치하기도 함) context 길이 200으로 지정\n",
        "        while length_text[end] - length_text[start] > 200 :  \n",
        "            if start_loc - length_text[start] > length_text[end] - start_loc :\n",
        "              start += 1\n",
        "            else :\n",
        "              end -= 1\n",
        "        \n",
        "        return text[length_text[start]:length_text[end]], start_loc - length_text[start]"
      ],
      "metadata": {
        "id": "Amz1l80dSL1m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate_fn(batch):\n",
        "    global tokenizer\n",
        "    question_list, context_list, answer_start, answer_text, after_list = [], [], [], [], []\n",
        "\n",
        "    for _question, _context, _start, _text in batch:\n",
        "        question_list.append(_question)\n",
        "        context_list.append(_context)\n",
        "        after_list.append(_context[_start:])\n",
        "        answer_start.append(_start)\n",
        "        answer_text.append(_text)\n",
        "    \n",
        "    tensorized_input = tokenizer(    # 정답 잘림 방지를 위해 max_len과 truncation 제외?\n",
        "        question_list, context_list,\n",
        "        add_special_tokens=True,\n",
        "        padding=\"longest\",  # 배치내 가장 긴 문장을 기준으로 부족한 문장은 [PAD] 토큰을 추가\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    # answer_start token의 위치를 찾기 위해 list로 반환\n",
        "    after_text = tokenizer(\n",
        "        after_list,\n",
        "        add_special_tokens=False,\n",
        "        return_tensors=None\n",
        "    ).input_ids\n",
        "\n",
        "    # answer text token만 변환\n",
        "    answer_tokens = tokenizer(\n",
        "        after_list,\n",
        "        add_special_tokens=False,\n",
        "        return_tensors=None\n",
        "    ).input_ids\n",
        "\n",
        "    tensorized_label = np.zeros(tensorized_input.input_ids.shape)    # input_ids만큼의 길이이고 0으로 된 array\n",
        "\n",
        "    for i, zipped in enumerate(zip(after_text, answer_tokens)) :\n",
        "        text, answer = zipped\n",
        "        padding_start = (tensorized_input['attention_mask'][i] == 1).nonzero()[-1].item()+1\n",
        "        tensorized_label[i, padding_start-len(text): padding_start-len(text)+len(answer)] = 1\n",
        "\n",
        "    return tensorized_input, torch.from_numpy(tensorized_label)"
      ],
      "metadata": {
        "id": "H8bLGE3dVVR8"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_dataloader(dataset, tokenizer, batch_size, s='train') :\n",
        "  dataloader = DataLoader(\n",
        "      dataset,\n",
        "      batch_size =batch_size,\n",
        "      sampler = RandomSampler(dataset) if s == 'train' else SequentialSampler(dataset),\n",
        "      collate_fn = custom_collate_fn\n",
        "  )\n",
        "  print(f'batch_size : {batch_size}')\n",
        "  return dataloader"
      ],
      "metadata": {
        "id": "1eTDFIVxYu73"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 모델 설명\n"
      ],
      "metadata": {
        "id": "IJoPh3xpqUKK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomBertForQuestionAnswering(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.labels_type = [0, 1]   # 1은 정답 token의 위치(여러개 가능)\n",
        "        self.num_labels = len(self.labels_type)\n",
        "        self.bert = BertModel(config, add_pooling_layer=False)\n",
        "        self.qa_output = nn.Linear(config.hidden_size, self.num_labels)\n",
        "        # self.dropout = nn.Dropout(config.dripout_rate)\n",
        "\n",
        "        self.post_init()\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n",
        "        \n",
        "        sequence_output = outputs[0]\n",
        "        logits = self.qa_output(sequence_output)    # linear 통과해서 num_label로 분류\n",
        "        return logits"
      ],
      "metadata": {
        "id": "AyiUyiJ6Mtse"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train"
      ],
      "metadata": {
        "id": "tZXfyh4_Mta4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def initializer(train_dataloader, epochs=2, model_name='klue/bert-base', lr=4e-5, wd=4e-5):\n",
        "    \"\"\"\n",
        "    모델, 옵티마이저, 스케쥴러 초기화\n",
        "    \"\"\"\n",
        "    config = BertConfig.from_pretrained(model_name)\n",
        "    config.max_length = 512\n",
        "    model = CustomBertForQuestionAnswering(config)\n",
        "\n",
        "    optimizer = AdamW(\n",
        "        model.parameters(), # update 대상 파라미터를 입력\n",
        "        lr=lr,    # 2e-5\n",
        "        eps=1e-8,\n",
        "        weight_decay=wd\n",
        "    )\n",
        "    \n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "    print(f\"Total train steps with {epochs} epochs: {total_steps}\")\n",
        "\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer, \n",
        "        num_warmup_steps = 0, # 여기서는 warmup을 사용하지 않는다.\n",
        "        num_training_steps = total_steps\n",
        "    )\n",
        "    print(f'model_name : {model_name}, lr : {lr}, weight_decay : {wd}, epochs : {epochs}')\n",
        "    return model, optimizer, scheduler"
      ],
      "metadata": {
        "id": "GDcl_0m3Zj0f"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(path, model, optimizer, scheduler, epoch, loss, f1, model_name=''):\n",
        "    file_name = f'{path}/epoch:{epoch}_loss:{loss:.4f}_f1:{f1:.4f}.ckpt'\n",
        "    \n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'loss' : loss,\n",
        "            'f1' : f1\n",
        "        }, \n",
        "        file_name\n",
        "    )\n",
        "    \n",
        "    print(f\"Saving epoch {epoch} checkpoint at {file_name}\")"
      ],
      "metadata": {
        "id": "TnpT2s-qZnG-"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### train code"
      ],
      "metadata": {
        "id": "ut6WMU93Z0in"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, scheduler, train_dataloader, valid_dataloader=None, epochs=1):\n",
        "  loss_fct = nn.MSELoss()\n",
        "  em_fct = load_metric('exact_match')\n",
        "  f1_fct = load_metric('f1')\n",
        "  acc_fct = load_metric('accuracy')\n",
        "\n",
        "  train_dict = {'loss' : [], 'f1' : []}\n",
        "  valid_dict = {'loss' : [], 'f1' : [], 'em' : []}\n",
        "\n",
        "  for epoch in range(epochs) :\n",
        "\n",
        "    print(f\"*****Epoch {epoch} Train Start*****\")\n",
        "    total_loss, total_acc, batch_acc, total_f1, batch_f1, batch_em, total_em, batch_loss, batch_count = 0,0,0,0,0,0,0,0,0\n",
        "    \n",
        "    model.train()\n",
        "    model.to(device)\n",
        "    \n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "      batch_count+=1\n",
        "      \n",
        "      batch = tuple(item.to(device) for item in batch)\n",
        "      batch_input, batch_label = batch\n",
        "      \n",
        "      model.zero_grad()\n",
        "      \n",
        "      outputs = model(**batch_input)  # forward\n",
        "      outputs = torch.argmax(outputs, dim=2)\n",
        "      loss = loss_fct(outputs, batch_label)\n",
        "      loss.requires_grad_(True)\n",
        "\n",
        "      batch_loss += loss.item()\n",
        "      total_loss += loss.item()\n",
        "\n",
        "      em = em_fct.compute(predictions=outputs.view([-1, 1]).squeeze(), references=batch_label.view([-1, 1]).squeeze())['exact_match']\n",
        "      f1 = f1_fct.compute(predictions=outputs.view([-1, 1]).squeeze(), references=batch_label.view([-1, 1]).squeeze(), average='macro')['f1']\n",
        "      accuracy = acc_fct.compute(predictions=outputs.view([-1, 1]).squeeze(), references=batch_label.view([-1, 1]).squeeze())['accuracy']\n",
        "\n",
        "      batch_em += em\n",
        "      total_em += em\n",
        "\n",
        "      batch_f1 += f1\n",
        "      total_f1 += f1\n",
        "      \n",
        "      batch_acc += accuracy\n",
        "      total_acc += accuracy\n",
        "\n",
        "      # backward -> 파라미터의 미분(gradient)를 자동으로 계산\n",
        "      loss.backward()\n",
        "\n",
        "      # gradient clipping 적용 \n",
        "      clip_grad_norm_(model.parameters(), 1.0)\n",
        "      \n",
        "      # optimizer & scheduler 업데이트\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "\n",
        "      # 그래디언트 초기화\n",
        "      model.zero_grad()\n",
        "\n",
        "      if (step % 128 == 0 and step != 0):\n",
        "          learning_rate = optimizer.param_groups[0]['lr']\n",
        "          print(f\"Epoch: {epoch}, Step : {step}, LR : {learning_rate:.10f}, Avg Loss : {batch_loss / batch_count:.4f}, f1 score : {batch_f1 / batch_count:.4f}\")\n",
        "          \n",
        "          if (round(batch_f1 / batch_count, 5) == 0) and (round(learning_rate, 10) == 0) :\n",
        "              print(\"Train Finished, learning_rate is 0 and train_f1 is 0\")\n",
        "              return train_dict, valid_dict\n",
        "\n",
        "          batch_loss, batch_f1, batch_count = 0,0,0\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch} Total Mean Loss : {total_loss/(step+1):.4f}\")\n",
        "    print(f\"Epoch {epoch} Total Mean f1 : {total_f1/(step+1):.4f}\")\n",
        "    print(f\"*****Epoch {epoch} Train Finish*****\\n\")\n",
        "\n",
        "    train_dict['f1'].append(total_f1/(step+1))\n",
        "    train_dict['loss'].append(total_loss/(step+1))\n",
        "    # train_dict['em'].append(total_em/(step+1))\n",
        "    \n",
        "    if valid_dataloader is not None:\n",
        "        print(f\"*****Epoch {epoch} Valid Start*****\")\n",
        "        valid_loss, valid_em, valid_f1, valid_acc = validate(model, valid_dataloader, f1_fct, em_fct, acc_fct)\n",
        "        print(f\"Epoch {epoch} Valid Loss : {valid_loss:.4f} Valid f1 : {valid_f1:.4f} Valid em : {valid_em:.4f} Valid acc : {valid_acc:.4f}\")\n",
        "        print(f\"*****Epoch {epoch} Valid Finish*****\\n\")\n",
        "\n",
        "    valid_dict['f1'].append(valid_f1)\n",
        "    valid_dict['loss'].append(valid_loss)\n",
        "    valid_dict['em'].append(valid_em)\n",
        "    if round(valid_f1, 4) == 0 :\n",
        "        break\n",
        "    # if before_loss > valid_loss :\n",
        "    #     before_loss = valid_loss\n",
        "    #     save_checkpoint(\"/content/drive/MyDrive/Colab Notebooks/nlp/qa\", model, optimizer, scheduler, epoch, valid_loss, valid_f1, model_name)\n",
        "\n",
        "    # elif before_f1 < valid_f1  :\n",
        "    #     before_f1 = valid_f1\n",
        "    #     save_checkpoint(\"/content/drive/MyDrive/Colab Notebooks/nlp/qa\", model, optimizer, scheduler, epoch, valid_loss, valid_f1, model_name)\n",
        "\n",
        "  print(\"Train Finished\")\n",
        "  return train_dict, valid_dict"
      ],
      "metadata": {
        "id": "0tx78MsnZzoL"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### validation code"
      ],
      "metadata": {
        "id": "5iByxzyfaJ_i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, valid_dataloader, f1_fct, em_fct, acc_fct):\n",
        "    loss_fct = nn.MSELoss()\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "    \n",
        "    total_loss, total_em, total_f1, total_acc= 0,0, 0, 0\n",
        "        \n",
        "    for step, batch in enumerate(valid_dataloader):\n",
        "        \n",
        "        batch = tuple(item.to(device) for item in batch)\n",
        "            \n",
        "        batch_input, batch_label = batch\n",
        "            \n",
        "        # gradient 계산하지 않음\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**batch_input)\n",
        "            outputs = torch.argmax(outputs, dim=2)\n",
        "\n",
        "        loss = loss_fct(outputs, batch_label)\n",
        "        \n",
        "        em = em_fct.compute(predictions=outputs.view([-1, 1]).squeeze(), references=batch_label.view([-1, 1]).squeeze())['exact_match']\n",
        "        f1 = f1_fct.compute(predictions=outputs.view([-1, 1]).squeeze(), references=batch_label.view([-1, 1]).squeeze())['f1']\n",
        "        acc = acc_fct.compute(predictions=outputs.view([-1, 1]).squeeze(), references=batch_label.view([-1, 1]).squeeze())['accuracy']\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        total_f1 += f1\n",
        "        total_em += em\n",
        "        total_acc += acc\n",
        "\n",
        "    total_loss = total_loss/(step+1)\n",
        "    total_em = total_em/(step+1)\n",
        "    total_f1 = total_f1/(step+1)\n",
        "    total_acc = total_acc/(step+1)\n",
        "    return total_loss, total_em, total_f1, total_acc"
      ],
      "metadata": {
        "id": "C0mOhdJxaJJN"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### draw_plot"
      ],
      "metadata": {
        "id": "z7z2RcB8aMbQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# loss와 f1-score의 변화를 epoch마다 보기 위한 plot\n",
        "def draw_plot(train_dict, valid_dict, i) :\n",
        "  print('green is loss, gray is f1')\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.title('Loss and F1 of Train data')\n",
        "  x_values= [n for n in range(len(train_dict['loss']))]\n",
        "  plt.plot(x_values, train_dict['loss'], color='green', marker='o')  # loss\n",
        "  plt.plot(x_values, train_dict['f1'], color='#AAAAAA', marker='*')  # f1\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.xlabel('Epochs')\n",
        "  plt.title('Loss and F1 of Validation data')\n",
        "  x_values= [n for n in range(len(valid_dict['loss']))]\n",
        "  plt.plot(x_values, valid_dict['loss'], color='green', marker='o')  # loss\n",
        "  plt.plot(x_values, valid_dict['f1'], color='#AAAAAA', marker='*')  # f1\n",
        "\n",
        "  plt.show()\n",
        "  plt.savefig(f'figure_{i}.png')"
      ],
      "metadata": {
        "id": "GB1JmBRkaNrs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = 'klue/bert-base'   # 다시 설정 필요\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "4YIfLFs9aTTU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(dataset['train'])\n",
        "valid_dataset = CustomDataset(dataset['validation'])\n",
        "\n",
        "del dataset"
      ],
      "metadata": {
        "id": "u7zOF7oMvtSE"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "XRje4KyFaj9t",
        "outputId": "05996c0c-e28c-4895-df7f-32d5afa3a5a9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "397"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = make_dataloader(train_dataset, model_name, 16, 'train')\n",
        "valid_dataloader = make_dataloader(valid_dataset, model_name, 8, 'valid')\n",
        "\n"
      ],
      "metadata": {
        "id": "0ye9Fwqve2LD",
        "outputId": "3cbaa047-8238-476c-ad87-211e34c0d75e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size : 16\n",
            "batch_size : 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "learning_rate = 5e-5\n",
        "weight_decay = 4e-5\n",
        "model, optimizer, scheduler = initializer(train_dataloader, 4, model_name, learning_rate, weight_decay)\n",
        "start = time.time()"
      ],
      "metadata": {
        "id": "KzpgKfJfJDEh",
        "outputId": "6908427a-3dd7-4a60-a295-7149ef9bb02e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total train steps with 4 epochs: 15104\n",
            "model_name : klue/bert-base, lr : 5e-05, weight_decay : 4e-05, epochs : 4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# del train_dataset\n",
        "# del valid_dataset"
      ],
      "metadata": {
        "id": "M0iKksS2ShWq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "gc.collect()"
      ],
      "metadata": {
        "id": "30hb6iqNrYPC",
        "outputId": "506cb741-421f-4ba7-9792-3c272d63885b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "109"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "metadata": {
        "id": "LA9hAnBnIOsg"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dict, valid_dict = train(model, optimizer, scheduler, train_dataloader, valid_dataloader, 4)\n",
        "end = time.time()\n",
        "print(f\"time : {(end - start)//60}분 {(end - start)%60}초\")\n",
        "\n",
        "# draw_plot(train_dict, valid_dict, 0)"
      ],
      "metadata": {
        "id": "1-L2JHPFhZKU",
        "outputId": "a3363106-9364-4c0a-d751-bd9ad1033eb2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*****Epoch 0 Train Start*****\n",
            "Epoch: 0, Step : 128, LR : 0.0000495730, Avg Loss : 0.4334, f1 score : 0.5426\n",
            "Epoch: 0, Step : 256, LR : 0.0000491492, Avg Loss : 0.4334, f1 score : 0.5418\n",
            "Epoch: 0, Step : 384, LR : 0.0000487255, Avg Loss : 0.4313, f1 score : 0.5425\n",
            "Epoch: 0, Step : 512, LR : 0.0000483018, Avg Loss : 0.4328, f1 score : 0.5416\n",
            "Epoch: 0, Step : 640, LR : 0.0000478780, Avg Loss : 0.4313, f1 score : 0.5429\n",
            "Epoch: 0, Step : 768, LR : 0.0000474543, Avg Loss : 0.4317, f1 score : 0.5432\n",
            "Epoch: 0, Step : 896, LR : 0.0000470306, Avg Loss : 0.4294, f1 score : 0.5435\n",
            "Epoch: 0, Step : 1024, LR : 0.0000466069, Avg Loss : 0.4310, f1 score : 0.5436\n",
            "Epoch: 0, Step : 1152, LR : 0.0000461831, Avg Loss : 0.4325, f1 score : 0.5432\n",
            "Epoch: 0, Step : 1280, LR : 0.0000457594, Avg Loss : 0.4311, f1 score : 0.5429\n",
            "Epoch: 0, Step : 1408, LR : 0.0000453357, Avg Loss : 0.4289, f1 score : 0.5443\n",
            "Epoch: 0, Step : 1536, LR : 0.0000449119, Avg Loss : 0.4324, f1 score : 0.5424\n",
            "Epoch: 0, Step : 1664, LR : 0.0000444882, Avg Loss : 0.4297, f1 score : 0.5438\n",
            "Epoch: 0, Step : 1792, LR : 0.0000440645, Avg Loss : 0.4285, f1 score : 0.5435\n",
            "Epoch: 0, Step : 1920, LR : 0.0000436408, Avg Loss : 0.4280, f1 score : 0.5440\n",
            "Epoch: 0, Step : 2048, LR : 0.0000432170, Avg Loss : 0.4328, f1 score : 0.5418\n",
            "Epoch: 0, Step : 2176, LR : 0.0000427933, Avg Loss : 0.4285, f1 score : 0.5447\n",
            "Epoch: 0, Step : 2304, LR : 0.0000423696, Avg Loss : 0.4310, f1 score : 0.5437\n",
            "Epoch: 0, Step : 2432, LR : 0.0000419458, Avg Loss : 0.4309, f1 score : 0.5431\n",
            "Epoch: 0, Step : 2560, LR : 0.0000415221, Avg Loss : 0.4281, f1 score : 0.5449\n",
            "Epoch: 0, Step : 2688, LR : 0.0000410984, Avg Loss : 0.4299, f1 score : 0.5441\n",
            "Epoch: 0, Step : 2816, LR : 0.0000406747, Avg Loss : 0.4278, f1 score : 0.5448\n",
            "Epoch: 0, Step : 2944, LR : 0.0000402509, Avg Loss : 0.4316, f1 score : 0.5427\n",
            "Epoch: 0, Step : 3072, LR : 0.0000398272, Avg Loss : 0.4332, f1 score : 0.5412\n",
            "Epoch: 0, Step : 3200, LR : 0.0000394035, Avg Loss : 0.4266, f1 score : 0.5450\n",
            "Epoch: 0, Step : 3328, LR : 0.0000389797, Avg Loss : 0.4287, f1 score : 0.5448\n",
            "Epoch: 0, Step : 3456, LR : 0.0000385560, Avg Loss : 0.4340, f1 score : 0.5426\n",
            "Epoch: 0, Step : 3584, LR : 0.0000381323, Avg Loss : 0.4313, f1 score : 0.5427\n",
            "Epoch: 0, Step : 3712, LR : 0.0000377086, Avg Loss : 0.4283, f1 score : 0.5444\n",
            "Epoch 0 Total Mean Loss : 0.4306\n",
            "Epoch 0 Total Mean f1 : 0.5433\n",
            "*****Epoch 0 Train Finish*****\n",
            "\n",
            "*****Epoch 0 Valid Start*****\n",
            "Epoch 0 Valid Loss : 0.5159 Valid f1 : 0.4673 Valid em : 0.0000 Valid acc : 0.4841\n",
            "*****Epoch 0 Valid Finish*****\n",
            "\n",
            "*****Epoch 1 Train Start*****\n",
            "Epoch: 1, Step : 128, LR : 0.0000370730, Avg Loss : 0.4336, f1 score : 0.5422\n",
            "Epoch: 1, Step : 256, LR : 0.0000366492, Avg Loss : 0.4343, f1 score : 0.5412\n",
            "Epoch: 1, Step : 384, LR : 0.0000362255, Avg Loss : 0.4294, f1 score : 0.5442\n",
            "Epoch: 1, Step : 512, LR : 0.0000358018, Avg Loss : 0.4299, f1 score : 0.5437\n",
            "Epoch: 1, Step : 640, LR : 0.0000353780, Avg Loss : 0.4327, f1 score : 0.5421\n",
            "Epoch: 1, Step : 768, LR : 0.0000349543, Avg Loss : 0.4301, f1 score : 0.5440\n",
            "Epoch: 1, Step : 896, LR : 0.0000345306, Avg Loss : 0.4288, f1 score : 0.5438\n",
            "Epoch: 1, Step : 1024, LR : 0.0000341069, Avg Loss : 0.4302, f1 score : 0.5437\n",
            "Epoch: 1, Step : 1152, LR : 0.0000336831, Avg Loss : 0.4293, f1 score : 0.5436\n",
            "Epoch: 1, Step : 1280, LR : 0.0000332594, Avg Loss : 0.4337, f1 score : 0.5417\n",
            "Epoch: 1, Step : 1408, LR : 0.0000328357, Avg Loss : 0.4298, f1 score : 0.5435\n",
            "Epoch: 1, Step : 1536, LR : 0.0000324119, Avg Loss : 0.4290, f1 score : 0.5445\n",
            "Epoch: 1, Step : 1664, LR : 0.0000319882, Avg Loss : 0.4318, f1 score : 0.5427\n",
            "Epoch: 1, Step : 1792, LR : 0.0000315645, Avg Loss : 0.4303, f1 score : 0.5437\n",
            "Epoch: 1, Step : 1920, LR : 0.0000311408, Avg Loss : 0.4289, f1 score : 0.5446\n",
            "Epoch: 1, Step : 2048, LR : 0.0000307170, Avg Loss : 0.4368, f1 score : 0.5402\n",
            "Epoch: 1, Step : 2176, LR : 0.0000302933, Avg Loss : 0.4264, f1 score : 0.5452\n",
            "Epoch: 1, Step : 2304, LR : 0.0000298696, Avg Loss : 0.4337, f1 score : 0.5413\n",
            "Epoch: 1, Step : 2432, LR : 0.0000294458, Avg Loss : 0.4359, f1 score : 0.5413\n",
            "Epoch: 1, Step : 2560, LR : 0.0000290221, Avg Loss : 0.4292, f1 score : 0.5444\n",
            "Epoch: 1, Step : 2688, LR : 0.0000285984, Avg Loss : 0.4318, f1 score : 0.5425\n",
            "Epoch: 1, Step : 2816, LR : 0.0000281747, Avg Loss : 0.4248, f1 score : 0.5455\n",
            "Epoch: 1, Step : 2944, LR : 0.0000277509, Avg Loss : 0.4301, f1 score : 0.5428\n",
            "Epoch: 1, Step : 3072, LR : 0.0000273272, Avg Loss : 0.4286, f1 score : 0.5441\n",
            "Epoch: 1, Step : 3200, LR : 0.0000269035, Avg Loss : 0.4323, f1 score : 0.5430\n",
            "Epoch: 1, Step : 3328, LR : 0.0000264797, Avg Loss : 0.4327, f1 score : 0.5420\n",
            "Epoch: 1, Step : 3456, LR : 0.0000260560, Avg Loss : 0.4262, f1 score : 0.5456\n",
            "Epoch: 1, Step : 3584, LR : 0.0000256323, Avg Loss : 0.4313, f1 score : 0.5427\n",
            "Epoch: 1, Step : 3712, LR : 0.0000252086, Avg Loss : 0.4285, f1 score : 0.5446\n",
            "Epoch 1 Total Mean Loss : 0.4308\n",
            "Epoch 1 Total Mean f1 : 0.5432\n",
            "*****Epoch 1 Train Finish*****\n",
            "\n",
            "*****Epoch 1 Valid Start*****\n",
            "Epoch 1 Valid Loss : 0.5159 Valid f1 : 0.4673 Valid em : 0.0000 Valid acc : 0.4841\n",
            "*****Epoch 1 Valid Finish*****\n",
            "\n",
            "*****Epoch 2 Train Start*****\n",
            "Epoch: 2, Step : 128, LR : 0.0000245730, Avg Loss : 0.4325, f1 score : 0.5425\n",
            "Epoch: 2, Step : 256, LR : 0.0000241492, Avg Loss : 0.4338, f1 score : 0.5416\n",
            "Epoch: 2, Step : 384, LR : 0.0000237255, Avg Loss : 0.4314, f1 score : 0.5436\n",
            "Epoch: 2, Step : 512, LR : 0.0000233018, Avg Loss : 0.4321, f1 score : 0.5426\n",
            "Epoch: 2, Step : 640, LR : 0.0000228780, Avg Loss : 0.4344, f1 score : 0.5416\n",
            "Epoch: 2, Step : 768, LR : 0.0000224543, Avg Loss : 0.4307, f1 score : 0.5438\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-913e55b277a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"time : {(end - start)//60}분 {(end - start)%60}초\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# draw_plot(train_dict, valid_dict, 0)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-bfe4c4febfb8>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, scheduler, train_dataloader, valid_dataloader, epochs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0mem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mem_fct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'exact_match'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m       \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_fct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'f1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m       \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc_fct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_label\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/metric.py\u001b[0m in \u001b[0;36mcompute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 427\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    428\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_finalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/metric.py\u001b[0m in \u001b[0;36madd_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    477\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"predictions\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"references\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mreferences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mintput_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mintput_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mintput_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 479\u001b[0;31m         \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    480\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36mencode_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m   1629\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m             \u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast_to_python_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0mencoded_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mencode_nested_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoded_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1629\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1630\u001b[0m             \u001b[0mcolumn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast_to_python_objects\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1631\u001b[0;31m             \u001b[0mencoded_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mencode_nested_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mencoded_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/features/features.py\u001b[0m in \u001b[0;36mencode_nested_example\u001b[0;34m(schema, obj, level)\u001b[0m\n\u001b[1;32m   1151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m \u001b[0;32mdef\u001b[0m \u001b[0mencode_nested_example\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m     \"\"\"Encode a nested example.\n\u001b[1;32m   1155\u001b[0m     \u001b[0mThis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mused\u001b[0m \u001b[0msince\u001b[0m \u001b[0msome\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32min\u001b[0m \u001b[0mparticular\u001b[0m \u001b[0mClassLabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mhave\u001b[0m \u001b[0msome\u001b[0m \u001b[0mlogic\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Pytorch Model 일부 Layer만 Freeze 하기](https://soyoung97.github.io/pytorch-model-%EC%9D%BC%EB%B6%80-layer%EB%A7%8C-freeze-%ED%95%98%EA%B8%B0/)"
      ],
      "metadata": {
        "id": "2eU78IIOWq0P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "0hdfuodjWugE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}