{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ttogle918/AI_practice/blob/main/ttogle918/search_bert/workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rECjE2KWO-NA"
      },
      "source": [
        "# 준비"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kss"
      ],
      "metadata": {
        "id": "Bg_4t6peRgbL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwNgmmrXb27d"
      },
      "outputs": [],
      "source": [
        "!pip install pytorch-transformers transformers\n",
        "!pip install sentence-transformers\n",
        "!pip install -q konlpy tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyDAzWS3buRc"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "from sklearn.metrics import f1_score\n",
        "from scipy import stats\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import math\n",
        "import random\n",
        "from sklearn.metrics import f1_score\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from sklearn.neighbors import NearestNeighbors\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "from transformers import AutoTokenizer, PreTrainedTokenizerFast, PreTrainedTokenizer\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRjlTzufb18G",
        "outputId": "fa4dbfcc-897d-455c-c559-13b0336a82bc"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(device(type='cuda', index=0), 1)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# gpu 연산이 가능하면 'cuda:0', 아니면 'cpu' 출력\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device, torch.cuda.device_count()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_RpBQ4ePA1Z"
      },
      "source": [
        "## 데이터셋 Load"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFRYgZzutn7E",
        "outputId": "ed13b00e-cc16-4497-e8c3-bb43038d1b49"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "try :\n",
        "  with open('/content/drive/MyDrive/비상바로가기/최지현/폴더 업로드 방침 등등.txt', 'r') as f :\n",
        "    print(f.readlines()[0])\n",
        "    print('ok')\n",
        "  data_path = f'/content/drive/MyDrive/비상바로가기/최지현'\n",
        "except FileNotFoundError as e :\n",
        "  print(e)\n",
        "  neo_path = \"/content/drive/MyDrive/== 비상교육 ==/최지현\"\n",
        "  data_path = f'{neo_path}'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VV5NCmXSeiS3",
        "outputId": "7bebcb53-30e9-4450-a784-6285bc711dc9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(f'{data_path}/<온리원>/생성된데이터/onlyone_question_temp.csv')\n",
        "print(df.columns)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Z4eWCl6XtdqM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_question = pd.read_csv(f'{data_path}/<온리원>/생성된데이터/only_questions.csv')\n",
        "print(df_question.columns)\n",
        "df_question.head(3)"
      ],
      "metadata": {
        "id": "ps8wYGXruLcE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_question = df_question.drop(columns=['답변키', '답변자키', '답변자명', '답변제목', '답변내용', '답변등록일'])\n",
        "df_question.head(2)"
      ],
      "metadata": {
        "id": "hv2hmEqqkHTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FilterClass"
      ],
      "metadata": {
        "id": "wBof20xlTVZC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class FilterClass() :\n",
        "  def __init__(self, sentiment_model_path, eth_model_path, fword_path) :\n",
        "    self.pipe_sentiment = pipeline(model=sentiment_model_path, tokenizer=AutoTokenizer.from_pretrained(sentiment_model_path, truncation_side='right')) # 감성 분석    # \n",
        "    self.pipe_eth = pipeline(model=eth_model_path, tokenizer=AutoTokenizer.from_pretrained(eth_model_path, truncation_side='right')) # 윤리적 문장인지 분류\n",
        "\n",
        "    # 욕설 단어를 포함한 게시글인지 확인하는 조건 load\n",
        "    self.fword = []\n",
        "    with open(fword_path, 'r') as f :\n",
        "      for line in f.readlines() :\n",
        "        self.fword.append(line.replace('\\n', '').strip())\n",
        "\n",
        "  def run(self, df) :\n",
        "    # 욕설 문장 제거\n",
        "    df, df_fword = self.remove_fword(df)\n",
        "    print(f'욕설이 포함된 질문 list. 총 {len(df_fword)}개\\n')\n",
        "    for q in df_fword.질문 :\n",
        "      print(q)\n",
        "    \n",
        "    # 부정 문장 제거\n",
        "    df, df_negative = self.remove_negative(df)\n",
        "    print(f'부정적인 질문 list. 총 {len(df_negative)}개\\n')\n",
        "    for q in df_negative.질문 :\n",
        "      print(q)\n",
        "\n",
        "    # 비윤리적 문장 제거\n",
        "    df, df_unethical = self.remove_uneth(df)\n",
        "    print(f'비윤리적 문장인 질문 list. 총 {len(df_unethical)}개\\n')\n",
        "    for q in df_unethical.질문 :\n",
        "      print(q)\n",
        "    return df, df_fword, df_negative, df_unethical\n",
        "\n",
        "  def remove_fword(self, df) :\n",
        "    df = df.get()   # 질문\n",
        "    df['have_fword'] = df['질문'].apply(lambda q : sum([q.count(fw) for fw in self.fword]) )\n",
        "    df_fword = df[df['have_fword'] > 0].copy()\n",
        "    df = df.drop(df[df['have_fword'] > 0].index)\n",
        "    df = df.drop(columns=['have_fword'])\n",
        "    return df, df_fword\n",
        "\n",
        "  def remove_uneth(self, df) :\n",
        "\n",
        "    # tokenizer_kwargs = {'padding':True,'truncation':True,'max_length':512,'return_tensors':'pt'}\n",
        "    result = self.pipe_eth(df['질문'].tolist(), batch_size=64, truncation=True)\n",
        "    \n",
        "\n",
        "    df[['unethical', 'score']]= pd.DataFrame.from_records(result)\n",
        "    df_unethical = df[df['unethical'] == 1].copy()\n",
        "    df = df.drop(df[df['unethical'] == 1].index)#.reset_index(drop=True)\n",
        "    df = df.drop(columns=['unethical'] )#.reset_index(drop=True)\n",
        "    return df, df_unethical\n",
        "\n",
        "  def remove_negative(self, df) :\n",
        "\n",
        "    result = self.pipe_sentiment(df['질문'].tolist(), batch_size=64, truncation=True)\n",
        "    df[['sentiment', 'score']]= pd.DataFrame.from_records(result)\n",
        "    df_negative = df[df['sentiment'] == 1].copy()\n",
        "    df = df.drop(df[df['sentiment'] == 1].index)#.reset_index(drop=True)\n",
        "    df = df.drop(columns=['sentiment'] )#.reset_index(drop=True)\n",
        "    return df, df_negative\n",
        "  # 다른 모델 사용 시\n",
        "  # def sentiment_test(self, model, test_loader, device):\n",
        "  #   model.to(device)\n",
        "  #   model.eval()\n",
        "  #   preds = []\n",
        "\n",
        "  #   with torch.no_grad():\n",
        "  #       for sentence in tqdm(iter(test_loader)):\n",
        "  #           sentence = sentence.to(device)\n",
        "  #           logit = model(sentence)\n",
        "  #           preds += logit.argmax(1).detach().cpu().numpy().tolist()\n",
        "  #   return preds"
      ],
      "metadata": {
        "id": "Il_s1bhj4GTr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CustomDataset : dataset"
      ],
      "metadata": {
        "id": "iWZ9Sc-aTOgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "type(df[['질문']]), type(df['질문'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5De6jV2Z6z3",
        "outputId": "5101d0c4-b613-4bc4-9696-6589926f1a84"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(pandas.core.frame.DataFrame, pandas.core.series.Series)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset():\n",
        "    def __init__(self, df):\n",
        "      # case 1. 질문, 답변 모두 들어오는 경우 ==> 기존 질문\n",
        "      # case 2. 질문만 입력되는 경우 ==> 새 질문(추론)\n",
        "      self.sentence = self.preprocessing(df)\n",
        "\n",
        "      if '답변내용' in df.columns :\n",
        "        self.answer = df['답변내용']\n",
        "        self.name = df['질문자명']\n",
        "    \n",
        "    def get(self) :\n",
        "      return self.sentence\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "      if index == '질문' :\n",
        "          return self.sentence\n",
        "      elif index == '답변내용' :\n",
        "          return self.answer\n",
        "      elif index == '질문자명' :\n",
        "          return self.name\n",
        "      \n",
        "      if self.answer is not None:\n",
        "          return self.sentence.iloc[index], self.answer.iloc[index], self.name.iloc[index]\n",
        "      else:\n",
        "          return self.sentence.iloc[index]\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.sentence)\n",
        "\n",
        "    def preprocessing(self, df_temp) :\n",
        "      df_temp['질문'] = df_temp['질문'].apply(lambda x : self.replace_continue_string(x))    # 반복 문자 없애기\n",
        "      df_temp['질문'] = df_temp['질문'].apply(lambda x : BeautifulSoup(x,\"html5lib\").get_text())    # html tag 없애기\n",
        "      df_temp['질문'] = df_temp['질문'].apply(self.remove_emoji)   # 이모지 없애기\n",
        "      df_temp['질문'] = df_temp['질문'].apply(lambda x : x.replace('\\xa0', ''))\n",
        "      return df_temp[['질문']]\n",
        "\n",
        "    def replace_continue_string(self, s) :\n",
        "      dic = self.continue_string(s)\n",
        "      for key in dic :\n",
        "        s = s.replace(key*dic[key], key*2)\n",
        "      return s\n",
        "      \n",
        "    def remove_emoji(self, text) :\n",
        "      only_BMP_pattern = re.compile(\"[\"\n",
        "              u\"\\U00010000-\\U0010FFFF\"  #BMP characters 이외\n",
        "                                \"]+\", flags=re.UNICODE)\n",
        "      return only_BMP_pattern.sub(r'', text) # no emoji\n",
        "\n",
        "    def continue_string(self, s) :\n",
        "      bef_string = ''\n",
        "      cnt, i = 1, 0\n",
        "      dic = {}\n",
        "      while i < len(s) :\n",
        "        if bef_string == s[i] :\n",
        "          cnt += 1\n",
        "        else :\n",
        "          bef_string = s[i]\n",
        "          cnt = 1\n",
        "        if cnt > 2 :\n",
        "          dic[bef_string] = cnt\n",
        "        i += 1\n",
        "      return dic"
      ],
      "metadata": {
        "id": "edblT6xljMrY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# CustomClass : 유사한 문장 얻기"
      ],
      "metadata": {
        "id": "K7vsoubv64SC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pandas의 dataframe, df['질문']이 input으로 들어감 ==> apply를 쓰려고 pandas 형태로 넣음\n",
        "# base_question에 필요한 것\n",
        "class CustomClass() :\n",
        "  def __init__(self, similarity_model_path, base_questions) :\n",
        "    # 유사도 측정한 모델\n",
        "    self.embedder = SentenceTransformer(similarity_model_path)    \n",
        "    # 기존 문장 전처리\n",
        "    # 기존 질문에 대해 filtering 수행 ==> 밖에서 수행해야함!!\n",
        "    # 기존 문장 임베딩\n",
        "    self.corpus_embeddings = self.embedder.encode(base_questions)\n",
        "\n",
        "  # 새 질문 필터링 후, 유사도 찾기\n",
        "  def run(self, df, threshold=-1) :\n",
        "    # 유사한 질문 문장 index 얻기\n",
        "    print('similarity')\n",
        "    if threshold >= 1 :   # 1이상이면 k개 선택하라는 의미\n",
        "      top_k = min(threshold, len(self.corpus_embeddings))\n",
        "      top_results = self.get_similar_sentences(df, top_k)    # score, idx\n",
        "    else :    # 1 미만이라면 ( 원래는 -1 ~ 1 까지의 값 가능 ) 유사도가 임계값 이상인 데이터만 사용하라는 의미\n",
        "      top_results = self.get_similar_sentences_by_threshold(df, threshold)    # score, idx\n",
        "\n",
        "    # 유사한 질문 문장과 그 답변 print\n",
        "    return top_results\n",
        "\n",
        "  # 임계값으로 구분 \n",
        "  def get_similar_sentences_by_threshold(self, df, threshold) :\n",
        "    ret_dict = {}\n",
        "    queries = df['질문'].tolist()\n",
        "    query_embeddings = self.embedder.encode(queries)\n",
        "\n",
        "    for i, query in enumerate(queries):\n",
        "      cos_scores = util.cos_sim(query_embeddings[i], self.corpus_embeddings)[0]\n",
        "      check = ( cos_scores>=threshold ).nonzero(as_tuple=True)[0]\n",
        "      ret_dict[query] = {'score' : cos_scores[check], 'idx' : check }\n",
        "    return ret_dict\n",
        "\n",
        "  def get_similar_sentences(self, df, top_k) :\n",
        "    ret_dict = {}\n",
        "    queries = df['질문'].tolist()\n",
        "    query_embeddings = self.embedder.encode(queries)\n",
        "\n",
        "    for i, query in enumerate(queries):\n",
        "      cos_scores = util.cos_sim(query_embeddings[i], self.corpus_embeddings)[0]\n",
        "      top_results = torch.topk(cos_scores, k=top_k)\n",
        "      ret_dict[query] = {'score' : top_results[0], 'idx' : top_results[1] }\n",
        "    return ret_dict\n",
        "\n",
        "  def make_dataloader(self, dataset, tok_model, batch_size, s='test') :\n",
        "    global tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(tok_model)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size =batch_size,\n",
        "        sampler = SequentialSampler(dataset),\n",
        "        collate_fn = self.custom_collate_fn\n",
        "        # num_workers = 1\n",
        "    )\n",
        "    print(f'batch_size : {batch_size}')\n",
        "    return dataloader\n",
        "  \n",
        "  def custom_collate_fn(self, batch):\n",
        "    input1_list = []\n",
        "    for _input1 in batch:\n",
        "        input1_list.append(_input1)\n",
        "    \n",
        "    tensorized_input = tokenizer(\n",
        "        input1_list,\n",
        "        add_special_tokens=True,\n",
        "        padding=\"longest\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "    return tensorized_input"
      ],
      "metadata": {
        "id": "YDU_mFvj1PNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_similar(df, top_results) :\n",
        "  for query in df['질문'].tolist() :\n",
        "    print(f'\\n질문 : {query}')\n",
        "\n",
        "    # for score, idx in zip(top_results[query]['dist'], top_results[query]['idx']):\n",
        "    if 'answer_score' in top_results[query] :\n",
        "      \n",
        "      score, answer_score, idx = top_results[query]['score'], top_results[query]['answer_score'], top_results[query]['idx']\n",
        "      for i in range(len(score)) :\n",
        "        print(f\"{base_dataset[idx[i]][0]}\", \"(Score: {:.4f})\".format(score[i]))\n",
        "        print(f\"{base_dataset[idx[i]][1]}\", \"(Score: {:.4f})\".format(answer_score[i]))\n",
        "        print('\\n')\n",
        "  \n",
        "def preprocessing(df) :\n",
        "  df['질문'] = df['질문'].apply(lambda x : BeautifulSoup(x,\"html5lib\").get_text())\n",
        "  # df['질문'] = df['질문'].apply(lambda x : ''.join(dict.fromkeys(x) ) )\n",
        "  return df"
      ],
      "metadata": {
        "id": "TQsmuPCnktS_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# de_identifyClass"
      ],
      "metadata": {
        "id": "anicfyEUvo5h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kss\n",
        "\n",
        "class de_identifyClass() :\n",
        "  # df column에 이름 내역 포함되어있어야 함\n",
        "  def __init__(self) :\n",
        "    reg_ptrn = \"(\\d{6}[ ,-]-?[1-4]\\d{6})|(\\d{6}[ ,-]?[1-4])\"\n",
        "    addr_ptrn = \"([가-힣]+[도시구군]\\s)+(\\(?[가-힣]+\\d?[읍로길동가](\\d[가-힣]\\s?)?\\)?\\s?([0-9]+)?)+-?[0-9]+?\\s?([가-힣]+\\s{0,2}아파트)?([가-힣]{2,10}빌)?\\s?(([0-9]{1,4}[동])?\\s?([0-9]{1,4}[호])?)?\"\n",
        "    # addr_ptrn = \"([가-힣]+[도시구군]\\s)+(\\(?[가-힣]+\\d?[읍로길동가](\\d[가-힣]\\s?)?\\)?\\s?([0-9]+)?)+-?[0-9]+([가-힣]아파트)?([가-힣]빌)?\\s?([0-9]+[동호]\\s?)?\"\n",
        "    mail_ptrn = \"^[\\d][A-Za-z0-9!-_\\.]+@[A-Za-z]+\\.[A-Za-z]+\"\n",
        "    account_ptrn = \"^(\\d{1,})(-(\\d{1,})){1,}\"\n",
        "    phone_ptrn = \"\\d{2,3}[ ,-]?\\d{2,4}[ ,-]?\\d{4}\"\n",
        "    self.embedding_dim = 512   # bert-base 임베딩 벡터 크기\n",
        "    \n",
        "    self.reg_prog = re.compile(reg_ptrn)\n",
        "    self.addr_prog = re.compile(addr_ptrn)\n",
        "    self.mail_prog = re.compile(mail_ptrn)\n",
        "    self.account_prog = re.compile(account_ptrn)\n",
        "    self.phone_prog = re.compile(phone_ptrn)\n",
        "\n",
        "  def de_identify_private_info(self, content) :\n",
        "    content = self.phone_prog.sub('(휴대폰번호)', content)\n",
        "    content = self.reg_prog.sub('(주민번호)', content)\n",
        "    content = self.mail_prog.sub('(이메일)', content)\n",
        "    content = self.account_prog.sub('(계좌번호)', content)\n",
        "    return content\n",
        "\n",
        "  def de_identify_address(self, sentence) :\n",
        "    sentence = self.addr_prog.sub('(주소)', sentence)\n",
        "\n",
        "    return sentence\n",
        "\n",
        "  def de_identify_name(self, sentence, name) :\n",
        "    # 이름 익명화\n",
        "    # 질문 = 질문.apply(작성자명없애기.)\n",
        "    sentence = sentence.replace(name[1:], '온리원')\n",
        "    sentence = sentence.replace(name, '온리원')\n",
        "    return sentence\n",
        "\n",
        "  def run(self, sentence, name) :\n",
        "    # 주소 : NER 추가 혹은 정규표현식 주석 제거 ==> 두가지 모두 사용, 합집합(더 넓은 범위) 적용\n",
        "    \n",
        "    # 정규표현식으로 패턴 매칭\n",
        "    sentence = self.get_private_info(sentence)\n",
        "    # 이름 수정 ==> \"온리원\"이~\n",
        "    sentence = self.de_identify_name(sentence, name)\n",
        "    return sentence\n",
        "\n",
        "  # NER 주소 \n",
        "  def get_entity_per_word(self, str_dic_list) :\n",
        "    print(\"get_entity_per_word\")\n",
        "    result_li = []\n",
        "    for dic_list in str_dic_list :\n",
        "      result_li.append(self.get_location_index(dic_list))\n",
        "    return result_li\n",
        "\n",
        "  def get_location_index(self, dic_list) :\n",
        "    if len(dic_list) == 0 :\n",
        "      return []\n",
        "    start_idx, end_idx = -3, -3\n",
        "    isLoc = False\n",
        "    ret_list = []   # stack ==> .pop()\n",
        "    word = ''\n",
        "    for dic in dic_list :\n",
        "      # 이어질 때\n",
        "      if end_idx + 2  >= dic['start'] :    # 띄어쓰기, 마침표 때문에 +2까지 허용\n",
        "        end_idx = dic['end']    # end index로 문장 길이 파악\n",
        "        word += dic['word']\n",
        "        if dic['entity'] == 'LOC-B' :\n",
        "          isLoc = True\n",
        "\n",
        "      else :    # 이어지지 않을 때\n",
        "        # 이전 개체가 loc이라면 저장\n",
        "        if isLoc == True :\n",
        "          # 주소인 location일 때는 7글자가 넘을 것이다. ==> (서울시) 중구 태평로1가 31, 중구 세종대로 (110)\n",
        "          if end_idx - start_idx > 7 :\n",
        "            ret_list.append([start_idx, end_idx, word])\n",
        "          isLoc = False\n",
        "\n",
        "        start_idx = dic['start']\n",
        "        end_idx = dic['end']\n",
        "        word = dic['word']\n",
        "        \n",
        "        if dic['entity'] == 'LOC-B' :\n",
        "          isLoc = True\n",
        "\n",
        "    if isLoc == True :\n",
        "      if end_idx - start_idx > 7 :\n",
        "        ret_list.append([start_idx, end_idx, word])\n",
        "    return ret_list\n",
        "\n",
        "  # token의 수가 512를 넘을 때 런타임 에러가 남. \n",
        "  # 개인정보 삭제가 목표이기 때문에 모든 문장을 확인할 필요가 있음.\n",
        "  # 문장 분리한 뒤 주소 확인, 다시 합침\n",
        "  def find_location(self, sentence) :\n",
        "    loc_li = []\n",
        "    try :\n",
        "      # loc_li : 찾아낸 주소를 [start_idx, end_idx, 주소명]으로 담고있는 리스트\n",
        "      loc_li = self.get_location_index(self.ner(sentence))\n",
        "    except RuntimeError :   # token의 길이가 임베딩 최대 길이보다 클 때 나타나는 오류\n",
        "      try :\n",
        "        # 1. 문장 split\n",
        "        splited_sent = kss.split_sentences(sentence)\n",
        "        # 최대한 많은 문장을 담아야 주소를 잘 인식함 ==> embedding 차원의 수를 넘지 않는 최대 길이의 문장 얻기\n",
        "        start_idx, sent_length, temp_sent = 0, 0, []\n",
        "        for i, sent in enumerate(splited_sent) :\n",
        "          sent_length += len(sent)\n",
        "          if sent_length > self.embedding_dim :\n",
        "            temp_sent.append(''.join(splited_sent[start_idx:i]))   # 이번 sent는 포함하지 않음\n",
        "            sent_length, start_idx = len(sent), i\n",
        "            # 한 문장이 임베딩 차원보다 클 때(한 문장 : 문장 분리기로 나눈 결과)\n",
        "            if sent_length > self.embedding_dim :\n",
        "              temp_sent.append(''.join(splited_sent[start_idx:i][:self.embedding_dim]))   # 이번 sent는 포함하지 않음\n",
        "              sent_length, start_idx = len(sent)-self.embedding_dim, i\n",
        "              \n",
        "        temp_sent.append(''.join(splited_sent[start_idx:]))\n",
        "\n",
        "        result = []\n",
        "        for i, li in enumerate(self.ner(temp_sent)) :\n",
        "          loc_li = self.get_location_index(li)\n",
        "          \n",
        "          while len(loc_li) > 0 :\n",
        "            start_idx, end_idx, word = loc_li.pop()\n",
        "            temp_sent[i] = temp_sent[i].replace(temp_sent[i][start_idx:end_idx], '(주소)')\n",
        "        return ''.join(temp_sent)\n",
        "      except :\n",
        "        print(sentence)\n",
        "        print(splited_sent)\n",
        "        print(sent_length)\n",
        "    try :\n",
        "      while len(loc_li) > 0 :\n",
        "        start_idx, end_idx, word = loc_li.pop()\n",
        "        sentence = sentence.replace(sentence[start_idx:end_idx], '(주소)') \n",
        "    except :\n",
        "      print(sentence)\n",
        "    return sentence\n"
      ],
      "metadata": {
        "id": "KpkhC9yel65s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# test : 질문 = 강의명 + 질문제목 + 질문 내용"
      ],
      "metadata": {
        "id": "eDYC5obUjpML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "CyJMjC02a2Ku"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.fillna('')\n",
        "df_question = df_question.fillna('')\n",
        "df.isna().sum()"
      ],
      "metadata": {
        "id": "d_BbFpgadfNv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['질문'] = df['강의명'] + df['질문제목'] + df['질문내용']\n",
        "df_question['질문'] = df_question['강의명'] + df_question['질문제목'] + df_question['질문내용']\n",
        "\n",
        "base_dataset = CustomDataset(df[['질문', '답변내용', '질문자명']])"
      ],
      "metadata": {
        "id": "RbgDfvfQjTT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_model_path = 'baikal-nlp/dbert-sentiment'\n",
        "eth_model_path = 'baikal-nlp/dbert-eth2'\n",
        "fword_path =f'{data_path}/<필터링단어>/fword_list.txt'\n",
        "\n",
        "filter = FilterClass(sentiment_model_path, eth_model_path, fword_path)"
      ],
      "metadata": {
        "id": "uLjV2s5RVlJo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df, df_fword, df_negative, df_unethical = filter.run(base_dataset)"
      ],
      "metadata": {
        "id": "6Da1P36sWGvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "CgbXxLT9UHpv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df.to_csv(f'{data_path}/요약:질문 검색/after_filtering.csv', index=False)"
      ],
      "metadata": {
        "id": "7-9GA-7tHGon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "similarity_embedder = f'{data_path}/요약:질문 검색/튜닝된모델'\n",
        "\n",
        "start_time = time.time()\n",
        "model = CustomClass(similarity_embedder, df['질문'].tolist())\n",
        "end_time = time.time()\n",
        "print('초기화에 걸린 시간 : ', end_time - start_time)"
      ],
      "metadata": {
        "id": "zyWPkeRI0Hjx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_question['질문'] = df_question['질문'].astype(str)\n",
        "df_question['질문여부'] = df_question['질문'].apply(lambda x : x.find('질문'))\n",
        "df_question = df_question[df_question['질문여부'] > -1]"
      ],
      "metadata": {
        "id": "t_YKzlDwkaL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df_question)"
      ],
      "metadata": {
        "id": "M0SaxAE6eMHp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 코사인유사도 값이 0.7(임계값) 이상일 때"
      ],
      "metadata": {
        "id": "DBz2HR_scoba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "# top_results = model.run(df_question.iloc[:100].질문.to_frame(), 2)    # 가장 비슷한 2개의 질문 출력 시 사용\n",
        "top_results = model.run(df_question.iloc[:100].질문.to_frame(), 0.7)\n",
        "end_time = time.time()\n",
        "\n",
        "print('추론에 걸린 시간 : ', end_time - start_time)"
      ],
      "metadata": {
        "id": "8twxSA2nb8Vl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_similar(df, top_results) :\n",
        "  for query in df['질문'].tolist() :\n",
        "    print(f'\\n질문 : {query}')\n",
        "\n",
        "    for score, idx in zip(top_results[query]['score'], top_results[query]['idx']):\n",
        "      print(f\"{de_identification.run(df[idx]['질문'], df[idx]['이름'])}\")\n",
        "      print(f\"(Score: )\".format(score))\n",
        "      print(f\"{df[idx][1]}\", \"(Score: {:.4f})\".format(score))\n",
        "      print(f\"(Score: )\".format(score))\n",
        "      print('\\n')\n",
        "  return top_results"
      ],
      "metadata": {
        "id": "RlHQTN67SrCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "de_identification = de_identifyClass(df, top_results)\n",
        "# idx, top_results, \n",
        "print_similar(df, top_results)\n",
        "end_time = time.time()\n",
        "print('익명화+결과도출에 걸린 시간 : ', end_time - start_time)"
      ],
      "metadata": {
        "id": "DCtKvuJsJOmd"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}